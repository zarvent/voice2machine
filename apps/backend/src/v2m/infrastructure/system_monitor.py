# This file is part of voice2machine.
#
# voice2machine is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# voice2machine is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with voice2machine.  If not, see <https://www.gnu.org/licenses/>.

"""
SYSTEM MONITOR SERVICE

Este servicio se encarga de recolectar m√©tricas del sistema en tiempo real.
Utiliza v2m_engine (Rust) para m√©tricas de CPU/RAM con bajo overhead, con fallback
autom√°tico a psutil si Rust no est√° disponible. GPU se monitorea via torch.cuda.

Se adhiere al principio de "Single Responsibility" proveyendo solo datos de observaci√≥n.
"""

import logging
from types import ModuleType
from typing import Any

import psutil

logger = logging.getLogger(__name__)

# Intenta importar el monitor Rust
try:
    from v2m_engine import SystemMonitor as RustSystemMonitor

    HAS_RUST_MONITOR = True
    logger.info("üöÄ monitor de sistema rust v2m_engine cargado")
except ImportError:
    HAS_RUST_MONITOR = False
    logger.warning("‚ö†Ô∏è monitor de sistema rust no disponible, usando fallback python")


class SystemMonitor:
    """
    Monitor de recursos del sistema para observabilidad en tiempo real.
    Provee m√©tricas de RAM, CPU y GPU (si est√° disponible).

    Utiliza `v2m_engine` (Rust) para m√©tricas de CPU/RAM si est√° disponible,
    fallback a `psutil` en Python.
    """

    def __init__(self) -> None:
        # Motor Rust (Opcional)
        self._rust_monitor = RustSystemMonitor() if HAS_RUST_MONITOR else None

        # OPTIMIZACI√ìN BOLT: Cachear m√≥dulo torch para evitar importaci√≥n repetida
        self._torch: ModuleType | None = None
        self._gpu_available = self._check_gpu_availability()

        # OPTIMIZACI√ìN BOLT: Cachear m√©tricas est√°ticas (Total RAM, GPU Name)
        # Esto evita syscalls y llamadas a driver redundantes en cada polling (500ms)
        try:
            if self._rust_monitor:
                self._rust_monitor.update()
                total_bytes, _, _ = self._rust_monitor.get_ram_usage()
                self._ram_total_gb = round(total_bytes / (1024**3), 2)
            else:
                mem = psutil.virtual_memory()
                self._ram_total_gb = round(mem.total / (1024**3), 2)
        except Exception as e:
            logger.warning(f"failed to cache ram info: {e}")
            self._ram_total_gb = 0.0

        self._gpu_static_info: dict[str, Any] = {}
        if self._gpu_available and self._torch:
            try:
                device = self._torch.cuda.current_device()
                props = self._torch.cuda.get_device_properties(device)
                self._gpu_static_info = {"name": props.name, "vram_total_mb": round(props.total_memory / (1024**2), 2)}
            except Exception as e:
                logger.warning(f"failed to cache gpu info: {e}")
                self._gpu_available = False

        logger.info(
            "system monitor initialized",
            extra={"gpu_available": self._gpu_available, "ram_total_gb": self._ram_total_gb},
        )

    def _check_gpu_availability(self) -> bool:
        """Verifica si hay una GPU NVIDIA disponible via torch.cuda."""
        try:
            import torch

            # OPTIMIZACI√ìN BOLT: Guardar referencia al m√≥dulo para reuso en polling
            self._torch = torch
            return torch.cuda.is_available()
        except ImportError:
            logger.warning("torch not available, GPU monitoring disabled")
            return False
        except Exception as e:
            logger.warning(f"failed to check GPU availability: {e}")
            return False

    def get_system_metrics(self) -> dict[str, Any]:
        """
        Obtiene una instant√°nea de las m√©tricas actuales del sistema.

        Returns:
            Dict con claves 'ram', 'cpu', 'gpu' (opcional)
        """
        if self._rust_monitor:
            # Actualizar snapshot en Rust
            self._rust_monitor.update()

        metrics = {
            "ram": self._get_ram_usage(),
            "cpu": self._get_cpu_usage(),
        }

        if self._gpu_available:
            metrics["gpu"] = self._get_gpu_usage()

        return metrics

    def _get_ram_usage(self) -> dict[str, float]:
        """Retorna uso de memoria RAM en GB y porcentaje."""
        if self._rust_monitor:
            _, used, percent = self._rust_monitor.get_ram_usage()
            return {"total_gb": self._ram_total_gb, "used_gb": round(used / (1024**3), 2), "percent": round(percent, 1)}

        mem = psutil.virtual_memory()
        return {
            "total_gb": self._ram_total_gb,  # Usar valor cacheado
            "used_gb": round(mem.used / (1024**3), 2),
            "percent": mem.percent,
        }

    def _get_cpu_usage(self) -> dict[str, Any]:
        """Retorna uso de CPU global."""
        if self._rust_monitor:
            return {"percent": round(self._rust_monitor.get_cpu_usage(), 1)}

        return {"percent": psutil.cpu_percent(interval=None)}

    def _get_gpu_usage(self) -> dict[str, Any]:
        """
        Retorna uso real de GPU usando torch.cuda.

        Returns:
            Dict con m√©tricas de GPU: name, vram_used_mb, vram_total_mb, temp_c
        """
        try:
            # OPTIMIZACI√ìN BOLT: Usar referencia cacheada self._torch
            # Evita b√∫squeda en sys.modules cada 500ms
            if not self._torch or not self._torch.cuda.is_available():
                return {"name": "N/A", "vram_used_mb": 0, "vram_total_mb": 0, "temp_c": 0}

            device = self._torch.cuda.current_device()
            # Usar est√°ticos cacheados
            static = self._gpu_static_info

            # VRAM metrics en MB (Din√°mico)
            vram_reserved = self._torch.cuda.memory_reserved(device) / (1024**2)

            return {
                "name": static.get("name", "Unknown"),
                "vram_used_mb": round(vram_reserved, 2),
                "vram_total_mb": static.get("vram_total_mb", 0),
                "temp_c": 0,  # torch no expone temperatura, requiere pynvml
            }
        except Exception as e:
            logger.error(f"failed to get GPU metrics: {e}", exc_info=True)
            return {"name": "Error", "vram_used_mb": 0, "vram_total_mb": 0, "temp_c": 0}

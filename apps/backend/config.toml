# voice2machine (v2m) - Configuration File

# LOAD PRIORITY (highest to lowest):
#   1. Constructor arguments (programmatic overrides)
#   2. Environment variables
#   3. .env file (local secrets)
#   4. THIS FILE (config.toml)
#   5. Defaults from Pydantic models

# ============================================================================
# PATHS - XDG Base Directory Compliant
# ============================================================================
# All paths are auto-generated dynamically in code using XDG_RUNTIME_DIR
# Default location: /run/user/<uid>/v2m (or /tmp/v2m_<uid> as fallback)
# No configuration needed - defaults are correct.
[paths]

# ============================================================================
# TRANSCRIPTION SERVICE
# ============================================================================
[transcription]
backend = "whisper"  # Backend selector: "whisper" (future: "vosk", "custom")

[transcription.whisper]
model = "large-v3-turbo"
language = "auto"
device = "cuda"
compute_type = "int8_float16"
device_index = 0
num_workers = 2  # 2 workers sufficient for single stream

# VAD parameters (optimized for Spanish prosody - SOTA 2026)
[transcription.whisper.vad_parameters]
threshold = 0.4              # Probability threshold 0.0-1.0 (0.4 avoids breathing noise)
min_speech_duration_ms = 250  # Minimum speech segment duration (filters brief noise)
min_silence_duration_ms = 1000  # Spanish prosody safe (1s preserves natural pauses)
speech_pad_ms = 400          # Padding on speech boundaries (keeps word edges)

# ============================================================================
# LLM SERVICE
# ============================================================================
[gemini]
model = "gemini-3-flash-preview"  # Latest: Gemini 3 Flash (Dec 17, 2025) | Alt: "gemini-2.5-flash"
temperature = 0.3  # 0.0 (deterministic) to 2.0 (creative)
max_tokens = 8192  # Gemini 3 Flash has 128k context - increased from 2048
max_input_chars = 12000  # Proportional to max_tokens increase
request_timeout = 30
retry_attempts = 3
retry_min_wait = 2
retry_max_wait = 10
api_key = "${GEMINI_API_KEY}"

# ============================================================================
# DESKTOP NOTIFICATIONS
# ============================================================================
[notifications]
expire_time_ms = 3000  # Auto-close time in milliseconds (3s default)
auto_dismiss = true    # Programmatic close via DBUS (for Unity/GNOME that ignore expire-time)

# ============================================================================
# LOCAL LLM (Alternative to Gemini)
# ============================================================================
[llm]
backend = "local"  # Options: "local" (llama.cpp), "gemini" (Google API)

[llm.local]
model_path = "models/qwen2.5-3b-instruct-q4_k_m.gguf"
n_gpu_layers = -1   # -1 = full GPU offload (all layers to GPU for fastest inference)
n_ctx = 2048        # Context window in tokens
temperature = 0.3   # 0.0 (deterministic) to 2.0 (creative)
max_tokens = 512    # Maximum tokens to generate

# ============================================================================
# OLLAMA LLM (Alternative local backend with structured outputs)
# ============================================================================
[llm.ollama]
host = "http://localhost:11434"
model = "gemma2:2b"      # ALT: "phi3.5-mini", "qwen2.5-coder:7b"
keep_alive = "5m"        # "0m" = free VRAM | "5m" = keep loaded | "30m" = min latency
temperature = 0.0        # 0.0 for deterministic structured outputs

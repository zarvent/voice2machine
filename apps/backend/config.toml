[paths]
recording_flag = "/tmp/v2m_recording.pid"
audio_file = "/tmp/v2m_audio.wav"
log_file = "/tmp/v2m.log"
venv_path = "~/v2m/venv"

[whisper]
model = "large-v3-turbo"
language = "auto"
device = "cuda"
compute_type = "float16"
device_index = 0
num_workers = 2  # 2 workers suficientes para single stream

beam_size = 1  # Greedy decode: ~60% más rápido que beam=4
best_of = 1  # Sin sampling múltiple
temperature = 0.0
vad_filter = false  # DEBUGGING: Deshabilitado temporalmente para diagnosticar

[whisper.vad_parameters]
# VAD ajustado para NO cortar frases y detectar voces suaves
threshold = 0.3  # Bajado de 0.5→0.3: más sensible a voces suaves/lejanas
min_speech_duration_ms = 300  # Subido de 250→300: evitar falsos positivos de ruidos breves
min_silence_duration_ms = 500  # Subido de 100→500: CRÍTICO - no cortar final de frases
backend = "onnx"  # ONNX es 3-5x más rápido en CPU que PyTorch

[gemini]
model = "models/gemini-1.5-flash-latest"
temperature = 0.3
max_tokens = 2048
max_input_chars = 6000
request_timeout = 30
retry_attempts = 3
retry_min_wait = 2
retry_max_wait = 10
api_key = "${GEMINI_API_KEY}" # Loaded from environment variable

[notifications]
expire_time_ms = 3000  # tiempo en ms antes de auto-cerrar (3s default)
auto_dismiss = true    # forzar cierre programático via DBUS (para Unity/GNOME)

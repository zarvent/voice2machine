[paths]
# recording_flag, audio_file y log_file se generan dinámicamente en directorios seguros
# (/run/user/<uid>/v2m o /tmp/v2m_<uid>) para evitar vulnerabilidades de seguridad.
# Descomentar solo si se requiere una ubicación específica y segura.
# recording_flag = "/tmp/v2m_recording.pid"
# audio_file = "/tmp/v2m_audio.wav"
# log_file = "/tmp/v2m.log"
venv_path = "~/v2m/venv"

[whisper]
model = "large-v3-turbo"
language = "auto"
device = "cuda"
compute_type = "int8_float16" # Eficiencia pura: 4x menos VRAM, usa Tensor Cores
device_index = 0
num_workers = 2  # 2 workers suficientes para single stream

beam_size = 5  # SOTA: beam search para mayor precisión (~1.3x más lento)
best_of = 3    # Candidatos por paso (equilibrio velocidad/calidad)
temperature = 0.0  # Determinístico: requerido para que beam_size funcione
vad_filter = true  # Enabled: uses faster-whisper native VAD

[whisper.vad_parameters]
# VAD optimizado para precisión con beam search
threshold = 0.35              # Ligeramente más resistente a ruido
min_speech_duration_ms = 250  # Default: beam maneja mejor audio corto
min_silence_duration_ms = 600 # Mejor manejo de pausas naturales


# --- TRANSCRIPTION CONFIG (arquitectura model-agnostic) ---
[transcription]
backend = "whisper"  # selector de backend: "whisper", futuro: "vosk", "custom"

# Configuración específica de whisper (alias de [whisper] para backwards compat)
[transcription.whisper]
model = "large-v3-turbo"
language = "auto"
device = "cuda"
compute_type = "int8_float16"
device_index = 0
num_workers = 2
beam_size = 5
best_of = 3
temperature = 0.0
vad_filter = true


[gemini]
model = "models/gemini-1.5-flash-latest"
temperature = 0.3
max_tokens = 2048
max_input_chars = 6000
request_timeout = 30
retry_attempts = 3
retry_min_wait = 2
retry_max_wait = 10
api_key = "${GEMINI_API_KEY}" # Loaded from environment variable

[notifications]
expire_time_ms = 3000  # tiempo en ms antes de auto-cerrar (3s default)
auto_dismiss = true    # forzar cierre programático via DBUS (para Unity/GNOME)

[llm]
backend = "local"  # "local" para modelo local (Qwen + llama.cpp), "gemini" para API cloud

[llm.local]
model_path = "models/qwen2.5-3b-instruct-q4_k_m.gguf"
n_gpu_layers = -1   # -1 = todas las capas en GPU (full offload)
n_ctx = 2048        # context window en tokens
temperature = 0.3   # 0.0-2.0, más bajo = más determinístico
max_tokens = 512    # máximo de tokens a generar

#!/usr/bin/env python3
"""
Descarga y prueba del modelo Whisper

¬øQu√© hace este script?
    Descarga el modelo de transcripci√≥n (Whisper large-v2) y verifica
    que se cargue correctamente en tu GPU. Es lo primero que debes
    correr despu√©s de instalar V2M.

¬øPor qu√© tarda tanto la primera vez?
    El modelo pesa ~3 GB y se descarga de internet. La primera
    ejecuci√≥n puede tomar 5-10 minutos dependiendo de tu conexi√≥n.
    Las siguientes veces es instant√°neo porque ya est√° en cach√©.

¬øC√≥mo lo uso?
    $ python scripts/test_whisper_gpu.py

¬øQu√© deber√≠a ver?
    üöÄ Descargando modelo large-v2 (3GB, primera vez solamente)...
    ‚úÖ Modelo cargado exitosamente en GPU RTX 3060!
    üìä Info del modelo:
       - Dispositivo: CUDA (RTX 3060)
       - Precisi√≥n: float16
       - Memoria GPU disponible: ~6GB

¬øCu√°nta VRAM necesito?
    El modelo large-v2 necesita ~5-6 GB de VRAM. Funciona bien en:
    - RTX 3060 (12 GB) ‚úÖ
    - RTX 3070/3080 ‚úÖ
    - RTX 2060 (6 GB) - Justo, pero funciona
    - GTX 1060 (6 GB) - Muy justo

¬øQu√© hago si no tengo suficiente VRAM?
    Puedes usar un modelo m√°s peque√±o editando config.toml:
    - "medium" necesita ~3 GB
    - "small" necesita ~1 GB
    - "tiny" necesita ~500 MB (calidad m√°s baja)

Para desarrolladores:
    El modelo se guarda en ~/.cache/huggingface/hub/
    Para limpiar el cach√©: rm -rf ~/.cache/huggingface/hub/
"""

from faster_whisper import WhisperModel
import time


def load_whisper_model() -> WhisperModel:
    """
    Descarga y carga el modelo Whisper en GPU.

    Retorna el modelo listo para transcribir. La primera vez
    descarga ~3 GB, despu√©s usa el cach√©.
    """
    print("üöÄ Descargando modelo large-v2 (3GB, primera vez solamente)...")
    print("Esto puede tomar 5-10 minutos dependiendo de tu internet.\n")

    # Inicializar modelo con GPU
    model = WhisperModel(
        "large-v2",
        device="cuda",
        compute_type="float16",
        device_index=0  # RTX 3060
    )

    return model


def main() -> None:
    """
    Funci√≥n principal de prueba del modelo Whisper.

    Carga el modelo y muestra informaci√≥n sobre la configuraci√≥n
    y tiempos de transcripci√≥n estimados.
    """
    model = load_whisper_model()

    print("‚úÖ Modelo cargado exitosamente en GPU RTX 3060!")
    print("\nüìä Info del modelo:")
    print(f"   - Dispositivo: CUDA (RTX 3060)")
    print(f"   - Precisi√≥n: float16")
    print(f"   - Memoria GPU disponible: ~6GB\n")

    # Test r√°pido si tienes un archivo de audio
    print("El modelo est√° listo para usar.")
    print("Con la RTX 3060 se esperara aproximadamente:")
    print("   ‚Ä¢ 3-5 segundos por cada minuto de audio")
    print("   ‚Ä¢ 10-15 segundos para audio de 3 minutos")
    print("   ‚Ä¢ 30-40 segundos para audio de 10 minutos\n")


if __name__ == "__main__":
    main()

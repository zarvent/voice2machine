#!/usr/bin/env python3
"""
descarga y prueba del modelo whisper

¬øqu√© hace este script?
    descarga el modelo de transcripci√≥n (whisper large-v2) y verifica
    que se cargue correctamente en tu gpu es lo primero que debes
    correr despu√©s de instalar v2m

¬øpor qu√© tarda tanto la primera vez?
    el modelo pesa ~3 gb y se descarga de internet la primera
    ejecuci√≥n puede tomar 5-10 minutos dependiendo de tu conexi√≥n
    las siguientes veces es instant√°neo porque ya est√° en cach√©

¬øc√≥mo lo uso?
    $ python scripts/test_whisper_gpu.py

¬øqu√© deber√≠a ver?
    üöÄ Descargando modelo large-v2 (3GB, primera vez solamente)...
    ‚úÖ Modelo cargado exitosamente en GPU RTX 3060!
    üìä Info del modelo:
       - Dispositivo: CUDA (RTX 3060)
       - Precisi√≥n: float16
       - Memoria GPU disponible: ~6GB

¬øcu√°nta vram necesito?
    el modelo large-v2 necesita ~5-6 gb de vram funciona bien en
    - rtx 3060 (12 gb) ‚úÖ
    - rtx 3070/3080 ‚úÖ
    - rtx 2060 (6 gb) - justo pero funciona
    - gtx 1060 (6 gb) - muy justo

¬øqu√© hago si no tengo suficiente vram?
    puedes usar un modelo m√°s peque√±o editando config.toml
    - "medium" necesita ~3 gb
    - "small" necesita ~1 gb
    - "tiny" necesita ~500 mb (calidad m√°s baja)

para desarrolladores
    el modelo se guarda en ~/.cache/huggingface/hub/
    para limpiar el cach√© rm -rf ~/.cache/huggingface/hub/
"""

from faster_whisper import WhisperModel
import time


def load_whisper_model() -> WhisperModel:
    """
    descarga y carga el modelo whisper en gpu

    retorna el modelo listo para transcribir la primera vez
    descarga ~3 gb despu√©s usa el cach√©

    returns:
        el modelo whispermodel cargado
    """
    print("üöÄ Descargando modelo large-v2 (3GB, primera vez solamente)...")
    print("Esto puede tomar 5-10 minutos dependiendo de tu internet.\n")

    # Inicializar modelo con GPU
    model = WhisperModel(
        "large-v2",
        device="cuda",
        compute_type="float16",
        device_index=0  # RTX 3060
    )

    return model


def main() -> None:
    """
    funci√≥n principal de prueba del modelo whisper

    carga el modelo y muestra informaci√≥n sobre la configuraci√≥n
    y tiempos de transcripci√≥n estimados
    """
    model = load_whisper_model()

    print("‚úÖ Modelo cargado exitosamente en GPU RTX 3060!")
    print("\nüìä Info del modelo:")
    print(f"   - Dispositivo: CUDA (RTX 3060)")
    print(f"   - Precisi√≥n: float16")
    print(f"   - Memoria GPU disponible: ~6GB\n")

    # Test r√°pido si tienes un archivo de audio
    print("El modelo est√° listo para usar.")
    print("Con la RTX 3060 se esperara aproximadamente:")
    print("   ‚Ä¢ 3-5 segundos por cada minuto de audio")
    print("   ‚Ä¢ 10-15 segundos para audio de 3 minutos")
    print("   ‚Ä¢ 30-40 segundos para audio de 10 minutos\n")


if __name__ == "__main__":
    main()

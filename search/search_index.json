{"config":{"lang":["es","en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"\ud83d\udde3\ufe0f Voice2Machine: Dictado por Voz Local","text":""},{"location":"#proposito","title":"\ud83c\udfaf Prop\u00f3sito","text":"<p>El objetivo es simple:</p> <p>Poder dictar texto en cualquier lugar del sistema operativo.</p> <p>La idea es transcribir audio utilizando tu GPU local para obtener la m\u00e1xima velocidad y precisi\u00f3n, sin importar la aplicaci\u00f3n que est\u00e9s usando (editor de c\u00f3digo, navegador, chat, etc.).</p> <p>Este proyecto transforma un script simple en una aplicaci\u00f3n modular robusta basada en un Backend Daemon (Python), dise\u00f1ada bajo Arquitectura Hexagonal para garantizar mantenibilidad, escalabilidad y privacidad absoluta.</p>"},{"location":"#documentacion","title":"\ud83d\udcda Documentaci\u00f3n","text":"<p>La documentaci\u00f3n est\u00e1 organizada para servir a diferentes necesidades:</p>"},{"location":"#exploracion","title":"\ud83d\ude80 Exploraci\u00f3n","text":"<ul> <li>Gu\u00eda R\u00e1pida: Comienza a dictar en minutos.</li> <li>Glosario: Define t\u00e9rminos clave como Daemon, Whisper y API REST.</li> </ul>"},{"location":"#procedimientos","title":"\ud83d\udee0\ufe0f Procedimientos","text":"<ul> <li>Instalaci\u00f3n: Gu\u00eda paso a paso para Ubuntu/Debian.</li> <li>Contribuci\u00f3n: C\u00f3mo colaborar en el proyecto.</li> </ul>"},{"location":"#referencia","title":"\u2699\ufe0f Referencia","text":"<ul> <li>Configuraci\u00f3n: Ajusta modelos, dispositivos y comportamientos.</li> <li>Atajos de Teclado: Referencia de comandos globales.</li> <li>API REST: Documentaci\u00f3n de endpoints HTTP y WebSocket.</li> <li>API Python: Referencia de clases y m\u00e9todos del backend.</li> </ul>"},{"location":"#conceptos","title":"\ud83e\udde0 Conceptos","text":"<ul> <li>Arquitectura: Dise\u00f1o Hexagonal y componentes del sistema.</li> <li>Decisiones (ADR): Registro de decisiones t\u00e9cnicas importantes.</li> </ul>"},{"location":"#mantenimiento","title":"\ud83d\udd27 Mantenimiento","text":"<ul> <li>Soluci\u00f3n de Problemas: Diagn\u00f3stico y correcci\u00f3n de errores comunes.</li> <li>Changelog: Historial de cambios del proyecto.</li> </ul>"},{"location":"arquitectura/","title":"\ud83e\udde9 Arquitectura del Sistema","text":"<p>Filosof\u00eda T\u00e9cnica</p> <p>Voice2Machine implementa una Arquitectura Hexagonal (Ports &amp; Adapters) estricta, priorizando el desacoplamiento, la testabilidad y la independencia tecnol\u00f3gica. El sistema se adhiere a est\u00e1ndares SOTA 2026 como tipos est\u00e1ticos en Python (Protocol) y separaci\u00f3n Frontend/Backend mediante API REST.</p>"},{"location":"arquitectura/#diagrama-de-alto-nivel","title":"\ud83c\udfd7\ufe0f Diagrama de Alto Nivel","text":"<pre><code>graph TD\n    subgraph Clients [\"\ud83d\udd0c Clientes (CLI / Scripts / GUI / Tauri)\"]\n        ClientApp[\"Cualquier cliente HTTP\"]\n    end\n\n    subgraph Backend [\"\ud83d\udc0d Backend Daemon (Python + FastAPI)\"]\n        API[\"FastAPI Package&lt;br&gt;(api/)\"]\n\n        subgraph Workflows [\"\ud83e\udde0 Workflows (Orchestration)\"]\n            RecWF[\"RecordingWorkflow\"]\n            LLMWF[\"LLMWorkflow\"]\n        end\n\n        subgraph Features [\"\ud83e\udde9 Features (Domain + Logic)\"]\n            AudioFeat[\"Audio Service\"]\n            TranscFeat[\"Transcription Service\"]\n            LLMFeat[\"LLM Service\"]\n        end\n\n        subgraph Shared [\"\u2699\ufe0f Shared (Foundation)\"]\n            Config[\"Config\"]\n            Errors[\"Errors\"]\n            Interfaces[\"Interfaces\"]\n        end\n    end\n\n    ClientApp &lt;--&gt;|REST + WebSocket| API\n    API --&gt; Workflows\n    Workflows --&gt; Features\n    Features --&gt; Shared\n\n    style Clients fill:#e3f2fd,stroke:#1565c0\n    style Backend fill:#e8f5e9,stroke:#2e7d32\n    style Workflows fill:#fff3e0,stroke:#ef6c00\n    style Features fill:#f3e5f5,stroke:#7b1fa2\n    style Shared fill:#eceff1,stroke:#455a64</code></pre>"},{"location":"arquitectura/#componentes-del-backend","title":"\ud83d\udce6 Componentes del Backend","text":""},{"location":"arquitectura/#1-api-layer-fastapi","title":"1. API Layer (FastAPI)","text":"<p>Ubicado en <code>apps/daemon/backend/src/v2m/api/</code>.</p> <ul> <li>M\u00f3dulos: <code>app.py</code>, <code>routes/</code>, <code>schemas.py</code></li> <li>Endpoints REST: <code>/toggle</code>, <code>/start</code>, <code>/stop</code>, <code>/status</code>, <code>/health</code></li> <li>WebSocket: <code>/ws/events</code> para streaming de transcripci\u00f3n en tiempo real</li> <li>Documentaci\u00f3n autom\u00e1tica: Swagger UI en <code>/docs</code></li> </ul> <p>Estructura Moderna</p> <p>A partir de la v0.3.0, la API se organiza como un paquete completo, separando rutas y esquemas para mayor mantenibilidad.</p>"},{"location":"arquitectura/#2-workflows-orquestacion","title":"2. Workflows (Orquestaci\u00f3n)","text":"<p>Ubicado en <code>apps/daemon/backend/src/v2m/orchestration/</code>.</p> <p>En lugar de un Orchestrator monol\u00edtico, el sistema utiliza Workflows especializados para cada flujo de negocio:</p> <ul> <li>RecordingWorkflow: Gestiona el ciclo de vida de captura y transcripci\u00f3n.</li> <li>LLMWorkflow: Coordina el procesamiento de texto y traducci\u00f3n.</li> </ul> <p>Este enfoque permite que cada flujo evolucione de forma independiente sin afectar al resto del sistema.</p>"},{"location":"arquitectura/#3-features-dominios","title":"3. Features (Dominios)","text":"<p>Ubicado en <code>apps/daemon/backend/src/v2m/features/</code>.</p> <p>Cada carpeta en <code>features/</code> representa un dominio de conocimiento autocontenido que incluye sus propios servicios y l\u00f3gica:</p> Feature Responsabilidad transcription Implementaciones de Whisper (<code>faster-whisper</code>). audio Captura de audio y manejo del motor Rust (<code>v2m_engine</code>). llm Integraciones con Gemini, Ollama y otros proveedores."},{"location":"arquitectura/#4-shared-base-comun","title":"4. Shared (Base Com\u00fan)","text":"<p>Ubicado en <code>apps/daemon/backend/src/v2m/shared/</code>.</p> <ul> <li>Interfaces: Definiciones globales mediante <code>typing.Protocol</code>.</li> <li>Config: Gesti\u00f3n de <code>config.toml</code> mediante Pydantic Settings.</li> <li>Errors: Jerarqu\u00edas de excepciones compartidas.</li> </ul>"},{"location":"arquitectura/#comunicacion-cliente-backend","title":"\u26a1 Comunicaci\u00f3n Cliente-Backend","text":"<p>Voice2Machine utiliza FastAPI REST + WebSocket para la comunicaci\u00f3n:</p>"},{"location":"arquitectura/#rest-sincrono","title":"REST (S\u00edncrono)","text":"<pre><code># Toggle grabaci\u00f3n\ncurl -X POST http://localhost:8765/toggle | jq\n\n# Verificar estado\ncurl http://localhost:8765/status | jq\n</code></pre>"},{"location":"arquitectura/#websocket-streaming","title":"WebSocket (Streaming)","text":"<pre><code>const ws = new WebSocket(\"ws://localhost:8765/ws/events\");\nws.onmessage = (e) =&gt; {\n  const { event, data } = JSON.parse(e.data);\n  if (event === \"transcription_update\") {\n    console.log(data.text, data.final);\n  }\n};\n</code></pre>"},{"location":"arquitectura/#extensiones-nativas-rust","title":"\ud83e\udd80 Extensiones Nativas (Rust)","text":"<p>Para tareas cr\u00edticas donde el GIL de Python es un cuello de botella, utilizamos extensiones nativas compiladas en Rust (<code>v2m_engine</code>):</p> Componente Funci\u00f3n Audio I/O Escritura de WAVs directa a disco (zero-copy) VAD Detecci\u00f3n de voz de ultra-baja latencia (Silero ONNX) Buffer Ring Buffer circular lock-free para audio en tiempo real"},{"location":"arquitectura/#flujo-de-datos","title":"\ud83d\udd04 Flujo de Datos","text":"<pre><code>sequenceDiagram\n    participant User\n    participant Client as Cliente HTTP\n    participant API as FastAPI\n    participant WF as Workflows\n    participant Audio as AudioService\n    participant Whisper as TranscriptionService\n\n    User-&gt;&gt;Client: Presiona atajo\n    Client-&gt;&gt;API: POST /toggle\n    API-&gt;&gt;WF: toggle() (RecordingWorkflow)\n\n    alt No grabando\n        WF-&gt;&gt;Audio: start_recording()\n        Audio--&gt;&gt;WF: OK\n        WF--&gt;&gt;API: status=recording\n    else Grabando\n        WF-&gt;&gt;Audio: stop_recording()\n        Audio--&gt;&gt;WF: audio_buffer\n        WF-&gt;&gt;Whisper: transcribe(buffer)\n        Whisper--&gt;&gt;WF: texto\n        WF--&gt;&gt;API: status=idle, text=...\n    end\n\n    API--&gt;&gt;Client: ToggleResponse\n    Client-&gt;&gt;User: Copia al clipboard</code></pre>"},{"location":"arquitectura/#principios-de-diseno-2026","title":"\ud83d\udee1\ufe0f Principios de Dise\u00f1o 2026","text":"Principio Implementaci\u00f3n Local-First Ning\u00fan dato sale de la m\u00e1quina a menos que se configure expl\u00edcitamente un proveedor cloud Privacy-By-Design Audio procesado en memoria, archivos temporales eliminados despu\u00e9s de transcripci\u00f3n Resiliencia Recuperaci\u00f3n autom\u00e1tica de errores, reinicio de subsistemas si fallan Observabilidad Logging estructurado (OpenTelemetry), m\u00e9tricas en tiempo real Performance is Design FastAPI async, Rust para hot paths, modelo warm en VRAM"},{"location":"atajos_teclado/","title":"\u2328\ufe0f Atajos de Teclado y Scripts","text":"<p>Filosof\u00eda de Integraci\u00f3n</p> <p>Voice2Machine no secuestra tu teclado. Proporciona scripts \"at\u00f3micos\" que t\u00fa vinculas a tu gestor de ventanas favorito (GNOME, KDE, Hyprland, i3). Esto garantiza compatibilidad universal y cero consumo de recursos en segundo plano para escuchar teclas.</p>"},{"location":"atajos_teclado/#scripts-principales","title":"\ud83d\udd17 Scripts Principales","text":"<p>Para activar las funciones, debes crear atajos globales que ejecuten estos scripts ubicados en <code>scripts/</code>.</p>"},{"location":"atajos_teclado/#1-dictado-toggle","title":"1. Dictado (Toggle)","text":"<ul> <li>Script: <code>scripts/v2m-toggle.sh</code></li> <li>Funci\u00f3n: Interruptor de grabaci\u00f3n.</li> <li>Estado Inactivo: Inicia grabaci\u00f3n \ud83d\udd34 (Sonido de confirmaci\u00f3n).</li> <li>Estado Grabando: Detiene, transcribe y pega el texto \ud83d\udfe2.</li> <li>Atajo Sugerido: <code>Super + V</code> o bot\u00f3n lateral del mouse.</li> </ul>"},{"location":"atajos_teclado/#2-refinado-con-ia","title":"2. Refinado con IA","text":"<ul> <li>Script: <code>scripts/v2m-llm.sh</code></li> <li>Funci\u00f3n: Mejora de texto contextual.</li> <li>Lee el portapapeles actual.</li> <li>Env\u00eda el texto al proveedor LLM configurado (Gemini/Ollama).</li> <li>Reemplaza el portapapeles con la versi\u00f3n mejorada.</li> <li>Atajo Sugerido: <code>Super + G</code>.</li> </ul>"},{"location":"atajos_teclado/#ejemplos-de-configuracion","title":"\ud83d\udc27 Ejemplos de Configuraci\u00f3n","text":""},{"location":"atajos_teclado/#gnome-ubuntu","title":"GNOME / Ubuntu","text":"<ol> <li>Ve a Configuraci\u00f3n &gt; Teclado &gt; Atajos de teclado &gt; Ver y personalizar.</li> <li>Selecciona Atajos personalizados.</li> <li>A\u00f1ade nuevo:<ul> <li>Nombre: <code>V2M: Dictar</code></li> <li>Comando: <code>/home/tu_usuario/voice2machine/scripts/v2m-toggle.sh</code></li> <li>Atajo: <code>Super+V</code></li> </ul> </li> </ol>"},{"location":"atajos_teclado/#hyprland","title":"Hyprland","text":"<p>En tu <code>hyprland.conf</code>:</p> <pre><code>bind = SUPER, V, exec, /home/$USER/voice2machine/scripts/v2m-toggle.sh\nbind = SUPER, G, exec, /home/$USER/voice2machine/scripts/v2m-llm.sh\n</code></pre>"},{"location":"atajos_teclado/#i3-sway","title":"i3 / Sway","text":"<p>En tu <code>config</code>:</p> <pre><code>bindsym Mod4+v exec --no-startup-id /home/$USER/voice2machine/scripts/v2m-toggle.sh\nbindsym Mod4+g exec --no-startup-id /home/$USER/voice2machine/scripts/v2m-llm.sh\n</code></pre>"},{"location":"atajos_teclado/#solucion-de-problemas","title":"\u26a0\ufe0f Soluci\u00f3n de Problemas","text":"<p>Permisos de Ejecuci\u00f3n</p> <p>Si el atajo parece \"muerto\", verifica que los scripts tengan permiso de ejecuci\u00f3n: <code>bash     chmod +x scripts/v2m-toggle.sh scripts/v2m-llm.sh</code></p> <p>Wayland vs X11</p> <p>Los scripts detectan autom\u00e1ticamente tu servidor gr\u00e1fico. - X11: Usa <code>xclip</code> y <code>xdotool</code>. - Wayland: Usa <code>wl-copy</code> y <code>wtype</code> (aseg\u00farate de tenerlos instalados si usas Wayland puro).</p> <p>Latencia</p> <p>Estos scripts usan comunicaci\u00f3n por sockets crudos (raw sockets) para hablar con el demonio, asegurando una latencia de activaci\u00f3n &lt; 10ms. No inician una instancia de Python pesada cada vez.</p>"},{"location":"changelog/","title":"Changelog","text":"<p>title: Changelog description: Registro de cambios del proyecto Voice2Machine. ai_context: \"Versiones, Historial de Cambios, SemVer\" depends_on: [] status: stable</p>"},{"location":"changelog/#changelog","title":"Changelog","text":"<p>Todos los cambios notables en este proyecto ser\u00e1n documentados en este archivo.</p> <p>El formato se basa en Keep a Changelog, y este proyecto se adhiere a Semantic Versioning.</p>"},{"location":"changelog/#unreleased","title":"[Unreleased]","text":""},{"location":"changelog/#added","title":"Added","text":"<ul> <li>Motor de Audio Zero-Copy: Nueva implementaci\u00f3n de <code>ZeroCopyAudioRecorder</code> en Rust usando memoria compartida (/dev/shm) para transferencia de audio sin copias.</li> <li>Detecci\u00f3n de Alucinaciones: Filtros heur\u00edsticos y par\u00e1metros de calidad (<code>no_speech</code>, <code>compression_ratio</code>) en <code>StreamingTranscriber</code> para reducir salidas err\u00f3neas en Whisper.</li> <li>M\u00e9tricas de Rendimiento: Seguimiento de latencia de inferencia en logs para diagn\u00f3stico detallado.</li> </ul>"},{"location":"changelog/#changed","title":"Changed","text":"<ul> <li>Configuraci\u00f3n Whisper Avanzada: Incrementado <code>beam_size</code> y <code>best_of</code> a 5 para mejorar la calidad de transcripci\u00f3n en el modelo \"large-v3-turbo\".</li> <li>Optimizaci\u00f3n de VAD: Ajustado el umbral por defecto a 0.35 para reducir falsos positivos por ruido ambiental y respiraci\u00f3n, preservando mejor las vocales finales del espa\u00f1ol.</li> <li>Gesti\u00f3n de Memoria: Reinicio forzado de la cach\u00e9 de CUDA (<code>torch.cuda.empty_cache()</code>) al descargar modelos para liberar VRAM de forma efectiva.</li> <li>Higiene de C\u00f3digo: Refactorizaci\u00f3n de imports y correcci\u00f3n de errores de linting (<code>ruff</code>) en toda la base de c\u00f3digo del backend.</li> </ul>"},{"location":"changelog/#planned","title":"Planned","text":"<ul> <li>Soporte para m\u00faltiples idiomas de transcripci\u00f3n simult\u00e1neos</li> <li>Dashboard web para monitoreo en tiempo real</li> <li>Integraci\u00f3n con m\u00e1s proveedores LLM</li> </ul>"},{"location":"changelog/#030-2026-01-23","title":"[0.3.0] - 2026-01-23","text":""},{"location":"changelog/#added_1","title":"Added","text":"<ul> <li>Arquitectura Basada en Features: Reestructuraci\u00f3n total hacia m\u00f3dulos autocontenidos en <code>features/</code> (audio, llm, transcription).</li> <li>Orquestaci\u00f3n mediante Workflows: Introducci\u00f3n de <code>RecordingWorkflow</code> y <code>LLMWorkflow</code> para desacoplar la l\u00f3gica de negocio del antiguo Orchestrator monol\u00edtico.</li> <li>Protocolos Estrictos: Implementaci\u00f3n de <code>typing.Protocol</code> para todos los servicios internos, permitiendo swapping f\u00e1cil de proveedores.</li> <li>API Modular: Estructura de paquetes en <code>api/</code> con rutas y esquemas separados.</li> </ul>"},{"location":"changelog/#changed_1","title":"Changed","text":"<ul> <li>Eliminaci\u00f3n de Orchestrator: <code>services/orchestrator.py</code> ha sido descompuesto y eliminado.</li> <li>Refactorizaci\u00f3n de Infraestructura: La carpeta <code>infrastructure/</code> ha sido integrada dentro de cada <code>feature</code> correspondiente.</li> <li>Core y Domain: Simplificados y movidos a <code>shared/</code> e interfaces locales.</li> </ul>"},{"location":"changelog/#removed","title":"Removed","text":"<ul> <li>Test de Audio Legacy: Eliminaci\u00f3n de pruebas obsoletas de la extensi\u00f3n Rust.</li> <li>System Monitor: Telemetr\u00eda de sistema eliminada por simplificaci\u00f3n del core.</li> </ul>"},{"location":"changelog/#020-2025-01-20","title":"[0.2.0] - 2025-01-20","text":""},{"location":"changelog/#added_2","title":"Added","text":"<ul> <li>FastAPI REST API: Nueva API HTTP que reemplaza el sistema IPC basado en Unix Sockets</li> <li>WebSocket streaming: Endpoint <code>/ws/events</code> para transcripci\u00f3n provisional en tiempo real</li> <li>Documentaci\u00f3n Swagger: UI interactiva en <code>/docs</code> para probar endpoints</li> <li>Orchestrator pattern: Nuevo patr\u00f3n de coordinaci\u00f3n que simplifica el flujo de trabajo</li> <li>Rust audio engine: Extensi\u00f3n nativa <code>v2m_engine</code> para captura de audio de baja latencia</li> <li>Sistema de documentaci\u00f3n MkDocs: Documentaci\u00f3n estructurada con Material theme</li> </ul>"},{"location":"changelog/#changed_2","title":"Changed","text":"<ul> <li>Arquitectura simplificada: De CQRS/CommandBus a Orchestrator pattern m\u00e1s directo</li> <li>Comunicaci\u00f3n: De Unix Domain Sockets binarios a HTTP REST est\u00e1ndar</li> <li>Modelo de estado: Gesti\u00f3n centralizada en <code>DaemonState</code> con lazy initialization</li> <li>Actualizaci\u00f3n de README.md con nueva arquitectura</li> </ul>"},{"location":"changelog/#removed_1","title":"Removed","text":"<ul> <li><code>daemon.py</code>: Reemplazado por <code>api.py</code> (FastAPI)</li> <li><code>client.py</code>: Ya no necesario, usar <code>curl</code> o cualquier cliente HTTP</li> <li>Protocolo IPC binario: Reemplazado por JSON est\u00e1ndar</li> </ul>"},{"location":"changelog/#fixed","title":"Fixed","text":"<ul> <li>Latencia de arranque: El servidor inicia en ~100ms, modelo carga en background</li> <li>Memory leaks en WebSocket connections</li> </ul>"},{"location":"changelog/#010-2024-03-20","title":"[0.1.0] - 2024-03-20","text":""},{"location":"changelog/#added_3","title":"Added","text":"<ul> <li>Versi\u00f3n inicial del sistema Voice2Machine</li> <li>Soporte para transcripci\u00f3n local con Whisper (faster-whisper)</li> <li>Integraci\u00f3n b\u00e1sica con LLMs (Ollama/Gemini)</li> <li>Sistema IPC basado en Unix Domain Sockets</li> <li>Arquitectura hexagonal con puertos y adaptadores</li> <li>Configuraci\u00f3n mediante TOML</li> </ul>"},{"location":"configuracion/","title":"\u2699\ufe0f Gu\u00eda de Configuraci\u00f3n","text":"<p>Gesti\u00f3n de Configuraci\u00f3n</p> <p>La configuraci\u00f3n se gestiona principalmente a trav\u00e9s de la interfaz gr\u00e1fica del Frontend (Icono de engranaje \u2699\ufe0f). Sin embargo, los usuarios avanzados pueden editar directamente el archivo <code>config.toml</code>.</p> <p>Ubicaci\u00f3n del archivo: <code>$XDG_CONFIG_HOME/v2m/config.toml</code> (usualmente <code>~/.config/v2m/config.toml</code>).</p>"},{"location":"configuracion/#1-transcripcion-local-transcription","title":"1. Transcripci\u00f3n Local (<code>[transcription]</code>)","text":"<p>El coraz\u00f3n del sistema. Estos par\u00e1metros controlan el motor Faster-Whisper.</p> Par\u00e1metro Tipo Default Descripci\u00f3n y \"Best Practice\" 2026 <code>model</code> <code>str</code> <code>distil-large-v3</code> Modelo a cargar. <code>distil-large-v3</code> ofrece velocidad extrema con precisi\u00f3n SOTA. Opciones: <code>large-v3-turbo</code>, <code>medium</code>. <code>device</code> <code>str</code> <code>cuda</code> <code>cuda</code> (GPU NVIDIA) es mandatorio para experiencia en tiempo real. <code>cpu</code> es funcional pero no recomendado. <code>compute_type</code> <code>str</code> <code>float16</code> Precisi\u00f3n de tensores. <code>float16</code> o <code>int8_float16</code> optimizan VRAM y throughput en GPUs modernas. <code>use_faster_whisper</code> <code>bool</code> <code>true</code> Habilita el backend optimizado CTranslate2."},{"location":"configuracion/#deteccion-de-voz-vad","title":"Detecci\u00f3n de Voz (VAD)","text":"<p>El sistema utiliza Silero VAD (versi\u00f3n Rust en <code>v2m_engine</code>) para filtrar silencio antes de invocar a Whisper, ahorrando GPU.</p> <ul> <li><code>vad_filter</code> (<code>true</code>): Activa el pre-filtrado.</li> <li><code>vad_parameters</code>: Ajuste fino de sensibilidad.</li> <li><code>threshold</code> (<code>0.4</code>): Umbral de probabilidad. Valores m\u00e1s altos reducen falsos positivos por ruido o respiraci\u00f3n.</li> <li><code>min_speech_duration_ms</code> (<code>150</code>): Duraci\u00f3n m\u00ednima para considerar un segmento como voz.</li> <li><code>min_silence_duration_ms</code> (<code>1000</code>): Tiempo de silencio para cortar un segmento (ajustado para espa\u00f1ol).</li> </ul>"},{"location":"configuracion/#parametros-de-calidad-y-anti-alucinacion-sota-2026","title":"Par\u00e1metros de Calidad y Anti-Alucinaci\u00f3n (SOTA 2026)","text":"<p>Para evitar que Whisper genere texto repetitivo o \"alucine\" en entornos ruidosos, se incluyen par\u00e1metros de control de calidad:</p> Par\u00e1metro Default Descripci\u00f3n <code>no_speech_threshold</code> <code>0.6</code> Filtra segmentos con alta probabilidad de no contener habla. <code>compression_ratio_threshold</code> <code>2.4</code> Detecta y descarta transcripciones altamente repetitivas (alucinaciones). <code>log_prob_threshold</code> <code>-1.0</code> Umbral de confianza del modelo. Filtra transcripciones de baja probabilidad."},{"location":"configuracion/#2-servicios-llm-llm","title":"2. Servicios LLM (<code>[llm]</code>)","text":"<p>Voice2Machine implementa un patr\u00f3n de Proveedor para soportar m\u00faltiples backends de IA para el refinado de texto.</p>"},{"location":"configuracion/#configuracion-global","title":"Configuraci\u00f3n Global","text":"Par\u00e1metro Descripci\u00f3n <code>provider</code> Proveedor activo: <code>gemini</code> (Nube) u <code>ollama</code> (Local). <code>model</code> Nombre del modelo espec\u00edfico (ej. <code>gemini-1.5-flash</code> o <code>llama3:8b</code>)."},{"location":"configuracion/#proveedores-especificos","title":"Proveedores Espec\u00edficos","text":""},{"location":"configuracion/#google-gemini-provider-gemini","title":"Google Gemini (<code>provider = \"gemini\"</code>)","text":"<p>Requiere API Key. Ideal para usuarios sin GPU potente (VRAM &lt; 8GB).</p> <ul> <li>Modelo recomendado: <code>gemini-1.5-flash-latest</code> (latencia m\u00ednima).</li> <li>Temperatura: <code>0.3</code> (conservador) para correcci\u00f3n gramatical.</li> </ul>"},{"location":"configuracion/#ollama-provider-ollama","title":"Ollama (<code>provider = \"ollama\"</code>)","text":"<p>Privacidad total. Requiere correr el servidor de Ollama (<code>ollama serve</code>).</p> <ul> <li>Endpoint: <code>http://localhost:11434</code></li> <li>Modelo recomendado: <code>qwen2.5:7b</code> o <code>llama3.1:8b</code>.</li> </ul>"},{"location":"configuracion/#3-grabacion-recording","title":"3. Grabaci\u00f3n (<code>[recording]</code>)","text":"<p>Controla la captura de audio mediante <code>SoundDevice</code> y <code>v2m_engine</code>.</p> <ul> <li><code>sample_rate</code>: <code>16000</code> (Fijo, requerido por Whisper).</li> <li><code>channels</code>: <code>1</code> (Mono).</li> <li><code>device_index</code>: ID del micr\u00f3fono. Si es <code>null</code>, usa el default del sistema (PulseAudio/PipeWire).</li> </ul>"},{"location":"configuracion/#4-sistema-system","title":"4. Sistema (<code>[system]</code>)","text":"<p>Configuraci\u00f3n de bajo nivel para el Daemon y comunicaci\u00f3n.</p> <ul> <li><code>host</code>: Host del servidor (<code>127.0.0.1</code> para acceso solo local).</li> <li><code>port</code>: Puerto HTTP (<code>8765</code> por defecto).</li> <li><code>log_level</code>: <code>INFO</code> por defecto. Cambiar a <code>DEBUG</code> para diagn\u00f3sticos profundos.</li> </ul>"},{"location":"configuracion/#secretos-y-seguridad","title":"Secretos y Seguridad","text":"<p>Las claves de API se gestionan mediante variables de entorno o almacenamiento seguro, nunca en texto plano dentro de <code>config.toml</code> si es posible.</p> <pre><code># Definir en .env o entorno del sistema\nexport GEMINI_API_KEY=\"AIzaSy_TU_CLAVE_AQUI\"\n</code></pre> <p>Importante</p> <p>Reinicia el demonio (usando <code>scripts/operations/daemon/restart_daemon.sh</code>) despu\u00e9s de editar manualmente el archivo de configuraci\u00f3n para aplicar los cambios.</p>"},{"location":"contribucion/","title":"Contribuir","text":"<p>title: Gu\u00eda de Contribuci\u00f3n description: Instrucciones y est\u00e1ndares para colaborar en Voice2Machine. status: stable last_update: 2026-01-23 language: Native Latin American Spanish</p>"},{"location":"contribucion/#guia-de-contribucion","title":"\u2764\ufe0f Gu\u00eda de Contribuci\u00f3n","text":"<p>\u00a1Gracias por tu inter\u00e9s en contribuir a Voice2Machine! Este proyecto se construye sobre la colaboraci\u00f3n y el c\u00f3digo de calidad.</p> <p>Para mantener nuestros est\u00e1ndares \"State of the Art 2026\", seguimos reglas estrictas pero justas. Por favor, lee esto antes de enviar tu primer Pull Request.</p>"},{"location":"contribucion/#flujo-de-trabajo","title":"\ud83d\ude80 Flujo de Trabajo","text":"<ol> <li>Discusi\u00f3n Primero: Antes de escribir c\u00f3digo, abre un Issue para discutir el cambio. Esto evita trabajo duplicado o rechazos por desalineaci\u00f3n arquitect\u00f3nica.</li> <li>Fork &amp; Branch:<ul> <li>Haz fork del repositorio.</li> <li>Crea una rama descriptiva: <code>feat/nuevo-soporte-gpu</code> o <code>fix/error-transcripcion</code>.</li> </ul> </li> <li>Desarrollo Local: Sigue la gu\u00eda de Instalaci\u00f3n para configurar tu entorno de desarrollo.</li> </ol>"},{"location":"contribucion/#estandares-de-calidad","title":"\ud83d\udccf Est\u00e1ndares de Calidad","text":""},{"location":"contribucion/#codigo","title":"C\u00f3digo","text":"<ul> <li>Backend (Python):</li> <li>Tipado est\u00e1tico estricto (100% Type Hints).</li> <li>Linter: <code>ruff check src/ --fix</code>.</li> <li>Formateador: <code>ruff format src/</code>.</li> <li>Tests: <code>pytest</code> debe pasar al 100%.</li> <li>Frontend (Tauri/React):</li> <li>TypeScript estricto (no <code>any</code>).</li> <li>Linter: <code>npm run lint</code>.</li> <li>Componentes funcionales y Hooks.</li> </ul>"},{"location":"contribucion/#commits","title":"Commits","text":"<p>Usamos Conventional Commits. Tu mensaje de commit debe seguir este formato:</p> <pre><code>&lt;tipo&gt;(&lt;alcance&gt;): &lt;descripci\u00f3n corta&gt;\n\n[Cuerpo opcional detallado]\n</code></pre> <p>Tipos permitidos:</p> <ul> <li><code>feat</code>: Nueva funcionalidad.</li> <li><code>fix</code>: Correcci\u00f3n de bug.</li> <li><code>docs</code>: Solo documentaci\u00f3n.</li> <li><code>refactor</code>: Cambio de c\u00f3digo que no arregla bugs ni a\u00f1ade features.</li> <li><code>test</code>: A\u00f1adir o corregir tests.</li> <li><code>chore</code>: Mantenimiento, dependencias.</li> </ul> <p>Ejemplo:</p> <p><code>feat(whisper): upgrade to faster-whisper 1.0.0 for 20% speedup</code></p>"},{"location":"contribucion/#documentacion-docs-as-code","title":"Documentaci\u00f3n (Docs as Code)","text":"<p>Si cambias funcionalidad, debes actualizar la documentaci\u00f3n en <code>docs/docs/es/</code>.</p> <ul> <li>Verifica que <code>mkdocs serve</code> funcione localmente.</li> <li>Sigue la Gu\u00eda de Estilo.</li> </ul>"},{"location":"contribucion/#checklist-de-pull-request","title":"\u2705 Checklist de Pull Request","text":"<p>Antes de enviar tu PR:</p> <ul> <li> He ejecutado los tests locales y pasan.</li> <li> He lintado el c\u00f3digo (<code>ruff</code>, <code>eslint</code>).</li> <li> He actualizado la documentaci\u00f3n relevante.</li> <li> He a\u00f1adido una entrada al <code>CHANGELOG.md</code> (si aplica).</li> <li> Mi c\u00f3digo sigue la Arquitectura Hexagonal (sin imports cruzados prohibidos).</li> </ul> <p>Ayuda</p> <p>Si tienes dudas sobre arquitectura o dise\u00f1o, consulta los documentos en <code>docs/docs/es/adr/</code> o pregunta en el Issue correspondiente.</p>"},{"location":"glosario/","title":"Glosario","text":"<p>Este glosario define t\u00e9rminos t\u00e9cnicos y de dominio utilizados en Voice2Machine.</p>"},{"location":"glosario/#terminos-generales","title":"T\u00e9rminos Generales","text":""},{"location":"glosario/#local-first","title":"Local-First","text":"<p>Filosof\u00eda de dise\u00f1o donde los datos (audio, texto) se procesan y almacenan exclusivamente en el dispositivo del usuario, sin depender de la nube.</p>"},{"location":"glosario/#daemon","title":"Daemon","text":"<p>Proceso en segundo plano (escrito en Python) que gestiona la grabaci\u00f3n, transcripci\u00f3n y comunicaci\u00f3n con el frontend.</p>"},{"location":"glosario/#api-rest","title":"API REST","text":"<p>Mecanismo de comunicaci\u00f3n entre el Daemon (Python) y los clientes (scripts, frontends). Utilizamos FastAPI con endpoints HTTP est\u00e1ndar y WebSocket para eventos en tiempo real.</p>"},{"location":"glosario/#componentes-tecnicos","title":"Componentes T\u00e9cnicos","text":""},{"location":"glosario/#whisper","title":"Whisper","text":"<p>Modelo de reconocimiento de voz (ASR) desarrollado por OpenAI. Voice2Machine utiliza <code>faster-whisper</code>, una implementaci\u00f3n optimizada con CTranslate2.</p>"},{"location":"glosario/#workflows-flujos-de-trabajo","title":"Workflows (Flujos de Trabajo)","text":"<p>Componentes especializados de coordinaci\u00f3n que gestionan el ciclo de vida completo de una tarea espec\u00edfica (ej: <code>RecordingWorkflow</code>, <code>LLMWorkflow</code>). Reemplazan al antiguo \"Orchestrator\" monol\u00edtico para una mejor trazabilidad y mantenibilidad.</p>"},{"location":"glosario/#features-caracteristicas","title":"Features (Caracter\u00edsticas)","text":"<p>M\u00f3dulos autocontenidos que agrupan la l\u00f3gica de dominio y sus adaptadores de infraestructura (audio, llm, transcripci\u00f3n). Representan las capacidades core del sistema.</p>"},{"location":"glosario/#backendprovider","title":"BackendProvider","text":"<p>Componente del frontend (React Context) que gestiona la conexi\u00f3n con el Daemon y distribuye el estado a la UI.</p>"},{"location":"glosario/#telemetrycontext","title":"TelemetryContext","text":"<p>Sub-contexto de React optimizado para actualizaciones de alta frecuencia (m\u00e9tricas de GPU, niveles de audio) para evitar re-renderizados innecesarios de la UI principal.</p>"},{"location":"glosario/#arquitectura-modular","title":"Arquitectura Modular","text":"<p>Evoluci\u00f3n de la Arquitectura Hexagonal que organiza el c\u00f3digo en torno a m\u00f3dulos de negocio (Features) y flujos de ejecuci\u00f3n (Workflows), minimizando el acoplamiento y maximizando la claridad.</p>"},{"location":"guia_rapida/","title":"\ud83d\udd79\ufe0f Gu\u00eda R\u00e1pida","text":"<p>Resumen Ejecutivo</p> <p>Voice2Machine tiene dos superpoderes: Dictado (Voz \u2192 Texto) y Refinado (Texto \u2192 Mejor Texto).</p> <p>Esta gu\u00eda visual te ayuda a entender los flujos de trabajo principales para que seas productivo en minutos.</p>"},{"location":"guia_rapida/#1-flujo-de-dictado-voz-texto","title":"1. Flujo de Dictado (Voz \u2192 Texto)","text":"<p>Ideal para: Escribir correos, c\u00f3digo o mensajes r\u00e1pidos sin tocar el teclado.</p> <ol> <li>Foco: Haz clic en el campo de texto donde quieres escribir.</li> <li>Activa el atajo (Configurable, por defecto ejecutando <code>v2m-toggle.sh</code>). Escuchar\u00e1s un sonido de inicio \ud83d\udd14.</li> <li>Habla claramente. No te preocupes por ser un robot, habla natural.</li> <li>Pulsa el atajo de nuevo para detener. Escuchar\u00e1s un sonido de fin \ud83d\udd15.</li> <li>El texto se pegar\u00e1 autom\u00e1ticamente en tu campo activo (o quedar\u00e1 en el portapapeles si la auto-escritura est\u00e1 desactivada).</li> </ol> <pre><code>flowchart LR\n    A((\ud83c\udfa4 INICIO)) --&gt;|Grabar| B{Whisper Local}\n    B --&gt;|Transcribir| C[\ud83d\udccb Portapapeles / Pegado]\n\n    style A fill:#ff6b6b,stroke:#333,stroke-width:2px,color:white\n    style B fill:#feca57,stroke:#333,stroke-width:2px\n    style C fill:#48dbfb,stroke:#333,stroke-width:2px</code></pre>"},{"location":"guia_rapida/#2-flujo-de-refinado-texto-ia-texto","title":"2. Flujo de Refinado (Texto \u2192 IA \u2192 Texto)","text":"<p>Ideal para: Corregir gram\u00e1tica, traducir o dar formato profesional a un borrador sucio.</p> <ol> <li>Selecciona y Copia (<code>Ctrl + C</code>) el texto que quieres mejorar.</li> <li>Activa el atajo de IA (ejecutando <code>v2m-llm.sh</code>).</li> <li>Espera unos segundos (la IA est\u00e1 pensando \ud83e\udde0).</li> <li>El texto mejorado reemplazar\u00e1 el contenido de tu portapapeles.</li> <li>Pega (<code>Ctrl + V</code>) el resultado.</li> </ol> <pre><code>flowchart LR\n    A[\ud83d\udccb Texto Original] --&gt;|Copiar| B((\ud83e\udde0 ATAJO IA))\n    B --&gt;|Procesar| C{Local LLM / Gemini}\n    C --&gt;|Mejorar| D[\u2728 Texto Pulido]\n\n    style A fill:#c8d6e5,stroke:#333,stroke-width:2px\n    style B fill:#5f27cd,stroke:#333,stroke-width:2px,color:white\n    style C fill:#feca57,stroke:#333,stroke-width:2px\n    style D fill:#1dd1a1,stroke:#333,stroke-width:2px</code></pre>"},{"location":"guia_rapida/#consejos-pro","title":"\ud83d\udca1 Consejos Pro","text":"<p>!!! tip \"Mejora tu Precisi\u00f3n\" - Habla fluido: Whisper entiende mejor el contexto de frases completas que palabras sueltas. - Hardware: Un micr\u00f3fono con cancelaci\u00f3n de ruido mejora dr\u00e1sticamente los resultados. - Configuraci\u00f3n: Puedes ajustar la \"temperatura\" del LLM en la configuraci\u00f3n para hacerlo m\u00e1s creativo o m\u00e1s literal.</p> <p>Privacidad Garantizada</p> <p>El Dictado es 100% local (ejecutado en tu GPU). El Refinado puede ser local (Ollama) o nube (Gemini), t\u00fa tienes el control total en la configuraci\u00f3n.</p>"},{"location":"instalacion/","title":"\ud83d\udee0\ufe0f Instalaci\u00f3n y Configuraci\u00f3n","text":"<p>Prerrequisito</p> <p>Este proyecto est\u00e1 optimizado para Linux (Debian/Ubuntu). Estado del Arte 2026: Utilizamos aceleraci\u00f3n por hardware (CUDA) y un enfoque modular para garantizar privacidad y rendimiento.</p> <p>Esta gu\u00eda te llevar\u00e1 desde cero hasta un sistema de dictado completamente funcional en tu m\u00e1quina local.</p>"},{"location":"instalacion/#metodo-1-instalacion-automatica-recomendado","title":"\ud83d\ude80 M\u00e9todo 1: Instalaci\u00f3n Autom\u00e1tica (Recomendado)","text":"<p>Hemos creado un script que maneja todo el \"trabajo sucio\" por ti: verifica tu sistema, instala dependencias (apt), crea el entorno virtual (venv) y configura las credenciales.</p> <pre><code># Ejecutar desde la ra\u00edz del proyecto\n./apps/daemon/backend/scripts/setup/install.sh\n</code></pre> <p>Lo que hace este script:</p> <ol> <li>\ud83d\udce6 Instala librer\u00edas del sistema (<code>ffmpeg</code>, <code>xclip</code>, <code>pulseaudio-utils</code>).</li> <li>\ud83d\udc0d Crea un entorno Python aislado (<code>venv</code>).</li> <li>\u2699\ufe0f Instala las dependencias del proyecto (<code>faster-whisper</code>, <code>torch</code>).</li> <li>\ud83d\udd11 Te ayuda a configurar tu API Key de Gemini (opcional, para IA generativa).</li> <li>\ud83d\udda5\ufe0f Verifica si tienes una GPU NVIDIA compatible.</li> </ol>"},{"location":"instalacion/#metodo-2-instalacion-manual","title":"\ud83d\udee0\ufe0f M\u00e9todo 2: Instalaci\u00f3n Manual","text":"<p>Si prefieres tener el control total o el script autom\u00e1tico falla, sigue estos pasos.</p>"},{"location":"instalacion/#1-dependencias-del-sistema-system-level","title":"1. Dependencias del Sistema (System Level)","text":"<p>Necesitamos herramientas para manipular audio y el portapapeles a nivel del SO.</p> <pre><code>sudo apt update\nsudo apt install ffmpeg xclip pulseaudio-utils python3-venv build-essential python3-dev\n</code></pre>"},{"location":"instalacion/#2-entorno-python","title":"2. Entorno Python","text":"<p>Aislamos las librer\u00edas para evitar conflictos.</p> <pre><code># Navegar al directorio del backend\ncd apps/daemon/backend\n\n# Crear entorno virtual\npython3 -m venv venv\n\n# Activar entorno (\u00a1Haz esto cada vez que trabajes en el proyecto!)\nsource venv/bin/activate\n\n# Instalar dependencias\npip install -e .\n</code></pre>"},{"location":"instalacion/#3-configuracion-de-ia-opcional","title":"3. Configuraci\u00f3n de IA (Opcional)","text":"<p>Para usar las funciones de \"Refinado de Texto\" (reescritura con LLM), necesitas una API Key de Google Gemini.</p> <ol> <li>Consigue tu clave en Google AI Studio.</li> <li>Crea un archivo <code>.env</code> en la ra\u00edz:</li> </ol> <pre><code>echo 'GEMINI_API_KEY=\"tu_clave_api_aqui\"' &gt; .env\n</code></pre>"},{"location":"instalacion/#verificacion","title":"\u2705 Verificaci\u00f3n","text":"<p>Aseg\u00farate de que todo funciona antes de continuar.</p>"},{"location":"instalacion/#1-verificar-aceleracion-gpu","title":"1. Verificar Aceleraci\u00f3n GPU","text":"<p>Esto confirma que Whisper puede usar tu tarjeta gr\u00e1fica (esencial para velocidad).</p> <pre><code>python apps/daemon/backend/scripts/diagnostics/check_cuda.py\n</code></pre>"},{"location":"instalacion/#2-diagnostico-del-sistema","title":"2. Diagn\u00f3stico del Sistema","text":"<p>Verifica que el demonio y los servicios de audio est\u00e9n listos.</p> <pre><code>python apps/daemon/backend/scripts/diagnostics/health_check.py\n</code></pre>"},{"location":"instalacion/#siguientes-pasos","title":"\u23ed\ufe0f Siguientes Pasos","text":"<p>Una vez instalado, es hora de configurar c\u00f3mo interact\u00faas con la herramienta.</p> <ul> <li>Configuraci\u00f3n Detallada - Ajusta modelos y sensibilidad.</li> <li>Atajos de Teclado - Configura tus teclas m\u00e1gicas.</li> </ul>"},{"location":"referencia_api/","title":"Referencia de API REST","text":"<p>Esta secci\u00f3n documenta la API REST del Daemon Voice2Machine (v0.2.0+).</p> <p>Arquitectura Actualizada</p> <p>Voice2Machine utiliza FastAPI para la comunicaci\u00f3n cliente-servidor, reemplazando el sistema anterior basado en Unix Sockets IPC. Esto permite probar endpoints directamente con <code>curl</code> o cualquier cliente HTTP.</p>"},{"location":"referencia_api/#informacion-general","title":"Informaci\u00f3n General","text":"Propiedad Valor Base URL <code>http://localhost:8765</code> Protocolo HTTP/1.1 + WebSocket Formato JSON (UTF-8) Documentaci\u00f3n Interactiva <code>http://localhost:8765/docs</code> (Swagger UI)"},{"location":"referencia_api/#endpoints-rest","title":"Endpoints REST","text":""},{"location":"referencia_api/#post-toggle","title":"POST <code>/toggle</code>","text":"<p>Toggle de grabaci\u00f3n (iniciar/detener). Este es el endpoint principal que usa el atajo de teclado.</p> Request <p><code>bash     curl -X POST http://localhost:8765/toggle | jq</code></p> Response (Iniciando) <p><code>json     {       \"status\": \"recording\",       \"message\": \"Grabaci\u00f3n iniciada\",       \"text\": null     }</code></p> Response (Deteniendo) <p><code>json     {       \"status\": \"idle\",       \"message\": \"Transcripci\u00f3n completada\",       \"text\": \"El texto transcrito aparece aqu\u00ed...\"     }</code></p>"},{"location":"referencia_api/#post-start","title":"POST <code>/start</code>","text":"<p>Inicia grabaci\u00f3n expl\u00edcitamente. \u00datil cuando necesitas control separado de inicio/fin.</p> Request <p><code>bash     curl -X POST http://localhost:8765/start | jq</code></p> Response <p><code>json     {       \"status\": \"recording\",       \"message\": \"Grabaci\u00f3n iniciada\",       \"text\": null     }</code></p>"},{"location":"referencia_api/#post-stop","title":"POST <code>/stop</code>","text":"<p>Detiene grabaci\u00f3n y transcribe el audio capturado.</p> Request <p><code>bash     curl -X POST http://localhost:8765/stop | jq</code></p> Response <p><code>json     {       \"status\": \"idle\",       \"message\": \"Transcripci\u00f3n completada\",       \"text\": \"El texto transcrito aparece aqu\u00ed...\"     }</code></p>"},{"location":"referencia_api/#post-llmprocess","title":"POST <code>/llm/process</code>","text":"<p>Procesa texto con LLM (limpieza, puntuaci\u00f3n, formato). El backend se selecciona seg\u00fan <code>config.toml</code>.</p> Request <p><code>bash     curl -X POST http://localhost:8765/llm/process \\       -H \"Content-Type: application/json\" \\       -d '{\"text\": \"hola como estas espero que bien\"}' | jq</code></p> Response <p><code>json     {       \"text\": \"Hola, \u00bfc\u00f3mo est\u00e1s? Espero que bien.\",       \"backend\": \"gemini\"     }</code></p>"},{"location":"referencia_api/#post-llmtranslate","title":"POST <code>/llm/translate</code>","text":"<p>Traduce texto a otro idioma usando LLM.</p> Request <p><code>bash     curl -X POST http://localhost:8765/llm/translate \\       -H \"Content-Type: application/json\" \\       -d '{\"text\": \"Buenos d\u00edas\", \"target_lang\": \"en\"}' | jq</code></p> Response <p><code>json     {       \"text\": \"Good morning\",       \"backend\": \"gemini\"     }</code></p>"},{"location":"referencia_api/#get-status","title":"GET <code>/status</code>","text":"<p>Retorna el estado actual del daemon.</p> Request <p><code>bash     curl http://localhost:8765/status | jq</code></p> Response <p><code>json     {       \"state\": \"idle\",       \"recording\": false,       \"model_loaded\": true     }</code></p> <p>Estados posibles:</p> Estado Descripci\u00f3n <code>idle</code> Esperando comandos <code>recording</code> Grabando audio <code>processing</code> Transcribiendo o procesando con LLM"},{"location":"referencia_api/#get-health","title":"GET <code>/health</code>","text":"<p>Health check para systemd/scripts de monitoreo.</p> Request <p><code>bash     curl http://localhost:8765/health | jq</code></p> Response <p><code>json     {       \"status\": \"ok\",       \"version\": \"0.2.0\"     }</code></p>"},{"location":"referencia_api/#websocket","title":"WebSocket","text":""},{"location":"referencia_api/#ws-wsevents","title":"WS <code>/ws/events</code>","text":"<p>Stream de eventos en tiempo real. \u00datil para mostrar transcripci\u00f3n provisional mientras el usuario habla.</p> Conexi\u00f3n (JavaScript) <pre><code>const ws = new WebSocket('ws://localhost:8765/ws/events');\n\n    ws.onmessage = (event) =&gt; {\n      const { event: eventType, data } = JSON.parse(event.data);\n      console.log(`Evento: ${eventType}`, data);\n    };\n    ```\n\n=== \"Conexi\u00f3n (Python)\"\n```python\nimport asyncio\nimport websockets\n\n    async def listen():\n        async with websockets.connect('ws://localhost:8765/ws/events') as ws:\n            async for message in ws:\n                print(message)\n\n    asyncio.run(listen())\n    ```\n\n**Eventos emitidos:**\n\n| Evento                 | Campos                           | Descripci\u00f3n                                          |\n| ---------------------- | -------------------------------- | ---------------------------------------------------- |\n| `transcription_update` | `text: str`, `final: bool`       | Actualizaci\u00f3n de transcripci\u00f3n (provisional o final) |\n| `heartbeat`            | `timestamp: float`, `state: str` | Latido para mantener conexi\u00f3n viva                   |\n\n---\n\n## Modelos de Datos\n\n### ToggleResponse\n\n```python\nclass ToggleResponse(BaseModel):\n    status: str      # 'recording' | 'idle'\n    message: str     # Mensaje descriptivo\n    text: str | None # Texto transcrito (solo en stop)\n</code></pre>"},{"location":"referencia_api/#statusresponse","title":"StatusResponse","text":"<pre><code>class StatusResponse(BaseModel):\n    state: str        # 'idle' | 'recording' | 'processing'\n    recording: bool   # True si est\u00e1 grabando\n    model_loaded: bool # True si Whisper est\u00e1 en VRAM\n</code></pre>"},{"location":"referencia_api/#llmresponse","title":"LLMResponse","text":"<pre><code>class LLMResponse(BaseModel):\n    text: str    # Texto procesado/traducido\n    backend: str # 'gemini' | 'ollama' | 'local'\n</code></pre>"},{"location":"referencia_api/#codigos-de-error","title":"C\u00f3digos de Error","text":"C\u00f3digo HTTP Significado <code>200</code> Operaci\u00f3n exitosa <code>422</code> Error de validaci\u00f3n (payload inv\u00e1lido) <code>500</code> Error interno del servidor <p>Depuraci\u00f3n</p> <p>Usa la documentaci\u00f3n interactiva en <code>http://localhost:8765/docs</code> para probar endpoints visualmente.</p>"},{"location":"style_guide/","title":"Gu\u00eda de Estilo y Gobernanza","text":"<p>Esta gu\u00eda define los est\u00e1ndares para la documentaci\u00f3n de Voice2Machine, alineados con el \"Estado del Arte 2026\".</p>"},{"location":"style_guide/#principios-fundamentales","title":"Principios Fundamentales","text":"<ol> <li>Docs as Code: La documentaci\u00f3n vive en el repositorio, se versiona con Git y se valida en CI/CD.</li> <li>Accesibilidad Universal: Cumplimiento estricto de WCAG 2.1 Level AA.</li> <li>Localizaci\u00f3n: La fuente de la verdad (<code>docs/</code>) est\u00e1 en Espa\u00f1ol Latinoamericano Nativo. Los archivos ra\u00edz (<code>README.md</code>, <code>AGENTS.md</code>) est\u00e1n en Ingl\u00e9s (USA) y Espa\u00f1ol.</li> </ol>"},{"location":"style_guide/#accesibilidad-wcag-21-aa","title":"Accesibilidad (WCAG 2.1 AA)","text":"<ul> <li>Texto Alternativo: Todas las im\u00e1genes deben tener <code>alt text</code> descriptivo.</li> <li>Jerarqu\u00eda de Encabezados: No saltar niveles (H1 -&gt; H2 -&gt; H3).</li> <li>Contraste: Diagramas y capturas deben tener alto contraste.</li> <li>Enlaces: Usar texto descriptivo (\"ver gu\u00eda de instalaci\u00f3n\" en lugar de \"clic aqu\u00ed\").</li> </ul>"},{"location":"style_guide/#tono-y-voz","title":"Tono y Voz","text":"<ul> <li>Audiencia: Desarrolladores y usuarios t\u00e9cnicos.</li> <li>Tono: Profesional, conciso, directo (\"Haga esto\" en lugar de \"Podr\u00eda hacer esto\").</li> <li>Persona: Segunda persona (\"Configura tu entorno\") o impersonal (\"Se configura el entorno\").</li> <li>Espa\u00f1ol: Neutro/Latinoamericano. Evitar modismos locales excesivos.</li> </ul>"},{"location":"style_guide/#estructura-de-markdown","title":"Estructura de Markdown","text":""},{"location":"style_guide/#admonitions-notas","title":"Admonitions (Notas)","text":"<p>Usa bloques de admonition para resaltar informaci\u00f3n:</p> <pre><code>!!! note \"Nota\"\nInformaci\u00f3n neutral.\n\n!!! tip \"Consejo\"\nAyuda para optimizar.\n\n!!! warning \"Advertencia\"\nCuidado con esto.\n\n!!! danger \"Peligro\"\nRiesgo de p\u00e9rdida de datos.\n</code></pre>"},{"location":"style_guide/#codigo","title":"C\u00f3digo","text":"<p>Bloques de c\u00f3digo con lenguaje especificado:</p> <pre><code>def mi_funcion():\n    pass\n</code></pre>"},{"location":"style_guide/#proceso-de-gobernanza","title":"Proceso de Gobernanza","text":"<ol> <li>Cambios: Todo cambio de c\u00f3digo que afecte funcionalidad requiere actualizaci\u00f3n de docs en el mismo PR.</li> <li>Revisi\u00f3n: Los PRs de documentaci\u00f3n requieren revisi\u00f3n humana.</li> <li>Mantenimiento: Revisi\u00f3n trimestral de obsolescencia.</li> </ol>"},{"location":"troubleshooting/","title":"\ud83d\udd27 Soluci\u00f3n de Problemas (Troubleshooting)","text":"<p>Regla de Oro</p> <p>Ante cualquier problema, el primer paso siempre es consultar los logs del sistema. <code>bash     # Ver logs en tiempo real     tail -f ~/.local/state/v2m/v2m.log</code></p>"},{"location":"troubleshooting/#audio-y-grabacion","title":"\ud83d\uded1 Audio y Grabaci\u00f3n","text":""},{"location":"troubleshooting/#no-se-escucha-nada-transcripcion-vacia","title":"No se escucha nada / Transcripci\u00f3n vac\u00eda","text":"<ul> <li>S\u00edntoma: La grabaci\u00f3n inicia y termina, pero no se genera texto.</li> <li>Diagn\u00f3stico:   Ejecuta el script de diagn\u00f3stico de audio:   <pre><code>python scripts/diagnose_audio.py\n</code></pre></li> <li>Soluciones:</li> <li>Driver de Audio: Voice2Machine usa <code>SoundDevice</code>. Aseg\u00farate de que tu sistema (PulseAudio/PipeWire) tenga un micr\u00f3fono predeterminado activo.</li> <li>Permisos: En Linux, tu usuario debe pertenecer al grupo <code>audio</code> (<code>sudo usermod -aG audio $USER</code>).</li> </ul>"},{"location":"troubleshooting/#frases-cortadas-o-incompletas","title":"Frases cortadas o incompletas","text":"<ul> <li>Causa: El detector de silencio (VAD) es demasiado agresivo.</li> <li>Soluci\u00f3n:   Ajusta la configuraci\u00f3n en <code>config.toml</code> o desde la GUI:</li> <li>Reduce el <code>threshold</code> (ej. de <code>0.35</code> a <code>0.30</code>).</li> <li>Aumenta el <code>min_silence_duration_ms</code> (ej. a <code>800ms</code>).</li> </ul>"},{"location":"troubleshooting/#rendimiento-y-gpu","title":"\ud83d\udc22 Rendimiento y GPU","text":""},{"location":"troubleshooting/#transcripcion-lenta-2-segundos","title":"Transcripci\u00f3n lenta (&gt; 2 segundos)","text":"<ul> <li>Causa Probable: Whisper est\u00e1 ejecut\u00e1ndose en CPU en lugar de GPU.</li> <li>Verificaci\u00f3n:   <pre><code>python scripts/test_whisper_gpu.py\n</code></pre></li> <li>Soluci\u00f3n:</li> <li>Instala drivers NVIDIA actualizados (compatible con CUDA 12).</li> <li>Verifica que <code>config.toml</code> tenga <code>device = \"cuda\"</code>.</li> <li>Si no tienes GPU dedicada, cambia el modelo a <code>distil-medium.en</code> o <code>base</code>.</li> </ul>"},{"location":"troubleshooting/#error-cuda-out-of-memory","title":"Error <code>CUDA out of memory</code>","text":"<ul> <li>Causa: Tu GPU no tiene suficiente VRAM para el modelo seleccionado.</li> <li>Soluci\u00f3n:</li> <li>Cambia <code>compute_type</code> a <code>int8_float16</code> (reduce uso de VRAM a la mitad).</li> <li>Usa un modelo m\u00e1s ligero (<code>distil-large-v3</code> consume menos que <code>large-v3</code> original).</li> </ul>"},{"location":"troubleshooting/#conectividad-y-demonio","title":"\ud83d\udd0c Conectividad y Demonio","text":""},{"location":"troubleshooting/#connection-refused-en-gui-o-scripts","title":"\"Connection refused\" en GUI o Scripts","text":"<ul> <li>Causa: El proceso backend (Python) no est\u00e1 corriendo o el puerto est\u00e1 ocupado.</li> <li>Soluci\u00f3n:</li> <li>Verifica el estado:       <pre><code>pgrep -a python | grep v2m\n</code></pre></li> <li>Si no corre, in\u00edcialo manualmente para ver errores de arranque:       <pre><code>python -m v2m.main\n</code></pre></li> <li>Si dice \"Address already in use\", mata el proceso existente:       <pre><code>pkill -f \"v2m.main\"\n</code></pre></li> </ul>"},{"location":"troubleshooting/#atajos-de-teclado-no-responden","title":"Atajos de teclado no responden","text":"<ul> <li>Causa: Problema de permisos o ruta incorrecta en la configuraci\u00f3n del gestor de ventanas.</li> <li>Soluci\u00f3n:</li> <li>Ejecuta el script manualmente en terminal: <code>scripts/v2m-toggle.sh</code>.</li> <li>Si funciona, el error est\u00e1 en tu configuraci\u00f3n de atajos (ej. ruta relativa <code>~/</code> en lugar de <code>/home/...</code>).</li> <li>Si no funciona, verifica permisos: <code>chmod +x scripts/*.sh</code>.</li> </ul>"},{"location":"troubleshooting/#errores-de-ia-llm","title":"\ud83e\udde0 Errores de IA (LLM)","text":""},{"location":"troubleshooting/#error-401403-con-gemini","title":"Error 401/403 con Gemini","text":"<ul> <li>Causa: API Key inv\u00e1lida o expirada.</li> <li>Soluci\u00f3n: Regenera tu clave en Google AI Studio y actualiza el archivo <code>.env</code> o la variable de entorno <code>GEMINI_API_KEY</code>.</li> </ul>"},{"location":"troubleshooting/#connection-refused-con-ollama","title":"\"Connection refused\" con Ollama","text":"<ul> <li>Causa: El servidor de Ollama no est\u00e1 corriendo.</li> <li>Soluci\u00f3n: Ejecuta <code>ollama serve</code> en otra terminal.</li> </ul>"},{"location":"adr/","title":"Architecture Decision Records (ADRs)","text":"<p>Un Registro de Decisiones de Arquitectura (ADR) es un documento que captura una decisi\u00f3n de arquitectura importante, junto con su contexto y consecuencias.</p>"},{"location":"adr/#indice-de-decisiones","title":"\u00cdndice de Decisiones","text":"ADR T\u00edtulo Estado Fecha ADR-001 Migraci\u00f3n de IPC Unix Sockets a FastAPI REST Aceptada 2025-01 ADR-002 Reemplazo de CQRS/CommandBus por Orchestrator Aceptada 2025-01 ADR-003 Selecci\u00f3n de faster-whisper sobre whisper.cpp Aceptada 2024-06 ADR-004 Arquitectura Hexagonal (Puertos y Adaptadores) Aceptada 2024-03 ADR-005 Motor de Audio en Rust (v2m_engine) Aceptada 2025-01 ADR-006 Local-first: Procesamiento sin Cloud Aceptada 2024-03"},{"location":"adr/#cuando-escribir-un-adr","title":"\u00bfCu\u00e1ndo escribir un ADR?","text":"<p>Escribe un ADR cuando tomes una decisi\u00f3n significativa que afecte a la estructura, dependencias, interfaces o tecnolog\u00eda del proyecto.</p> <p>Ver Plantilla de ADR para el formato.</p>"},{"location":"adr/001-fastapi-migration/","title":"ADR-001: Migraci\u00f3n de IPC Unix Sockets a FastAPI REST","text":""},{"location":"adr/001-fastapi-migration/#estado","title":"Estado","text":"<p>Aceptada</p>"},{"location":"adr/001-fastapi-migration/#fecha","title":"Fecha","text":"<p>2025-01-15</p>"},{"location":"adr/001-fastapi-migration/#contexto","title":"Contexto","text":"<p>El sistema original de Voice2Machine utilizaba Unix Domain Sockets con un protocolo binario personalizado para la comunicaci\u00f3n entre el Daemon (Python) y los clientes (scripts, frontend Tauri).</p>"},{"location":"adr/001-fastapi-migration/#limitaciones-del-sistema-anterior","title":"Limitaciones del sistema anterior:","text":"<ol> <li>Complejidad de debugging: Los mensajes binarios requer\u00edan herramientas especializadas para inspecci\u00f3n</li> <li>Curva de aprendizaje: Los nuevos desarrolladores necesitaban entender el protocolo propietario</li> <li>Incompatibilidad con herramientas est\u00e1ndar: No se pod\u00eda usar <code>curl</code>, Postman, o navegadores para testing</li> <li>Mantenimiento del protocolo: Cada cambio requer\u00eda actualizar cliente y servidor sincronizadamente</li> <li>Documentaci\u00f3n interactiva: No hab\u00eda forma de generar docs autom\u00e1ticamente</li> </ol>"},{"location":"adr/001-fastapi-migration/#requisitos","title":"Requisitos:","text":"<ul> <li>Mantener latencia &lt; 50ms para operaciones cr\u00edticas (toggle)</li> <li>Permitir streaming de eventos en tiempo real</li> <li>Simplificar onboarding de nuevos desarrolladores</li> <li>Facilitar testing y debugging</li> </ul>"},{"location":"adr/001-fastapi-migration/#decision","title":"Decisi\u00f3n","text":"<p>Migrar a FastAPI como framework de API REST, reemplazando completamente el sistema IPC propietario.</p>"},{"location":"adr/001-fastapi-migration/#implementacion","title":"Implementaci\u00f3n:","text":"<ul> <li>FastAPI + Uvicorn: Servidor HTTP async con rendimiento comparable a Go/Rust</li> <li>WebSocket: Para streaming de eventos (transcripci\u00f3n provisional)</li> <li>Pydantic V2: Validaci\u00f3n autom\u00e1tica y generaci\u00f3n de OpenAPI schema</li> <li>Swagger UI: Documentaci\u00f3n interactiva en <code>/docs</code></li> </ul>"},{"location":"adr/001-fastapi-migration/#consecuencias","title":"Consecuencias","text":""},{"location":"adr/001-fastapi-migration/#positivas","title":"Positivas","text":"<ul> <li>\u2705 Debugging trivial: <code>curl -X POST localhost:8765/toggle | jq</code></li> <li>\u2705 Documentaci\u00f3n autom\u00e1tica: Swagger UI incluido sin esfuerzo adicional</li> <li>\u2705 Ecosistema est\u00e1ndar: Compatible con cualquier cliente HTTP</li> <li>\u2705 Testing simplificado: FastAPI TestClient para tests de integraci\u00f3n</li> <li>\u2705 Onboarding r\u00e1pido: Un Junior puede entender la API en minutos</li> </ul>"},{"location":"adr/001-fastapi-migration/#negativas","title":"Negativas","text":"<ul> <li>\u26a0\ufe0f Overhead HTTP: ~2-5ms adicionales vs sockets raw (aceptable)</li> <li>\u26a0\ufe0f Puerto expuesto: Requiere configuraci\u00f3n de firewall (mitigado con <code>127.0.0.1</code> only)</li> <li>\u26a0\ufe0f Dependencia adicional: FastAPI + Uvicorn (~2MB)</li> </ul>"},{"location":"adr/001-fastapi-migration/#alternativas-consideradas","title":"Alternativas Consideradas","text":""},{"location":"adr/001-fastapi-migration/#grpc","title":"gRPC","text":"<ul> <li>Rechazado: Requiere tooling adicional (protoc), curva de aprendizaje similar al IPC original, no hay Swagger UI nativo.</li> </ul>"},{"location":"adr/001-fastapi-migration/#graphql","title":"GraphQL","text":"<ul> <li>Rechazado: Overhead innecesario para operaciones simples RPC-style, mayor complejidad.</li> </ul>"},{"location":"adr/001-fastapi-migration/#mantener-unix-sockets","title":"Mantener Unix Sockets","text":"<ul> <li>Rechazado: No resolv\u00eda los problemas de debugging y onboarding.</li> </ul>"},{"location":"adr/001-fastapi-migration/#referencias","title":"Referencias","text":"<ul> <li>FastAPI Documentation</li> <li>Uvicorn Performance</li> </ul>"},{"location":"adr/002-orchestrator-pattern/","title":"ADR-002: Reemplazo de CQRS/CommandBus por Orchestrator","text":""},{"location":"adr/002-orchestrator-pattern/#estado","title":"Estado","text":"<p>Aceptada</p>"},{"location":"adr/002-orchestrator-pattern/#fecha","title":"Fecha","text":"<p>2025-01-15</p>"},{"location":"adr/002-orchestrator-pattern/#contexto","title":"Contexto","text":"<p>El backend original implementaba el patr\u00f3n CQRS (Command Query Responsibility Segregation) con un CommandBus para procesar acciones del usuario.</p>"},{"location":"adr/002-orchestrator-pattern/#problemas-identificados","title":"Problemas identificados:","text":"<ol> <li>Sobre-ingenier\u00eda: Para un sistema con ~10 comandos, el overhead de CQRS era desproporcionado</li> <li>Indirecci\u00f3n excesiva: Command \u2192 CommandBus \u2192 Handler \u2192 Result \u2192 Response</li> <li>Boilerplate: Cada nueva funcionalidad requer\u00eda crear Command DTO + Handler + registrar en bus</li> <li>Debugging complejo: Stack traces profundos oscurec\u00edan el flujo real</li> <li>Testing verbose: Mocks de CommandBus en cada test</li> </ol>"},{"location":"adr/002-orchestrator-pattern/#requisitos","title":"Requisitos:","text":"<ul> <li>Mantener separaci\u00f3n de concerns (no acoplar API a infraestructura)</li> <li>Simplificar el flujo de control</li> <li>Reducir boilerplate para nuevas features</li> <li>Facilitar testing y debugging</li> </ul>"},{"location":"adr/002-orchestrator-pattern/#decision","title":"Decisi\u00f3n","text":"<p>Reemplazar CQRS/CommandBus por un Orchestrator central que coordina el flujo de trabajo directamente.</p>"},{"location":"adr/002-orchestrator-pattern/#implementacion","title":"Implementaci\u00f3n:","text":"<pre><code>class Orchestrator:\n    async def toggle(self) -&gt; ToggleResponse: ...\n    async def start(self) -&gt; ToggleResponse: ...\n    async def stop(self) -&gt; ToggleResponse: ...\n</code></pre> <p>El Orchestrator:</p> <ul> <li>Expone m\u00e9todos directos para cada operaci\u00f3n</li> <li>Coordina adaptadores (AudioRecorder, WhisperAdapter, LLMProvider)</li> <li>Mantiene el estado del sistema</li> <li>Emite eventos a WebSocket clients</li> </ul>"},{"location":"adr/002-orchestrator-pattern/#consecuencias","title":"Consecuencias","text":""},{"location":"adr/002-orchestrator-pattern/#positivas","title":"Positivas","text":"<ul> <li>\u2705 Flujo expl\u00edcito: API \u2192 Orchestrator \u2192 Adapters (3 capas vs 6+)</li> <li>\u2705 Menos c\u00f3digo: Eliminamos ~500 LOC de CommandBus infrastructure</li> <li>\u2705 Debugging simple: Stack traces claros y cortos</li> <li>\u2705 Testing directo: Mockeamos adaptadores, no buses abstractos</li> <li>\u2705 Onboarding: Nuevos devs entienden el sistema en minutos</li> </ul>"},{"location":"adr/002-orchestrator-pattern/#negativas","title":"Negativas","text":"<ul> <li>\u26a0\ufe0f Menos extensible: Agregar un \"middleware\" global es menos trivial</li> <li>\u26a0\ufe0f Orchestrator \"god object\": Riesgo de que crezca demasiado (mitigado con composici\u00f3n)</li> </ul>"},{"location":"adr/002-orchestrator-pattern/#alternativas-consideradas","title":"Alternativas Consideradas","text":""},{"location":"adr/002-orchestrator-pattern/#mantener-cqrs-simplificado","title":"Mantener CQRS simplificado","text":"<ul> <li>Rechazado: Incluso simplificado, el overhead conceptual no se justificaba.</li> </ul>"},{"location":"adr/002-orchestrator-pattern/#event-sourcing","title":"Event Sourcing","text":"<ul> <li>Rechazado: Sobre-ingenier\u00eda a\u00fan mayor para el caso de uso actual.</li> </ul>"},{"location":"adr/002-orchestrator-pattern/#simple-functions-sin-clase","title":"Simple Functions (sin clase)","text":"<ul> <li>Rechazado: Perd\u00edamos gesti\u00f3n de estado y lifecycle.</li> </ul>"},{"location":"adr/002-orchestrator-pattern/#referencias","title":"Referencias","text":"<ul> <li>Martin Fowler on CQRS</li> <li>When not to use CQRS</li> </ul>"},{"location":"adr/003-faster-whisper/","title":"ADR-003: Selecci\u00f3n de faster-whisper sobre whisper.cpp","text":""},{"location":"adr/003-faster-whisper/#estado","title":"Estado","text":"<p>Aceptada</p>"},{"location":"adr/003-faster-whisper/#fecha","title":"Fecha","text":"<p>2024-06-01</p>"},{"location":"adr/003-faster-whisper/#contexto","title":"Contexto","text":"<p>Para la transcripci\u00f3n de voz local, necesit\u00e1bamos elegir una implementaci\u00f3n del modelo Whisper de OpenAI que maximizara rendimiento en GPUs NVIDIA consumer (RTX 3060-4090).</p>"},{"location":"adr/003-faster-whisper/#opciones-evaluadas","title":"Opciones evaluadas:","text":"<ol> <li>OpenAI Whisper (original): Implementaci\u00f3n de referencia en PyTorch</li> <li>whisper.cpp: Implementaci\u00f3n en C++ puro con soporte CUDA</li> <li>faster-whisper: Implementaci\u00f3n sobre CTranslate2 (C++/CUDA optimizado)</li> </ol>"},{"location":"adr/003-faster-whisper/#requisitos","title":"Requisitos:","text":"<ul> <li>Latencia &lt; 500ms para 5 segundos de audio (8x real-time m\u00ednimo)</li> <li>Soporte para modelos <code>large-v3</code> y variantes <code>distil</code></li> <li>Cuantizaci\u00f3n INT8/FP16 para optimizar VRAM</li> <li>API Python para integraci\u00f3n con el backend</li> <li>Streaming/chunking de audio</li> </ul>"},{"location":"adr/003-faster-whisper/#decision","title":"Decisi\u00f3n","text":"<p>Adoptar faster-whisper como motor de transcripci\u00f3n principal.</p>"},{"location":"adr/003-faster-whisper/#justificacion","title":"Justificaci\u00f3n:","text":"Criterio Whisper (PyTorch) whisper.cpp faster-whisper Velocidad 1x (baseline) 4x 4-8x VRAM (large-v3) 10GB 6GB 4-5GB Python API \u2705 Nativa \u274c Bindings \u2705 Excelente Cuantizaci\u00f3n Limited \u2705 \u2705 INT8/FP16 Mantenimiento OpenAI Comunidad Activo (Systran)"},{"location":"adr/003-faster-whisper/#consecuencias","title":"Consecuencias","text":""},{"location":"adr/003-faster-whisper/#positivas","title":"Positivas","text":"<ul> <li>\u2705 4-8x m\u00e1s r\u00e1pido que Whisper original con misma precisi\u00f3n</li> <li>\u2705 ~50% menos VRAM: Permite usar large-v3 en GPUs de 6GB</li> <li>\u2705 API Pythonica: Integraci\u00f3n natural con FastAPI async</li> <li>\u2705 Soporte distil models: <code>distil-large-v3</code> para latencia m\u00ednima</li> </ul>"},{"location":"adr/003-faster-whisper/#negativas","title":"Negativas","text":"<ul> <li>\u26a0\ufe0f Dependencia adicional: CTranslate2 binario (~100MB)</li> <li>\u26a0\ufe0f Menos portable: Requiere CUDA toolkit compatible</li> <li>\u26a0\ufe0f Lag en nuevos modelos: Nuevos releases de OpenAI tardan ~2 semanas en estar disponibles</li> </ul>"},{"location":"adr/003-faster-whisper/#alternativas-consideradas","title":"Alternativas Consideradas","text":""},{"location":"adr/003-faster-whisper/#whispercpp","title":"whisper.cpp","text":"<ul> <li>Rechazado: Bindings Python inmaduros, debugging m\u00e1s complejo.</li> </ul>"},{"location":"adr/003-faster-whisper/#openai-whisper","title":"OpenAI Whisper","text":"<ul> <li>Rechazado: Demasiado lento para experiencia real-time sin hardware enterprise.</li> </ul>"},{"location":"adr/003-faster-whisper/#whisper-jax","title":"Whisper JAX","text":"<ul> <li>Rechazado: Requiere TPU o configuraci\u00f3n compleja de JAX en CUDA.</li> </ul>"},{"location":"adr/003-faster-whisper/#referencias","title":"Referencias","text":"<ul> <li>faster-whisper GitHub</li> <li>CTranslate2</li> <li>Whisper Benchmarks</li> </ul>"},{"location":"adr/004-hexagonal-architecture/","title":"ADR-004: Arquitectura Hexagonal (Puertos y Adaptadores)","text":""},{"location":"adr/004-hexagonal-architecture/#estado","title":"Estado","text":"<p>Aceptada</p>"},{"location":"adr/004-hexagonal-architecture/#fecha","title":"Fecha","text":"<p>2024-03-01</p>"},{"location":"adr/004-hexagonal-architecture/#contexto","title":"Contexto","text":"<p>Voice2Machine comenz\u00f3 como un script monol\u00edtico de ~200 l\u00edneas. Al crecer en funcionalidad, enfrentamos problemas t\u00edpicos de c\u00f3digo acoplado:</p> <ol> <li>Testing dif\u00edcil: Mocks de GPU, audio, API externa</li> <li>Cambios cascada: Modificar Whisper requer\u00eda tocar 5+ archivos</li> <li>Vendor lock-in: Cambiar de Ollama a Gemini requer\u00eda refactor masivo</li> <li>Responsabilidades difusas: No estaba claro d\u00f3nde poner nueva l\u00f3gica</li> </ol>"},{"location":"adr/004-hexagonal-architecture/#requisitos","title":"Requisitos:","text":"<ul> <li>N\u00facleo de negocio agn\u00f3stico a frameworks</li> <li>Adaptadores intercambiables (ej: cambiar Whisper por otro ASR)</li> <li>Testabilidad sin hardware real</li> <li>Boundaries claros entre capas</li> </ul>"},{"location":"adr/004-hexagonal-architecture/#decision","title":"Decisi\u00f3n","text":"<p>Adoptar Arquitectura Hexagonal (Ports &amp; Adapters) como patr\u00f3n estructural.</p>"},{"location":"adr/004-hexagonal-architecture/#estructura-de-carpetas","title":"Estructura de carpetas:","text":"<pre><code>src/v2m/\n\u251c\u2500\u2500 core/           # Configuraci\u00f3n, logging, interfaces base\n\u251c\u2500\u2500 domain/         # Modelos, puertos (interfaces), errores\n\u251c\u2500\u2500 services/       # Orchestrator, coordinaci\u00f3n\n\u2514\u2500\u2500 infrastructure/ # Adaptadores (Whisper, Audio, LLM)\n</code></pre>"},{"location":"adr/004-hexagonal-architecture/#implementacion-de-puertos","title":"Implementaci\u00f3n de puertos:","text":"<pre><code>from typing import Protocol, runtime_checkable\n\n@runtime_checkable\nclass TranscriptionService(Protocol):\n    async def transcribe(self, audio: bytes) -&gt; str: ...\n</code></pre> <p>Los adaptadores implementan los puertos:</p> <pre><code>class WhisperAdapter:\n    async def transcribe(self, audio: bytes) -&gt; str:\n        # Implementaci\u00f3n concreta con faster-whisper\n</code></pre>"},{"location":"adr/004-hexagonal-architecture/#consecuencias","title":"Consecuencias","text":""},{"location":"adr/004-hexagonal-architecture/#positivas","title":"Positivas","text":"<ul> <li>\u2705 Testing aislado: Tests unitarios sin GPU ni red</li> <li>\u2705 Flexibilidad: Cambiar Gemini por Ollama es editar 1 archivo</li> <li>\u2705 Onboarding: Estructura predecible y documentada</li> <li>\u2705 Type safety: <code>Protocol</code> + mypy detecta incompatibilidades en compile time</li> </ul>"},{"location":"adr/004-hexagonal-architecture/#negativas","title":"Negativas","text":"<ul> <li>\u26a0\ufe0f M\u00e1s archivos: ~20 archivos vs ~5 del script original</li> <li>\u26a0\ufe0f Indirecci\u00f3n: Hay que navegar entre capas para entender flujo completo</li> <li>\u26a0\ufe0f Overhead inicial: Setup m\u00e1s complejo para features simples</li> </ul>"},{"location":"adr/004-hexagonal-architecture/#alternativas-consideradas","title":"Alternativas Consideradas","text":""},{"location":"adr/004-hexagonal-architecture/#clean-architecture-uncle-bob","title":"Clean Architecture (Uncle Bob)","text":"<ul> <li>Rechazado: Demasiadas capas (Entities, Use Cases, Interface Adapters, Frameworks) para el scope.</li> </ul>"},{"location":"adr/004-hexagonal-architecture/#mvcmvp","title":"MVC/MVP","text":"<ul> <li>Rechazado: Orientado a UI, no aplica bien a un daemon backend.</li> </ul>"},{"location":"adr/004-hexagonal-architecture/#simple-modules","title":"Simple Modules","text":"<ul> <li>Rechazado: En la pr\u00e1ctica volv\u00edamos al acoplamiento original.</li> </ul>"},{"location":"adr/004-hexagonal-architecture/#referencias","title":"Referencias","text":"<ul> <li>Alistair Cockburn - Hexagonal Architecture</li> <li>Netflix - Ready for changes with Hexagonal Architecture</li> </ul>"},{"location":"adr/005-rust-audio-engine/","title":"ADR-005: Motor de Audio en Rust (v2m_engine)","text":""},{"location":"adr/005-rust-audio-engine/#estado","title":"Estado","text":"<p>Aceptada</p>"},{"location":"adr/005-rust-audio-engine/#fecha","title":"Fecha","text":"<p>2025-01-10</p>"},{"location":"adr/005-rust-audio-engine/#contexto","title":"Contexto","text":"<p>La captura de audio en Python puro presentaba limitaciones cr\u00edticas para una experiencia de dictado real-time:</p>"},{"location":"adr/005-rust-audio-engine/#problemas-identificados","title":"Problemas identificados:","text":"<ol> <li>GIL blocking: La captura de audio compet\u00eda con transcripci\u00f3n por el GIL</li> <li>Latencia variable: Jitter de 10-50ms en buffering de audio</li> <li>Overhead de sounddevice: Callbacks Python a\u00f1ad\u00edan latencia</li> <li>VAD ineficiente: Silero VAD en Python procesaba samples con overhead</li> </ol>"},{"location":"adr/005-rust-audio-engine/#requisitos","title":"Requisitos:","text":"<ul> <li>Latencia de captura &lt; 10ms</li> <li>VAD pre-procesado antes de Python</li> <li>Buffer circular lock-free</li> <li>Zero-copy cuando sea posible</li> </ul>"},{"location":"adr/005-rust-audio-engine/#decision","title":"Decisi\u00f3n","text":"<p>Desarrollar extensi\u00f3n nativa en Rust (<code>v2m_engine</code>) para tareas cr\u00edticas de audio.</p>"},{"location":"adr/005-rust-audio-engine/#componentes","title":"Componentes:","text":"<pre><code>v2m_engine/\n\u251c\u2500\u2500 src/\n\u2502   \u251c\u2500\u2500 audio_capture.rs  # CPAL-based capture\n\u2502   \u251c\u2500\u2500 ring_buffer.rs    # Lock-free circular buffer\n\u2502   \u251c\u2500\u2500 vad.rs            # Silero ONNX inference\n\u2502   \u2514\u2500\u2500 lib.rs            # PyO3 bindings\n</code></pre>"},{"location":"adr/005-rust-audio-engine/#interfaz-python","title":"Interfaz Python:","text":"<pre><code>from v2m_engine import AudioCapture, VADProcessor\n\ncapture = AudioCapture(sample_rate=16000, buffer_size=4096)\nvad = VADProcessor(threshold=0.5)\n\nasync with capture.stream() as audio:\n    if vad.contains_speech(audio):\n        await transcriber.process(audio)\n</code></pre>"},{"location":"adr/005-rust-audio-engine/#consecuencias","title":"Consecuencias","text":""},{"location":"adr/005-rust-audio-engine/#positivas","title":"Positivas","text":"<ul> <li>\u2705 Latencia &lt; 5ms: CPAL + lock-free buffers</li> <li>\u2705 GIL-free: Audio thread independiente de Python</li> <li>\u2705 VAD eficiente: ONNX runtime nativo, 10x m\u00e1s r\u00e1pido</li> <li>\u2705 Zero-copy: NumPy arrays comparten memoria con Rust</li> </ul>"},{"location":"adr/005-rust-audio-engine/#negativas","title":"Negativas","text":"<ul> <li>\u26a0\ufe0f Complejidad de build: Requiere Rust toolchain + maturin</li> <li>\u26a0\ufe0f Debugging cross-language: Stack traces mixtos Python/Rust</li> <li>\u26a0\ufe0f Portabilidad: Binarios espec\u00edficos por plataforma</li> </ul>"},{"location":"adr/005-rust-audio-engine/#alternativas-consideradas","title":"Alternativas Consideradas","text":""},{"location":"adr/005-rust-audio-engine/#pure-python-sounddevice-numpy","title":"Pure Python (sounddevice + numpy)","text":"<ul> <li>Rechazado: GIL blocking y latencia inaceptable.</li> </ul>"},{"location":"adr/005-rust-audio-engine/#cython","title":"Cython","text":"<ul> <li>Rechazado: Todav\u00eda atado al GIL, beneficios limitados.</li> </ul>"},{"location":"adr/005-rust-audio-engine/#c-extension","title":"C++ Extension","text":"<ul> <li>Rechazado: Rust ofrece memory safety sin GC, mejor tooling (cargo).</li> </ul>"},{"location":"adr/005-rust-audio-engine/#referencias","title":"Referencias","text":"<ul> <li>PyO3 - Rust bindings for Python</li> <li>CPAL - Cross-platform Audio Library</li> <li>Lock-free Ring Buffer</li> </ul>"},{"location":"adr/006-local-first/","title":"ADR-006: Local-first: Procesamiento sin Cloud","text":""},{"location":"adr/006-local-first/#estado","title":"Estado","text":"<p>Aceptada</p>"},{"location":"adr/006-local-first/#fecha","title":"Fecha","text":"<p>2024-03-01</p>"},{"location":"adr/006-local-first/#contexto","title":"Contexto","text":"<p>Los servicios de dictado existentes (Google Speech-to-Text, Whisper API, Dragon) requieren enviar audio a servidores externos.</p>"},{"location":"adr/006-local-first/#problemas-con-cloud-based-dictation","title":"Problemas con cloud-based dictation:","text":"<ol> <li>Privacidad: Audio sensible (m\u00e9dico, legal, personal) sale de la m\u00e1quina</li> <li>Latencia de red: 100-500ms RTT adicionales</li> <li>Disponibilidad: Requiere conexi\u00f3n a internet</li> <li>Costos: APIs de transcripci\u00f3n cobran por minuto</li> <li>Rate limits: Throttling en uso intensivo</li> </ol>"},{"location":"adr/006-local-first/#requisitos-del-usuario","title":"Requisitos del usuario:","text":"<ul> <li>Privacidad absoluta: Ning\u00fan dato debe salir de la m\u00e1quina</li> <li>Funcionamiento offline: El sistema debe operar sin internet</li> <li>Latencia m\u00ednima: &lt; 500ms end-to-end</li> <li>Costo cero: Sin suscripciones ni pagos por uso</li> </ul>"},{"location":"adr/006-local-first/#decision","title":"Decisi\u00f3n","text":"<p>Adoptar filosof\u00eda \"Local-first\" donde todo el procesamiento de voz ocurre en el dispositivo del usuario.</p>"},{"location":"adr/006-local-first/#implementacion","title":"Implementaci\u00f3n:","text":"Componente Soluci\u00f3n Local Transcripci\u00f3n faster-whisper en GPU local LLM (opcional) Ollama con modelos locales Audio Procesado en memoria RAM Almacenamiento Solo archivos temporales, eliminados post-uso"},{"location":"adr/006-local-first/#excepciones-configurables","title":"Excepciones configurables:","text":"<p>El usuario puede optar-in a servicios cloud para el refinamiento de texto:</p> <ul> <li>Google Gemini API (para LLM)</li> <li>Pero nunca para el audio crudo</li> </ul>"},{"location":"adr/006-local-first/#consecuencias","title":"Consecuencias","text":""},{"location":"adr/006-local-first/#positivas","title":"Positivas","text":"<ul> <li>\u2705 Privacidad garantizada: Audio nunca sale del dispositivo</li> <li>\u2705 Sin latencia de red: Todo procesamiento local</li> <li>\u2705 Funciona offline: No requiere internet para dictar</li> <li>\u2705 Costo predecible: Solo hardware (GPU), sin suscripciones</li> <li>\u2705 Compliance: Compatible con regulaciones (HIPAA, GDPR)</li> </ul>"},{"location":"adr/006-local-first/#negativas","title":"Negativas","text":"<ul> <li>\u26a0\ufe0f Requiere GPU: Sin GPU NVIDIA, rendimiento degradado</li> <li>\u26a0\ufe0f Modelos locales LLM: Calidad inferior a GPT-4/Gemini Pro</li> <li>\u26a0\ufe0f Actualizaciones manuales: Modelos no se auto-actualizan</li> </ul>"},{"location":"adr/006-local-first/#alternativas-consideradas","title":"Alternativas Consideradas","text":""},{"location":"adr/006-local-first/#hybrid-local-stt-cloud-llm-default","title":"Hybrid (local STT + cloud LLM default)","text":"<ul> <li>Rechazado: Viola principio de privacidad-por-defecto.</li> </ul>"},{"location":"adr/006-local-first/#cloud-first-con-cache-local","title":"Cloud-first con cache local","text":"<ul> <li>Rechazado: Complejidad innecesaria, audio a\u00fan debe subirse.</li> </ul>"},{"location":"adr/006-local-first/#federated-learning","title":"Federated Learning","text":"<ul> <li>Rechazado: Sobre-ingenier\u00eda para el scope actual.</li> </ul>"},{"location":"adr/006-local-first/#referencias","title":"Referencias","text":"<ul> <li>Local-first Software</li> <li>Ink &amp; Switch - Seven Ideals</li> <li>GDPR and Voice Data</li> </ul>"},{"location":"adr/template/","title":"ADR-XXX: T\u00edtulo Corto de la Decisi\u00f3n","text":""},{"location":"adr/template/#estado","title":"Estado","text":"<p>[Propuesto | Aceptado | Rechazado | Obsoleto]</p>"},{"location":"adr/template/#contexto","title":"Contexto","text":"<p>Describe el contexto y el problema que estamos resolviendo. - \u00bfCu\u00e1l es la limitaci\u00f3n actual? - \u00bfQu\u00e9 requisitos t\u00e9cnicos o de negocio impulsan esto?</p>"},{"location":"adr/template/#decision","title":"Decisi\u00f3n","text":"<p>Describir la decisi\u00f3n tomada. - \"Usaremos X tecnolog\u00eda para Y componente...\"</p>"},{"location":"adr/template/#consecuencias","title":"Consecuencias","text":"<p>\u00bfQu\u00e9 se vuelve m\u00e1s f\u00e1cil o dif\u00edcil debido a este cambio?</p>"},{"location":"adr/template/#positivas","title":"Positivas","text":"<p>-</p>"},{"location":"adr/template/#negativas","title":"Negativas","text":"<p>-</p>"},{"location":"adr/template/#alternativas-consideradas","title":"Alternativas Consideradas","text":"<ul> <li>Opci\u00f3n A: Por qu\u00e9 se rechaz\u00f3.</li> <li>Opci\u00f3n B: Por qu\u00e9 se rechaz\u00f3.</li> </ul>"},{"location":"api/","title":"API Python - \u00cdndice","text":"<p>Esta secci\u00f3n proporciona documentaci\u00f3n auto-generada de las clases y funciones Python del backend de Voice2Machine.</p> <p>Generado con mkdocstrings</p> <p>Esta documentaci\u00f3n se extrae autom\u00e1ticamente de los docstrings del c\u00f3digo fuente. Para la versi\u00f3n m\u00e1s actualizada, consulta siempre el c\u00f3digo en <code>apps/daemon/backend/src/v2m/</code>.</p>"},{"location":"api/#modulos-principales","title":"M\u00f3dulos Principales","text":""},{"location":"api/#interfaces","title":"Interfaces","text":"<p>Protocolos y contratos que definen el comportamiento esperado de los adaptadores.</p>"},{"location":"api/#dominio","title":"Dominio","text":"<p>Modelos de dominio, puertos y tipos de error.</p>"},{"location":"api/#servicios","title":"Servicios","text":"<p>Servicios de aplicaci\u00f3n incluyendo el Orchestrator principal.</p>"},{"location":"api/#navegacion-rapida","title":"Navegaci\u00f3n R\u00e1pida","text":"Clase/Funci\u00f3n Descripci\u00f3n <code>Orchestrator</code> Coordinador central del flujo de trabajo <code>TranscriptionService</code> Puerto para servicios de transcripci\u00f3n <code>AudioRecorder</code> Interfaz para captura de audio <code>LLMProvider</code> Interfaz base para proveedores de IA"},{"location":"api/domain/","title":"Dominio","text":"<p>Esta p\u00e1gina documenta los modelos de dominio y tipos de datos del sistema.</p>"},{"location":"api/domain/#modelos-de-datos-pydantic-v2","title":"Modelos de Datos (Pydantic V2)","text":"<p>Los modelos de datos de Voice2Machine utilizan Pydantic V2 para validaci\u00f3n estricta y serializaci\u00f3n r\u00e1pida.</p>"},{"location":"api/domain/#modelos-de-api-schemas","title":"Modelos de API (Schemas)","text":"<p>Se encuentran en <code>v2m.api.schemas</code> y definen los contratos de entrada/salida para la API REST y WebSockets.</p> Clase Prop\u00f3sito <code>StatusResponse</code> Estado actual del daemon (idle, recording, etc) <code>ToggleResponse</code> Resultado de iniciar/detener grabaci\u00f3n <code>TranscriptionUpdate</code> Evento de streaming con texto provisional <code>LLMResponse</code> Resultado del procesamiento de texto"},{"location":"api/domain/#correctionresult","title":"CorrectionResult","text":"<p>Modelo de salida estructurada para refinamiento de texto (usado por los Workflows de LLM).</p> <pre><code>class CorrectionResult(BaseModel):\n    corrected_text: str = Field(description=\"Texto corregido\")\n    explanation: str | None = Field(default=None, description=\"Cambios realizados\")\n</code></pre>"},{"location":"api/domain/#excepciones","title":"Excepciones","text":"<p>El sistema utiliza una jerarqu\u00eda de excepciones basada en <code>ApplicationError</code>.</p> Excepci\u00f3n Contexto <code>AudioError</code> Fallos en el hardware o buffer de audio <code>TranscriptionError</code> Fallos en el modelo Whisper o VRAM <code>LLMProviderError</code> Error de conexi\u00f3n o cuota con Gemini/Ollama <code>ConfigError</code> Error de validaci\u00f3n en <code>config.toml</code>"},{"location":"api/interfaces/","title":"Protocolos y Contratos","text":"<p>Esta p\u00e1gina documenta los protocolos (interfaces) que definen los contratos del sistema en el \"Estado del Arte 2026\".</p>"},{"location":"api/interfaces/#workflows-protocolo","title":"Workflows (Protocolo)","text":"<p>Los flujos de trabajo siguen un protocolo est\u00e1ndar para asegurar que la API pueda interactuar con ellos de forma gen\u00e9rica.</p> <pre><code>from typing import Protocol, runtime_checkable\n\n@runtime_checkable\nclass Workflow(Protocol):\n    async def execute(self, *args, **kwargs):\n        \"\"\"Punto de entrada principal para la ejecuci\u00f3n del flujo.\"\"\"\n        ...\n</code></pre>"},{"location":"api/interfaces/#adaptadores-de-caracteristicas","title":"Adaptadores de Caracter\u00edsticas","text":"<p>Cada caracter\u00edstica (<code>Feature</code>) define su propia interfaz mediante protocolos de Python para permitir el intercambio de implementaciones (ej: cambiar <code>Gemini</code> por <code>Ollama</code>).</p>"},{"location":"api/interfaces/#llmprovider","title":"LLMProvider","text":"<pre><code>class LLMProvider(Protocol):\n    async def process(self, text: str, system_prompt: str) -&gt; str:\n        \"\"\"Procesa texto usando un modelo de lenguaje.\"\"\"\n</code></pre>"},{"location":"api/interfaces/#audiosource","title":"AudioSource","text":"<pre><code>class AudioSource(Protocol):\n    def read(self, frames: int) -&gt; np.ndarray:\n        \"\"\"Lee frames de audio del buffer.\"\"\"\n</code></pre>"},{"location":"api/services/","title":"Servicios y L\u00f3gica de Aplicaci\u00f3n","text":"<p>En el modelo 2026, la l\u00f3gica de aplicaci\u00f3n se divide en Flujos de Trabajo (Workflows) y Caracter\u00edsticas (Features).</p>"},{"location":"api/services/#flujos-de-trabajo-workflows","title":"Flujos de Trabajo (Workflows)","text":"<p>Ubicados en <code>v2m.orchestration</code>. Son los coordinadores de alto nivel.</p> <ul> <li>RecordingWorkflow: Gestiona el ciclo audio -&gt; VAD -&gt; transcripci\u00f3n -&gt; pegado.</li> <li>LLMWorkflow: Gestiona el ciclo texto -&gt; refinamiento/traducci\u00f3n -&gt; pegado.</li> </ul>"},{"location":"api/services/#caracteristicas-features","title":"Caracter\u00edsticas (Features)","text":"<p>Ubicados en <code>v2m.features</code>. Son los bloques de construcci\u00f3n funcionales del sistema.</p> Caracter\u00edstica Prop\u00f3sito Implementaci\u00f3n Principal Audio Captura y pre-procesamiento <code>v2m_engine</code> (Rust) Transcription Conversi\u00f3n audio-a-texto <code>faster-whisper</code> LLM Procesamiento inteligente <code>gemini</code> / <code>ollama</code>"},{"location":"api/services/#inicializacion-lazy","title":"Inicializaci\u00f3n Lazy","text":"<p>Para minimizar el consumo de recursos (especialmente VRAM), los servicios pesados se inicializan solo al ser requeridos por primera vez.</p>"},{"location":"api/backend/","title":"API Backend (Python)","text":"<p>Esta secci\u00f3n contiene documentaci\u00f3n generada autom\u00e1ticamente desde el c\u00f3digo fuente del backend de Voice2Machine.</p> <p>Auto-generada</p> <p>Esta documentaci\u00f3n se sincroniza autom\u00e1ticamente con los docstrings del c\u00f3digo. La fuente de verdad es: <code>apps/daemon/backend/src/v2m/</code></p>"},{"location":"api/backend/#modulos-principales","title":"M\u00f3dulos Principales","text":""},{"location":"api/backend/#orquestacion","title":"Orquestaci\u00f3n","text":"<ul> <li>Workflows - Coordinadores de flujos de negocio (Recording, LLM)</li> <li>API REST - Endpoints FastAPI y modelos de datos (Paquete <code>api/</code>)</li> </ul>"},{"location":"api/backend/#cimientos","title":"Cimientos","text":"<ul> <li>Config - Sistema de configuraci\u00f3n tipada (<code>shared/config/</code>)</li> </ul>"},{"location":"api/backend/#funcionalidades-features","title":"Funcionalidades (Features)","text":"<ul> <li>Transcripci\u00f3n - Whisper y motores de inferencia</li> <li>LLM Services - Gemini, Ollama y Providers locales</li> </ul>"},{"location":"api/backend/#navegacion-por-capas","title":"Navegaci\u00f3n por Capas","text":"<pre><code>graph TD\n    A[API REST] --&gt; B[Workflows]\n    B --&gt; C[Features]\n    C --&gt; D[Audio]\n    C --&gt; E[Transcription]\n    C --&gt; F[LLM]\n\n    style A fill:#e3f2fd\n    style B fill:#fff3e0\n    style C fill:#f3e5f5</code></pre> Capa Responsabilidad API Endpoints HTTP, validaci\u00f3n, serializaci\u00f3n Orchestration Coordinaci\u00f3n de flujos de trabajo (Workflows) Features L\u00f3gica de dominio y adaptadores especializados"},{"location":"api/backend/#estado-del-codigo","title":"Estado del C\u00f3digo","text":"M\u00e9trica Valor Archivos Python 27 Cobertura docstrings ~70% Estilo Google Style"},{"location":"api/backend/api/","title":"API REST (Backend)","text":"<p>Documentaci\u00f3n de los endpoints FastAPI y modelos de datos.</p>"},{"location":"api/backend/api/#modelos-de-requestresponse-schemas","title":"Modelos de Request/Response (Schemas)","text":"<p>options: show_source: false</p> <p>options: show_source: false</p> <p>options: show_source: false</p> <p>options: show_source: false</p> <p>options: show_source: false</p>"},{"location":"api/backend/api/#v2m.api.schemas.ToggleResponse","title":"<code>v2m.api.schemas.ToggleResponse</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Respuesta del endpoint /toggle.</p> Source code in <code>apps/daemon/backend/src/v2m/api/schemas.py</code> <pre><code>class ToggleResponse(BaseModel):\n    \"\"\"Respuesta del endpoint /toggle.\"\"\"\n\n    status: str = Field(description=\"Estado actual: 'recording' o 'idle'\")\n    message: str = Field(description=\"Mensaje descriptivo para el usuario\")\n    text: str | None = Field(default=None, description=\"Texto transcrito (solo en stop)\")\n</code></pre>"},{"location":"api/backend/api/#v2m.api.schemas.StatusResponse","title":"<code>v2m.api.schemas.StatusResponse</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Respuesta del endpoint /status.</p> Source code in <code>apps/daemon/backend/src/v2m/api/schemas.py</code> <pre><code>class StatusResponse(BaseModel):\n    \"\"\"Respuesta del endpoint /status.\"\"\"\n\n    state: str = Field(description=\"Estado del daemon: 'idle', 'recording', 'processing'\")\n    recording: bool = Field(description=\"True si est\u00e1 grabando actualmente\")\n    model_loaded: bool = Field(description=\"True si el modelo Whisper est\u00e1 cargado\")\n</code></pre>"},{"location":"api/backend/api/#v2m.api.schemas.LLMResponse","title":"<code>v2m.api.schemas.LLMResponse</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Respuesta de endpoints LLM.</p> Source code in <code>apps/daemon/backend/src/v2m/api/schemas.py</code> <pre><code>class LLMResponse(BaseModel):\n    \"\"\"Respuesta de endpoints LLM.\"\"\"\n\n    text: str = Field(description=\"Texto procesado/traducido\")\n    backend: str = Field(description=\"Backend usado: 'gemini', 'ollama', 'local'\")\n</code></pre>"},{"location":"api/backend/api/#v2m.api.schemas.ProcessTextRequest","title":"<code>v2m.api.schemas.ProcessTextRequest</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Request para /llm/process.</p> Source code in <code>apps/daemon/backend/src/v2m/api/schemas.py</code> <pre><code>class ProcessTextRequest(BaseModel):\n    \"\"\"Request para /llm/process.\"\"\"\n\n    text: str = Field(min_length=1, max_length=10000, description=\"Texto a procesar\")\n</code></pre>"},{"location":"api/backend/api/#v2m.api.schemas.TranslateTextRequest","title":"<code>v2m.api.schemas.TranslateTextRequest</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Request para /llm/translate.</p> Source code in <code>apps/daemon/backend/src/v2m/api/schemas.py</code> <pre><code>class TranslateTextRequest(BaseModel):\n    \"\"\"Request para /llm/translate.\"\"\"\n\n    text: str = Field(min_length=1, max_length=10000, description=\"Texto a traducir\")\n    target_lang: str = Field(default=\"en\", description=\"Idioma destino (ej. 'en', 'es')\")\n</code></pre>"},{"location":"api/backend/api/#estado-global","title":"Estado Global","text":"<p>options: show_source: true members: - init - recording - llm - broadcast_event</p>"},{"location":"api/backend/api/#v2m.api.app.DaemonState","title":"<code>v2m.api.app.DaemonState</code>","text":"<p>Estado global del daemon (Singleton para la API).</p> Source code in <code>apps/daemon/backend/src/v2m/api/app.py</code> <pre><code>class DaemonState:\n    \"\"\"Estado global del daemon (Singleton para la API).\"\"\"\n\n    def __init__(self) -&gt; None:\n        \"\"\"Inicializa los componentes del estado global.\"\"\"\n        self._recording_workflow: RecordingWorkflow | None = None\n        self._llm_workflow: LLMWorkflow | None = None\n        self._websocket_clients: set[WebSocket] = set()\n\n    @property\n    def recording(self) -&gt; RecordingWorkflow:\n        \"\"\"Obtiene la instancia perezosa del workflow de grabaci\u00f3n.\"\"\"\n        if self._recording_workflow is None:\n            self._recording_workflow = RecordingWorkflow(broadcast_fn=self.broadcast_event)\n        return self._recording_workflow\n\n    @property\n    def llm(self) -&gt; LLMWorkflow:\n        \"\"\"Obtiene la instancia perezosa del workflow de LLM.\"\"\"\n        if self._llm_workflow is None:\n            self._llm_workflow = LLMWorkflow()\n        return self._llm_workflow\n\n    async def broadcast_event(self, event_type: str, data: dict[str, Any]) -&gt; None:\n        \"\"\"Emite un evento a todos los clientes WebSocket conectados.\"\"\"\n        if not self._websocket_clients:\n            return\n\n        message = {\"event\": event_type, \"data\": data}\n        disconnected: list[WebSocket] = []\n\n        for ws in self._websocket_clients:\n            try:\n                await ws.send_json(message)\n            except Exception:\n                disconnected.append(ws)\n\n        for ws in disconnected:\n            self._websocket_clients.discard(ws)\n</code></pre>"},{"location":"api/backend/api/#v2m.api.app.DaemonState.llm","title":"<code>llm</code>  <code>property</code>","text":"<p>Obtiene la instancia perezosa del workflow de LLM.</p>"},{"location":"api/backend/api/#v2m.api.app.DaemonState.recording","title":"<code>recording</code>  <code>property</code>","text":"<p>Obtiene la instancia perezosa del workflow de grabaci\u00f3n.</p>"},{"location":"api/backend/api/#v2m.api.app.DaemonState.__init__","title":"<code>__init__()</code>","text":"<p>Inicializa los componentes del estado global.</p> Source code in <code>apps/daemon/backend/src/v2m/api/app.py</code> <pre><code>def __init__(self) -&gt; None:\n    \"\"\"Inicializa los componentes del estado global.\"\"\"\n    self._recording_workflow: RecordingWorkflow | None = None\n    self._llm_workflow: LLMWorkflow | None = None\n    self._websocket_clients: set[WebSocket] = set()\n</code></pre>"},{"location":"api/backend/api/#v2m.api.app.DaemonState.broadcast_event","title":"<code>broadcast_event(event_type, data)</code>  <code>async</code>","text":"<p>Emite un evento a todos los clientes WebSocket conectados.</p> Source code in <code>apps/daemon/backend/src/v2m/api/app.py</code> <pre><code>async def broadcast_event(self, event_type: str, data: dict[str, Any]) -&gt; None:\n    \"\"\"Emite un evento a todos los clientes WebSocket conectados.\"\"\"\n    if not self._websocket_clients:\n        return\n\n    message = {\"event\": event_type, \"data\": data}\n    disconnected: list[WebSocket] = []\n\n    for ws in self._websocket_clients:\n        try:\n            await ws.send_json(message)\n        except Exception:\n            disconnected.append(ws)\n\n    for ws in disconnected:\n        self._websocket_clients.discard(ws)\n</code></pre>"},{"location":"api/backend/api/#endpoints","title":"Endpoints","text":""},{"location":"api/backend/api/#grabacion-recording","title":"Grabaci\u00f3n (Recording)","text":"<p>options: show_source: true</p> <p>options: show_source: true</p> <p>options: show_source: true</p>"},{"location":"api/backend/api/#v2m.api.routes.recording.toggle_recording","title":"<code>v2m.api.routes.recording.toggle_recording()</code>  <code>async</code>","text":"<p>Inicia o detiene la grabaci\u00f3n dependiendo del estado actual.</p> Source code in <code>apps/daemon/backend/src/v2m/api/routes/recording.py</code> <pre><code>@router.post(\"/toggle\", response_model=ToggleResponse)\nasync def toggle_recording() -&gt; ToggleResponse:\n    \"\"\"Inicia o detiene la grabaci\u00f3n dependiendo del estado actual.\"\"\"\n    return await state.recording.toggle()\n</code></pre>"},{"location":"api/backend/api/#v2m.api.routes.recording.start_recording","title":"<code>v2m.api.routes.recording.start_recording()</code>  <code>async</code>","text":"<p>Inicia expl\u00edcitamente la grabaci\u00f3n.</p> Source code in <code>apps/daemon/backend/src/v2m/api/routes/recording.py</code> <pre><code>@router.post(\"/start\", response_model=ToggleResponse)\nasync def start_recording() -&gt; ToggleResponse:\n    \"\"\"Inicia expl\u00edcitamente la grabaci\u00f3n.\"\"\"\n    return await state.recording.start()\n</code></pre>"},{"location":"api/backend/api/#v2m.api.routes.recording.stop_recording","title":"<code>v2m.api.routes.recording.stop_recording()</code>  <code>async</code>","text":"<p>Detiene la grabaci\u00f3n y realiza la transcripci\u00f3n final.</p> Source code in <code>apps/daemon/backend/src/v2m/api/routes/recording.py</code> <pre><code>@router.post(\"/stop\", response_model=ToggleResponse)\nasync def stop_recording() -&gt; ToggleResponse:\n    \"\"\"Detiene la grabaci\u00f3n y realiza la transcripci\u00f3n final.\"\"\"\n    return await state.recording.stop()\n</code></pre>"},{"location":"api/backend/api/#estado-status","title":"Estado (Status)","text":"<p>options: show_source: true</p> <p>options: show_source: true</p>"},{"location":"api/backend/api/#v2m.api.routes.status.get_status","title":"<code>v2m.api.routes.status.get_status()</code>  <code>async</code>","text":"<p>Obtiene el estado actual del daemon (grabaci\u00f3n, modelo cargado, etc.).</p> Source code in <code>apps/daemon/backend/src/v2m/api/routes/status.py</code> <pre><code>@router.get(\"/status\", response_model=StatusResponse)\nasync def get_status() -&gt; StatusResponse:\n    \"\"\"Obtiene el estado actual del daemon (grabaci\u00f3n, modelo cargado, etc.).\"\"\"\n    return state.recording.get_status()\n</code></pre>"},{"location":"api/backend/api/#v2m.api.routes.status.health_check","title":"<code>v2m.api.routes.status.health_check()</code>  <code>async</code>","text":"<p>Verifica la salud del servicio API.</p> Source code in <code>apps/daemon/backend/src/v2m/api/routes/status.py</code> <pre><code>@router.get(\"/health\", response_model=HealthResponse)\nasync def health_check() -&gt; HealthResponse:\n    \"\"\"Verifica la salud del servicio API.\"\"\"\n    return HealthResponse()\n</code></pre>"},{"location":"api/backend/api/#llm","title":"LLM","text":"<p>options: show_source: true</p> <p>options: show_source: true</p>"},{"location":"api/backend/api/#v2m.api.routes.llm.process_text","title":"<code>v2m.api.routes.llm.process_text(request)</code>  <code>async</code>","text":"<p>Procesa texto con el LLM (refinamiento, correcci\u00f3n, etc.).</p> Source code in <code>apps/daemon/backend/src/v2m/api/routes/llm.py</code> <pre><code>@router.post(\"/process\", response_model=LLMResponse)\nasync def process_text(request: ProcessTextRequest) -&gt; LLMResponse:\n    \"\"\"Procesa texto con el LLM (refinamiento, correcci\u00f3n, etc.).\"\"\"\n    return await state.llm.process_text(request.text)\n</code></pre>"},{"location":"api/backend/api/#v2m.api.routes.llm.translate_text","title":"<code>v2m.api.routes.llm.translate_text(request)</code>  <code>async</code>","text":"<p>Traduce texto al idioma destino especificado.</p> Source code in <code>apps/daemon/backend/src/v2m/api/routes/llm.py</code> <pre><code>@router.post(\"/translate\", response_model=LLMResponse)\nasync def translate_text(request: TranslateTextRequest) -&gt; LLMResponse:\n    \"\"\"Traduce texto al idioma destino especificado.\"\"\"\n    return await state.llm.translate_text(request.text, request.target_lang)\n</code></pre>"},{"location":"api/backend/config/","title":"Configuraci\u00f3n","text":"<p>Sistema de configuraci\u00f3n tipada usando Pydantic Settings.</p>"},{"location":"api/backend/config/#settings-principal","title":"Settings Principal","text":"<p>options: show_source: false members: - paths - transcription - llm - notifications</p>"},{"location":"api/backend/config/#v2m.shared.config.Settings","title":"<code>v2m.shared.config.Settings</code>","text":"<p>               Bases: <code>BaseSettings</code></p> <p>Configuraci\u00f3n Principal de la Aplicaci\u00f3n.</p> <p>Agrega todas las secciones de configuraci\u00f3n utilizando Pydantic Settings.</p> Atributos <p>paths: Configuraci\u00f3n de rutas. transcription: Configuraci\u00f3n de transcripci\u00f3n. gemini: Configuraci\u00f3n de Gemini LLM. notifications: Configuraci\u00f3n de notificaciones. llm: Configuraci\u00f3n de LLM.</p> Source code in <code>apps/daemon/backend/src/v2m/shared/config/__init__.py</code> <pre><code>class Settings(BaseSettings):\n    \"\"\"Configuraci\u00f3n Principal de la Aplicaci\u00f3n.\n\n    Agrega todas las secciones de configuraci\u00f3n utilizando Pydantic Settings.\n\n    Atributos:\n        paths: Configuraci\u00f3n de rutas.\n        transcription: Configuraci\u00f3n de transcripci\u00f3n.\n        gemini: Configuraci\u00f3n de Gemini LLM.\n        notifications: Configuraci\u00f3n de notificaciones.\n        llm: Configuraci\u00f3n de LLM.\n    \"\"\"\n\n    paths: PathsConfig = Field(default_factory=PathsConfig)\n    # whisper field removed in favor of transcription.whisper\n    gemini: GeminiConfig = Field(default_factory=GeminiConfig)\n    notifications: NotificationsConfig = Field(default_factory=NotificationsConfig)\n    llm: LLMConfig = Field(default_factory=LLMConfig)\n    transcription: TranscriptionConfig = Field(default_factory=TranscriptionConfig)\n\n    model_config = SettingsConfigDict(\n        env_file=\".env\",\n        env_file_encoding=\"utf-8\",\n        extra=\"ignore\",\n        toml_file=BASE_DIR / \"config.toml\",\n        frozen=True,\n    )\n\n    @classmethod\n    def settings_customise_sources(\n        cls,\n        settings_cls: type[BaseSettings],\n        init_settings: PydanticBaseSettingsSource,\n        env_settings: PydanticBaseSettingsSource,\n        dotenv_settings: PydanticBaseSettingsSource,\n        file_secret_settings: PydanticBaseSettingsSource,\n    ) -&gt; tuple[PydanticBaseSettingsSource, ...]:\n        \"\"\"Personaliza la prioridad de las fuentes de configuraci\u00f3n.\"\"\"\n        return (\n            init_settings,\n            env_settings,\n            dotenv_settings,\n            TomlConfigSettingsSource(settings_cls),\n            file_secret_settings,\n        )\n</code></pre>"},{"location":"api/backend/config/#v2m.shared.config.Settings.settings_customise_sources","title":"<code>settings_customise_sources(settings_cls, init_settings, env_settings, dotenv_settings, file_secret_settings)</code>  <code>classmethod</code>","text":"<p>Personaliza la prioridad de las fuentes de configuraci\u00f3n.</p> Source code in <code>apps/daemon/backend/src/v2m/shared/config/__init__.py</code> <pre><code>@classmethod\ndef settings_customise_sources(\n    cls,\n    settings_cls: type[BaseSettings],\n    init_settings: PydanticBaseSettingsSource,\n    env_settings: PydanticBaseSettingsSource,\n    dotenv_settings: PydanticBaseSettingsSource,\n    file_secret_settings: PydanticBaseSettingsSource,\n) -&gt; tuple[PydanticBaseSettingsSource, ...]:\n    \"\"\"Personaliza la prioridad de las fuentes de configuraci\u00f3n.\"\"\"\n    return (\n        init_settings,\n        env_settings,\n        dotenv_settings,\n        TomlConfigSettingsSource(settings_cls),\n        file_secret_settings,\n    )\n</code></pre>"},{"location":"api/backend/config/#configuracion-de-rutas","title":"Configuraci\u00f3n de Rutas","text":"<p>options: show_source: false</p>"},{"location":"api/backend/config/#v2m.shared.config.PathsConfig","title":"<code>v2m.shared.config.PathsConfig</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuraci\u00f3n para rutas de archivos y directorios.</p> Atributos <p>recording_flag: Ruta al archivo PID que indica grabaci\u00f3n activa. audio_file: Ruta al archivo WAV temporal para audio grabado. log_file: Ruta al archivo de log para depuraci\u00f3n. venv_path: Ruta al entorno virtual de Python.</p> Source code in <code>apps/daemon/backend/src/v2m/shared/config/__init__.py</code> <pre><code>class PathsConfig(BaseModel):\n    \"\"\"Configuraci\u00f3n para rutas de archivos y directorios.\n\n    Atributos:\n        recording_flag: Ruta al archivo PID que indica grabaci\u00f3n activa.\n        audio_file: Ruta al archivo WAV temporal para audio grabado.\n        log_file: Ruta al archivo de log para depuraci\u00f3n.\n        venv_path: Ruta al entorno virtual de Python.\n    \"\"\"\n\n    recording_flag: Path = Field(default=RUNTIME_DIR / \"v2m_recording.pid\")\n    audio_file: Path = Field(default=RUNTIME_DIR / \"v2m_audio.wav\")\n    log_file: Path = Field(default=RUNTIME_DIR / \"v2m_debug.log\")\n    venv_path: Path = Field(default=BASE_DIR / \"venv\")\n</code></pre>"},{"location":"api/backend/config/#configuracion-de-transcripcion","title":"Configuraci\u00f3n de Transcripci\u00f3n","text":"<p>options: show_source: false</p> <p>options: show_source: false</p> <p>options: show_source: false</p>"},{"location":"api/backend/config/#v2m.shared.config.TranscriptionConfig","title":"<code>v2m.shared.config.TranscriptionConfig</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuraci\u00f3n del Servicio de Transcripci\u00f3n.</p> Atributos <p>backend: Selector de backend (\"whisper\"). Defecto: \"whisper\" whisper: Configuraci\u00f3n para el backend Whisper.</p> Source code in <code>apps/daemon/backend/src/v2m/shared/config/__init__.py</code> <pre><code>class TranscriptionConfig(BaseModel):\n    \"\"\"Configuraci\u00f3n del Servicio de Transcripci\u00f3n.\n\n    Atributos:\n        backend: Selector de backend (\"whisper\"). Defecto: \"whisper\"\n        whisper: Configuraci\u00f3n para el backend Whisper.\n    \"\"\"\n\n    backend: str = Field(default=\"whisper\")\n    whisper: WhisperConfig = Field(default_factory=WhisperConfig)\n</code></pre>"},{"location":"api/backend/config/#v2m.shared.config.WhisperConfig","title":"<code>v2m.shared.config.WhisperConfig</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuraci\u00f3n del modelo de transcripci\u00f3n Whisper.</p> Atributos <p>model: Nombre o ruta del modelo Whisper (ej. 'tiny', 'base', 'large-v3').     Defecto: 'large-v2' language: C\u00f3digo de idioma ISO 639-1 (ej. 'es', 'en') o 'auto'.     Defecto: 'es' device: Dispositivo de c\u00f3mputo ('cuda' para GPU, 'cpu').     Defecto: 'cuda' compute_type: Precisi\u00f3n num\u00e9rica ('float16', 'int8_float16', 'int8').     Defecto: 'int8_float16' device_index: \u00cdndice de GPU a utilizar. Defecto: 0 num_workers: N\u00famero de workers para procesamiento paralelo. Defecto: 4 beam_size: Tama\u00f1o del beam search. Defecto: 5 (SOTA 2026 - \u00f3ptimo para large-v3-turbo) best_of: N\u00famero de candidatos a considerar. Defecto: 5 temperature: Temperatura de muestreo (0.0 para determin\u00edstico).     Defecto: 0.0 vad_filter: Activar filtrado VAD. Defecto: True vad_parameters: Configuraci\u00f3n detallada del VAD. audio_device_index: \u00cdndice del dispositivo de entrada de audio (None para defecto). no_speech_threshold: Umbral de probabilidad de no-habla (0.0-1.0).     Valores altos filtran m\u00e1s segmentos sin habla. Defecto: 0.6 compression_ratio_threshold: Umbral de ratio de compresi\u00f3n para detectar     salidas repetitivas/alucinaciones. Defecto: 2.4 log_prob_threshold: Umbral de log-probabilidad para filtrar     transcripciones de baja confianza. Defecto: -1.0</p> Source code in <code>apps/daemon/backend/src/v2m/shared/config/__init__.py</code> <pre><code>class WhisperConfig(BaseModel):\n    \"\"\"Configuraci\u00f3n del modelo de transcripci\u00f3n Whisper.\n\n    Atributos:\n        model: Nombre o ruta del modelo Whisper (ej. 'tiny', 'base', 'large-v3').\n            Defecto: 'large-v2'\n        language: C\u00f3digo de idioma ISO 639-1 (ej. 'es', 'en') o 'auto'.\n            Defecto: 'es'\n        device: Dispositivo de c\u00f3mputo ('cuda' para GPU, 'cpu').\n            Defecto: 'cuda'\n        compute_type: Precisi\u00f3n num\u00e9rica ('float16', 'int8_float16', 'int8').\n            Defecto: 'int8_float16'\n        device_index: \u00cdndice de GPU a utilizar. Defecto: 0\n        num_workers: N\u00famero de workers para procesamiento paralelo. Defecto: 4\n        beam_size: Tama\u00f1o del beam search. Defecto: 5 (SOTA 2026 - \u00f3ptimo para large-v3-turbo)\n        best_of: N\u00famero de candidatos a considerar. Defecto: 5\n        temperature: Temperatura de muestreo (0.0 para determin\u00edstico).\n            Defecto: 0.0\n        vad_filter: Activar filtrado VAD. Defecto: True\n        vad_parameters: Configuraci\u00f3n detallada del VAD.\n        audio_device_index: \u00cdndice del dispositivo de entrada de audio (None para defecto).\n        no_speech_threshold: Umbral de probabilidad de no-habla (0.0-1.0).\n            Valores altos filtran m\u00e1s segmentos sin habla. Defecto: 0.6\n        compression_ratio_threshold: Umbral de ratio de compresi\u00f3n para detectar\n            salidas repetitivas/alucinaciones. Defecto: 2.4\n        log_prob_threshold: Umbral de log-probabilidad para filtrar\n            transcripciones de baja confianza. Defecto: -1.0\n    \"\"\"\n\n    model: str = \"large-v2\"\n    language: str = \"es\"\n    device: str = \"cuda\"\n    compute_type: str = \"int8_float16\"\n    device_index: int = 0\n    num_workers: int = 4\n    beam_size: int = 5\n    best_of: int = 5\n    temperature: float | list[float] = 0.0\n    vad_filter: bool = True\n    audio_device_index: int | None = None\n    keep_warm: bool = Field(default=True)\n    vad_parameters: VadParametersConfig = Field(default_factory=VadParametersConfig)\n    # Par\u00e1metros de calidad para reducir alucinaciones (SOTA 2026)\n    no_speech_threshold: float = Field(default=0.6, ge=0.0, le=1.0)\n    compression_ratio_threshold: float = Field(default=2.4, ge=1.0, le=5.0)\n    log_prob_threshold: float = Field(default=-1.0, ge=-5.0, le=0.0)\n</code></pre>"},{"location":"api/backend/config/#v2m.shared.config.VadParametersConfig","title":"<code>v2m.shared.config.VadParametersConfig</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Par\u00e1metros para la Detecci\u00f3n de Actividad de Voz (VAD).</p> <p>El VAD filtra segmentos de silencio antes de la transcripci\u00f3n para mejorar la eficiencia y reducir alucinaciones del modelo.</p> Atributos <p>threshold: Umbral de probabilidad (0.0 a 1.0) para clasificar un segmento como habla.     Defecto: 0.35 (SOTA 2026 - preserva vocales finales del espa\u00f1ol) min_speech_duration_ms: Duraci\u00f3n m\u00ednima (ms) para ser considerado habla.     Defecto: 250ms min_silence_duration_ms: Duraci\u00f3n m\u00ednima de silencio (ms) para considerar que el habla termin\u00f3.     Defecto: 1000ms (Spanish prosody safe - preserves natural pauses) speech_pad_ms: Relleno aplicado al inicio/fin de segmentos de habla detectados.     Defecto: 400ms (keeps the start/end of words)</p> Source code in <code>apps/daemon/backend/src/v2m/shared/config/__init__.py</code> <pre><code>class VadParametersConfig(BaseModel):\n    \"\"\"Par\u00e1metros para la Detecci\u00f3n de Actividad de Voz (VAD).\n\n    El VAD filtra segmentos de silencio antes de la transcripci\u00f3n para mejorar la eficiencia\n    y reducir alucinaciones del modelo.\n\n    Atributos:\n        threshold: Umbral de probabilidad (0.0 a 1.0) para clasificar un segmento como habla.\n            Defecto: 0.35 (SOTA 2026 - preserva vocales finales del espa\u00f1ol)\n        min_speech_duration_ms: Duraci\u00f3n m\u00ednima (ms) para ser considerado habla.\n            Defecto: 250ms\n        min_silence_duration_ms: Duraci\u00f3n m\u00ednima de silencio (ms) para considerar que el habla termin\u00f3.\n            Defecto: 1000ms (Spanish prosody safe - preserves natural pauses)\n        speech_pad_ms: Relleno aplicado al inicio/fin de segmentos de habla detectados.\n            Defecto: 400ms (keeps the start/end of words)\n    \"\"\"\n\n    threshold: float = 0.35\n    min_speech_duration_ms: int = 250\n    min_silence_duration_ms: int = 1000\n    speech_pad_ms: int = 400\n</code></pre>"},{"location":"api/backend/config/#configuracion-llm","title":"Configuraci\u00f3n LLM","text":"<p>options: show_source: false</p> <p>options: show_source: false</p> <p>options: show_source: false</p> <p>options: show_source: false</p>"},{"location":"api/backend/config/#v2m.shared.config.LLMConfig","title":"<code>v2m.shared.config.LLMConfig</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuraci\u00f3n del Servicio LLM.</p> Atributos <p>backend: Selector de backend (\"local\", \"gemini\" u \"ollama\"). Defecto: \"local\" local: Configuraci\u00f3n para el backend local llama.cpp. ollama: Configuraci\u00f3n para el backend Ollama.</p> Source code in <code>apps/daemon/backend/src/v2m/shared/config/__init__.py</code> <pre><code>class LLMConfig(BaseModel):\n    \"\"\"Configuraci\u00f3n del Servicio LLM.\n\n    Atributos:\n        backend: Selector de backend (\"local\", \"gemini\" u \"ollama\"). Defecto: \"local\"\n        local: Configuraci\u00f3n para el backend local llama.cpp.\n        ollama: Configuraci\u00f3n para el backend Ollama.\n    \"\"\"\n\n    backend: Literal[\"local\", \"gemini\", \"ollama\"] = Field(default=\"local\")\n    local: LocalLLMConfig = Field(default_factory=LocalLLMConfig)\n    ollama: OllamaConfig = Field(default_factory=OllamaConfig)\n</code></pre>"},{"location":"api/backend/config/#v2m.shared.config.GeminiConfig","title":"<code>v2m.shared.config.GeminiConfig</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuraci\u00f3n del servicio LLM Google Gemini.</p> Atributos <p>model: Identificador del modelo Gemini (ej. 'models/gemini-1.5-flash-latest'). temperature: Temperatura de generaci\u00f3n (0.0 a 2.0). Defecto: 0.3 max_tokens: M\u00e1ximo de tokens a generar. Defecto: 2048 max_input_chars: L\u00edmite de caracteres de entrada. Defecto: 6000 request_timeout: Tiempo de espera de solicitud HTTP en segundos. Defecto: 30 retry_attempts: N\u00famero de reintentos autom\u00e1ticos. Defecto: 3 retry_min_wait: Espera m\u00ednima entre reintentos (segundos). Defecto: 2 retry_max_wait: Espera m\u00e1xima entre reintentos (segundos). Defecto: 10 api_key: Clave de API para Google Cloud (configurar v\u00eda variable de entorno GEMINI_API_KEY).</p> Source code in <code>apps/daemon/backend/src/v2m/shared/config/__init__.py</code> <pre><code>class GeminiConfig(BaseModel):\n    \"\"\"Configuraci\u00f3n del servicio LLM Google Gemini.\n\n    Atributos:\n        model: Identificador del modelo Gemini (ej. 'models/gemini-1.5-flash-latest').\n        temperature: Temperatura de generaci\u00f3n (0.0 a 2.0). Defecto: 0.3\n        max_tokens: M\u00e1ximo de tokens a generar. Defecto: 2048\n        max_input_chars: L\u00edmite de caracteres de entrada. Defecto: 6000\n        request_timeout: Tiempo de espera de solicitud HTTP en segundos. Defecto: 30\n        retry_attempts: N\u00famero de reintentos autom\u00e1ticos. Defecto: 3\n        retry_min_wait: Espera m\u00ednima entre reintentos (segundos). Defecto: 2\n        retry_max_wait: Espera m\u00e1xima entre reintentos (segundos). Defecto: 10\n        api_key: Clave de API para Google Cloud (configurar v\u00eda variable de entorno GEMINI_API_KEY).\n    \"\"\"\n\n    model: str = \"models/gemini-1.5-flash-latest\"\n    temperature: float = 0.3\n    max_tokens: int = 2048\n    max_input_chars: int = 6000\n    request_timeout: int = 30\n    retry_attempts: int = 3\n    retry_min_wait: int = 2\n    retry_max_wait: int = 10\n    translation_temperature: float = 0.3\n    api_key: str | None = Field(default=None)\n</code></pre>"},{"location":"api/backend/config/#v2m.shared.config.OllamaConfig","title":"<code>v2m.shared.config.OllamaConfig</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuraci\u00f3n para backend LLM Ollama (SOTA 2026).</p> Atributos <p>host: URL del servidor Ollama. Defecto: http://localhost:11434 model: Nombre del modelo (gemma2:2b, phi3.5-mini, qwen2.5-coder:7b). keep_alive: Tiempo para mantener el modelo cargado. \"0m\" libera VRAM inmediatamente. temperature: Temperatura de generaci\u00f3n. 0.0 para salidas estructuradas determin\u00edsticas. translation_temperature: Temperatura para tareas de traducci\u00f3n.</p> Source code in <code>apps/daemon/backend/src/v2m/shared/config/__init__.py</code> <pre><code>class OllamaConfig(BaseModel):\n    \"\"\"Configuraci\u00f3n para backend LLM Ollama (SOTA 2026).\n\n    Atributos:\n        host: URL del servidor Ollama. Defecto: http://localhost:11434\n        model: Nombre del modelo (gemma2:2b, phi3.5-mini, qwen2.5-coder:7b).\n        keep_alive: Tiempo para mantener el modelo cargado. \"0m\" libera VRAM inmediatamente.\n        temperature: Temperatura de generaci\u00f3n. 0.0 para salidas estructuradas determin\u00edsticas.\n        translation_temperature: Temperatura para tareas de traducci\u00f3n.\n    \"\"\"\n\n    host: str = Field(default=\"http://localhost:11434\")\n    model: str = Field(default=\"gemma2:2b\")\n    keep_alive: str = Field(default=\"5m\")\n    temperature: float = Field(default=0.0, ge=0.0, le=2.0)\n    translation_temperature: float = Field(default=0.3, ge=0.0, le=2.0)\n</code></pre>"},{"location":"api/backend/config/#v2m.shared.config.LocalLLMConfig","title":"<code>v2m.shared.config.LocalLLMConfig</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuraci\u00f3n para LLM local usando llama.cpp.</p> Atributos <p>model_path: Ruta al archivo del modelo GGUF. n_gpu_layers: N\u00famero de capas para descargar a la GPU (-1 para todas). n_ctx: Tama\u00f1o de la ventana de contexto. Defecto: 2048 temperature: Temperatura de generaci\u00f3n. Defecto: 0.3 max_tokens: M\u00e1ximo de tokens a generar. Defecto: 512</p> Source code in <code>apps/daemon/backend/src/v2m/shared/config/__init__.py</code> <pre><code>class LocalLLMConfig(BaseModel):\n    \"\"\"Configuraci\u00f3n para LLM local usando llama.cpp.\n\n    Atributos:\n        model_path: Ruta al archivo del modelo GGUF.\n        n_gpu_layers: N\u00famero de capas para descargar a la GPU (-1 para todas).\n        n_ctx: Tama\u00f1o de la ventana de contexto. Defecto: 2048\n        temperature: Temperatura de generaci\u00f3n. Defecto: 0.3\n        max_tokens: M\u00e1ximo de tokens a generar. Defecto: 512\n    \"\"\"\n\n    model_path: Path = Field(default=Path(\"models/qwen2.5-3b-instruct-q4_k_m.gguf\"))\n    n_gpu_layers: int = Field(default=-1)\n    n_ctx: int = Field(default=2048, ge=512, le=32768)\n    temperature: float = Field(default=0.3, ge=0.0, le=2.0)\n    translation_temperature: float = Field(default=0.3, ge=0.0, le=2.0)\n    max_tokens: int = Field(default=512, ge=1, le=4096)\n</code></pre>"},{"location":"api/backend/config/#configuracion-de-notificaciones","title":"Configuraci\u00f3n de Notificaciones","text":"<p>options: show_source: false</p>"},{"location":"api/backend/config/#v2m.shared.config.NotificationsConfig","title":"<code>v2m.shared.config.NotificationsConfig</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuraci\u00f3n de notificaciones de escritorio.</p> Atributos <p>expire_time_ms: Tiempo en ms antes del cierre autom\u00e1tico. Defecto: 3000 auto_dismiss: Forzar cierre program\u00e1tico. Defecto: True</p> Source code in <code>apps/daemon/backend/src/v2m/shared/config/__init__.py</code> <pre><code>class NotificationsConfig(BaseModel):\n    \"\"\"Configuraci\u00f3n de notificaciones de escritorio.\n\n    Atributos:\n        expire_time_ms: Tiempo en ms antes del cierre autom\u00e1tico. Defecto: 3000\n        auto_dismiss: Forzar cierre program\u00e1tico. Defecto: True\n    \"\"\"\n\n    expire_time_ms: int = Field(default=3000, ge=500, le=30000)\n    auto_dismiss: bool = Field(default=True)\n</code></pre>"},{"location":"api/backend/llm/","title":"Servicios LLM","text":"<p>Proveedores de modelos de lenguaje para procesamiento de texto.</p>"},{"location":"api/backend/llm/#google-gemini-cloud","title":"Google Gemini (Cloud)","text":"<p>Servicio LLM que conecta con la API de Google Gemini para procesamiento de texto y traducciones.</p> <p>Ubicaci\u00f3n: <code>v2m/features/llm/gemini_service.py</code></p> <p>M\u00e9todos principales:</p> <ul> <li><code>process_text(text: str) -&gt; str</code> - Refina texto con puntuaci\u00f3n y gram\u00e1tica</li> <li><code>translate_text(text: str, target_lang: str) -&gt; str</code> - Traduce texto</li> </ul>"},{"location":"api/backend/llm/#ollama-local","title":"Ollama (Local)","text":"<p>Servicio LLM local que conecta con el servidor Ollama para privacidad total.</p> <p>Ubicaci\u00f3n: <code>v2m/features/llm/ollama_service.py</code></p> <p>Configuraci\u00f3n: <code>http://localhost:11434</code></p>"},{"location":"api/backend/llm/#local-llamacpp","title":"Local (llama.cpp)","text":"<p>Servicio LLM embebido usando llama-cpp-python directamente.</p> <p>Ubicaci\u00f3n: <code>v2m/features/llm/local_service.py</code></p>"},{"location":"api/backend/llm/#patron-de-diseno","title":"Patr\u00f3n de Dise\u00f1o","text":"<p>Todos los servicios LLM implementan una interfaz com\u00fan (Protocolo):</p> <pre><code>class ILLMService(Protocol):\n    async def process_text(self, text: str) -&gt; str:\n        \"\"\"Refina texto con gram\u00e1tica y puntuaci\u00f3n.\"\"\"\n        ...\n\n    async def translate_text(self, text: str, target_lang: str) -&gt; str:\n        \"\"\"Traduce texto al idioma especificado.\"\"\"\n        ...\n</code></pre> <p>El <code>LLMWorkflow</code> selecciona el backend seg\u00fan <code>config.llm.provider</code>:</p> <ul> <li><code>\"gemini\"</code> \u2192 GeminiLLMService</li> <li><code>\"ollama\"</code> \u2192 OllamaLLMService</li> <li><code>\"local\"</code> \u2192 LocalLLMService</li> </ul>"},{"location":"api/backend/transcription/","title":"Transcripci\u00f3n","text":"<p>Servicios de transcripci\u00f3n de audio a texto usando faster-whisper.</p>"},{"location":"api/backend/transcription/#persistentwhisperworker","title":"PersistentWhisperWorker","text":"<p>Worker persistente que mantiene el modelo Whisper cargado en VRAM entre sesiones.</p> <p>Ubicaci\u00f3n: <code>v2m/features/transcription/persistent_model.py</code></p>"},{"location":"api/backend/transcription/#caracteristicas","title":"Caracter\u00edsticas","text":"<ul> <li>Lazy Loading: El modelo se carga la primera vez que se necesita</li> <li>Keep-Warm Policy: Mantiene el modelo en VRAM seg\u00fan configuraci\u00f3n</li> <li>GPU Optimizado: Usa <code>float16</code> o <code>int8_float16</code> para m\u00e1ximo rendimiento</li> </ul>"},{"location":"api/backend/transcription/#metodos","title":"M\u00e9todos","text":"<pre><code>class PersistentWhisperWorker:\n    def initialize_sync(self) -&gt; None:\n        \"\"\"Carga el modelo en VRAM (s\u00edncrono, para warmup).\"\"\"\n\n    async def transcribe(self, audio: np.ndarray) -&gt; str:\n        \"\"\"Transcribe audio a texto.\"\"\"\n\n    async def unload(self) -&gt; None:\n        \"\"\"Libera el modelo de VRAM.\"\"\"\n</code></pre>"},{"location":"api/backend/transcription/#streamingtranscriber","title":"StreamingTranscriber","text":"<p>Transcriptor en tiempo real que proporciona feedback provisional mientras el usuario habla.</p> <p>Ubicaci\u00f3n: <code>v2m/features/audio/streaming_transcriber.py</code></p>"},{"location":"api/backend/transcription/#flujo-de-datos","title":"Flujo de Datos","text":"<pre><code>graph LR\n    A[AudioRecorder] --&gt; B[VAD]\n    B --&gt; C[StreamingTranscriber]\n    C --&gt; D[WhisperWorker]\n    D --&gt; E[WebSocket]\n\n    style C fill:#e3f2fd</code></pre>"},{"location":"api/backend/transcription/#integracion","title":"Integraci\u00f3n","text":"<p>El <code>StreamingTranscriber</code> emite eventos via WebSocket:</p> <ul> <li><code>transcription_update</code>: Texto provisional durante grabaci\u00f3n</li> <li><code>transcription_final</code>: Texto final al detener</li> </ul>"},{"location":"api/backend/workflows/","title":"Workflows (Orquestaci\u00f3n)","text":"<p>Los Workflows son los componentes encargados de coordinar las diferentes funcionalidades del sistema para completar tareas complejas de negocio.</p>"},{"location":"api/backend/workflows/#recordingworkflow","title":"RecordingWorkflow","text":"<p>Gestiona el proceso completo de grabaci\u00f3n, desde la captura inicial hasta la transcripci\u00f3n final.</p> <p>options: show_source: true</p>"},{"location":"api/backend/workflows/#v2m.orchestration.recording_workflow.RecordingWorkflow","title":"<code>v2m.orchestration.recording_workflow.RecordingWorkflow</code>","text":"<p>Orquestador para el flujo de grabaci\u00f3n y transcripci\u00f3n as\u00edncrona.</p> Source code in <code>apps/daemon/backend/src/v2m/orchestration/recording_workflow.py</code> <pre><code>class RecordingWorkflow:\n    \"\"\"Orquestador para el flujo de grabaci\u00f3n y transcripci\u00f3n as\u00edncrona.\"\"\"\n\n    def __init__(self, broadcast_fn: BroadcastFn | None = None) -&gt; None:\n        \"\"\"Inicializa el workflow.\n\n        Args:\n            broadcast_fn: Funci\u00f3n opcional para emitir actualizaciones de estado.\n        \"\"\"\n        self._is_recording = False\n        self._model_loaded = False\n        self._broadcast_fn = broadcast_fn\n\n        self._worker: PersistentWhisperWorker | None = None\n        self._recorder: AudioRecorder | None = None\n        self._transcriber: StreamingTranscriber | None = None\n        self._clipboard: LinuxClipboardAdapter | None = None\n        self._notifications: LinuxNotificationService | None = None\n\n    @property\n    def worker(self):\n        if self._worker is None:\n            from v2m.features.transcription.persistent_model import PersistentWhisperWorker\n\n            whisper_cfg = config.transcription.whisper\n            self._worker = PersistentWhisperWorker(\n                model_size=whisper_cfg.model,\n                device=whisper_cfg.device,\n                compute_type=whisper_cfg.compute_type,\n                device_index=whisper_cfg.device_index,\n                num_workers=whisper_cfg.num_workers,\n                keep_warm=whisper_cfg.keep_warm,\n            )\n        return self._worker\n\n    @property\n    def recorder(self):\n        if self._recorder is None:\n            from v2m.features.audio.recorder import AudioRecorder\n\n            whisper_cfg = config.transcription.whisper\n            self._recorder = AudioRecorder(\n                sample_rate=16000,\n                channels=1,\n                device_index=whisper_cfg.audio_device_index,\n            )\n        return self._recorder\n\n    @property\n    def transcriber(self):\n        if self._transcriber is None:\n            from v2m.features.audio.streaming_transcriber import StreamingTranscriber\n\n            session_adapter = WebSocketSessionAdapter(self._broadcast_fn)\n            self._transcriber = StreamingTranscriber(\n                worker=self.worker,\n                session_manager=session_adapter,\n                recorder=self.recorder,\n            )\n        return self._transcriber\n\n    @property\n    def clipboard(self):\n        if self._clipboard is None:\n            from v2m.features.desktop.linux_adapters import LinuxClipboardAdapter\n\n            self._clipboard = LinuxClipboardAdapter()\n        return self._clipboard\n\n    @property\n    def notifications(self):\n        if self._notifications is None:\n            from v2m.features.desktop.notification_service import LinuxNotificationService\n\n            self._notifications = LinuxNotificationService()\n        return self._notifications\n\n    async def warmup(self) -&gt; None:\n        if self._model_loaded:\n            return\n        try:\n            loop = asyncio.get_running_loop()\n            await loop.run_in_executor(None, self.worker.initialize_sync)\n            self._model_loaded = True\n            logger.info(\"\u2705 Modelo Whisper precargado en VRAM\")\n        except Exception as e:\n            logger.error(f\"\u274c Error en warmup del modelo: {e}\")\n\n    async def toggle(self) -&gt; \"ToggleResponse\":\n        if not self._is_recording:\n            return await self.start()\n        return await self.stop()\n\n    async def start(self) -&gt; \"ToggleResponse\":\n        from v2m.api.schemas import ToggleResponse\n\n        if self._is_recording:\n            return ToggleResponse(status=\"recording\", message=\"\u26a0\ufe0f Ya est\u00e1 grabando\")\n        try:\n            await self.transcriber.start()\n            self._is_recording = True\n            config.paths.recording_flag.touch()\n            self.notifications.notify(\"\ud83c\udfa4 voice2machine\", \"grabaci\u00f3n iniciada...\")\n            logger.info(\"\ud83c\udf99\ufe0f Grabaci\u00f3n iniciada\")\n            return ToggleResponse(status=\"recording\", message=\"\ud83c\udf99\ufe0f Grabando...\")\n        except Exception as e:\n            logger.error(f\"Error iniciando grabaci\u00f3n: {e}\")\n            return ToggleResponse(status=\"error\", message=f\"\u274c Error: {e}\")\n\n    async def stop(self) -&gt; \"ToggleResponse\":\n        from v2m.api.schemas import ToggleResponse\n\n        if not self._is_recording:\n            return ToggleResponse(status=\"idle\", message=\"\u26a0\ufe0f No hay grabaci\u00f3n en curso\")\n        try:\n            self._is_recording = False\n            if config.paths.recording_flag.exists():\n                config.paths.recording_flag.unlink()\n            self.notifications.notify(\"\u26a1 v2m procesando\", \"procesando...\")\n            transcription = await self.transcriber.stop()\n            if not transcription or not transcription.strip():\n                # Diagn\u00f3stico mejorado: reportar estado de la cola y duraci\u00f3n de grabaci\u00f3n\n                try:\n                    queue_size = self.transcriber._audio_queue.qsize()\n                    logger.warning(\n                        f\"Transcripci\u00f3n vac\u00eda: audio_queue_size={queue_size}, \"\n                        f\"verificar logs de VAD y Whisper para diagn\u00f3stico detallado\"\n                    )\n                except Exception as diag_err:\n                    logger.debug(f\"Error obteniendo diagn\u00f3stico: {diag_err}\")\n\n                self.notifications.notify(\"\u274c whisper\", \"no se detect\u00f3 voz en el audio\")\n                return ToggleResponse(status=\"idle\", message=\"\u274c No se detect\u00f3 voz\", text=None)\n            self.clipboard.copy(transcription)\n            preview = transcription[:80]\n            self.notifications.notify(\"\u2705 whisper - copiado\", f\"{preview}...\")\n            logger.info(f\"\u2705 Transcripci\u00f3n completada: {len(transcription)} chars\")\n            return ToggleResponse(status=\"idle\", message=\"\u2705 Copiado al portapapeles\", text=transcription)\n        except Exception as e:\n            logger.error(f\"Error deteniendo grabaci\u00f3n: {e}\")\n            self._is_recording = False\n            return ToggleResponse(status=\"error\", message=f\"\u274c Error: {e}\")\n\n    def get_status(self) -&gt; \"StatusResponse\":\n        from v2m.api.schemas import StatusResponse\n\n        state = \"recording\" if self._is_recording else \"idle\"\n        return StatusResponse(state=state, recording=self._is_recording, model_loaded=self._model_loaded)\n\n    async def shutdown(self) -&gt; None:\n        \"\"\"Apaga el workflow, deteniendo grabaciones y descargando modelos.\"\"\"\n        if self._is_recording:\n            with contextlib.suppress(Exception):\n                await self.stop()\n\n        if self._worker:\n            with contextlib.suppress(Exception):\n                await self._worker.unload()\n\n        if self._notifications:\n            self._notifications.shutdown(wait=False)\n</code></pre>"},{"location":"api/backend/workflows/#v2m.orchestration.recording_workflow.RecordingWorkflow.__init__","title":"<code>__init__(broadcast_fn=None)</code>","text":"<p>Inicializa el workflow.</p> <p>Parameters:</p> Name Type Description Default <code>broadcast_fn</code> <code>BroadcastFn | None</code> <p>Funci\u00f3n opcional para emitir actualizaciones de estado.</p> <code>None</code> Source code in <code>apps/daemon/backend/src/v2m/orchestration/recording_workflow.py</code> <pre><code>def __init__(self, broadcast_fn: BroadcastFn | None = None) -&gt; None:\n    \"\"\"Inicializa el workflow.\n\n    Args:\n        broadcast_fn: Funci\u00f3n opcional para emitir actualizaciones de estado.\n    \"\"\"\n    self._is_recording = False\n    self._model_loaded = False\n    self._broadcast_fn = broadcast_fn\n\n    self._worker: PersistentWhisperWorker | None = None\n    self._recorder: AudioRecorder | None = None\n    self._transcriber: StreamingTranscriber | None = None\n    self._clipboard: LinuxClipboardAdapter | None = None\n    self._notifications: LinuxNotificationService | None = None\n</code></pre>"},{"location":"api/backend/workflows/#v2m.orchestration.recording_workflow.RecordingWorkflow.shutdown","title":"<code>shutdown()</code>  <code>async</code>","text":"<p>Apaga el workflow, deteniendo grabaciones y descargando modelos.</p> Source code in <code>apps/daemon/backend/src/v2m/orchestration/recording_workflow.py</code> <pre><code>async def shutdown(self) -&gt; None:\n    \"\"\"Apaga el workflow, deteniendo grabaciones y descargando modelos.\"\"\"\n    if self._is_recording:\n        with contextlib.suppress(Exception):\n            await self.stop()\n\n    if self._worker:\n        with contextlib.suppress(Exception):\n            await self._worker.unload()\n\n    if self._notifications:\n        self._notifications.shutdown(wait=False)\n</code></pre>"},{"location":"api/backend/workflows/#llmworkflow","title":"LLMWorkflow","text":"<p>Coordina el procesamiento de texto mediante proveedores de lenguaje (LLM), incluyendo refinamiento y traducci\u00f3n.</p> <p>options: show_source: true</p>"},{"location":"api/backend/workflows/#v2m.orchestration.llm_workflow.LLMWorkflow","title":"<code>v2m.orchestration.llm_workflow.LLMWorkflow</code>","text":"<p>Orquestador para el refinamiento y traducci\u00f3n de texto mediante LLM.</p> Source code in <code>apps/daemon/backend/src/v2m/orchestration/llm_workflow.py</code> <pre><code>class LLMWorkflow:\n    \"\"\"Orquestador para el refinamiento y traducci\u00f3n de texto mediante LLM.\"\"\"\n\n    def __init__(self) -&gt; None:\n        \"\"\"Inicializa el workflow de LLM.\"\"\"\n        self._llm_service: Any | None = None\n        self._clipboard: LinuxClipboardAdapter | None = None\n        self._notifications: LinuxNotificationService | None = None\n\n    @property\n    def clipboard(self):\n        \"\"\"Adaptador de portapapeles (Linux).\"\"\"\n        if self._clipboard is None:\n            from v2m.features.desktop.linux_adapters import LinuxClipboardAdapter\n\n            self._clipboard = LinuxClipboardAdapter()\n        return self._clipboard\n\n    @property\n    def notifications(self):\n        \"\"\"Servicio de notificaciones del sistema.\"\"\"\n        if self._notifications is None:\n            from v2m.features.desktop.notification_service import LinuxNotificationService\n\n            self._notifications = LinuxNotificationService()\n        return self._notifications\n\n    @property\n    def llm_service(self) -&gt; Any:\n        \"\"\"Servicio LLM configurado (Gemini, Ollama o Local).\"\"\"\n        if self._llm_service is None:\n            backend = config.llm.backend\n            if backend == \"gemini\":\n                from v2m.features.llm.gemini_service import GeminiLLMService\n\n                self._llm_service = GeminiLLMService()\n            elif backend == \"ollama\":\n                from v2m.features.llm.ollama_service import OllamaLLMService\n\n                self._llm_service = OllamaLLMService()\n            else:\n                from v2m.features.llm.local_service import LocalLLMService\n\n                self._llm_service = LocalLLMService()\n            logger.info(f\"LLM backend inicializado: {backend}\")\n        return self._llm_service\n\n    async def process_text(self, text: str) -&gt; \"LLMResponse\":\n        \"\"\"Refina el texto usando el LLM y lo copia al portapapeles.\"\"\"\n        from v2m.api.schemas import LLMResponse\n\n        backend_name = config.llm.backend\n        try:\n            if asyncio.iscoroutinefunction(self.llm_service.process_text):\n                refined = await self.llm_service.process_text(text)\n            else:\n                refined = await asyncio.to_thread(self.llm_service.process_text, text)\n            self.clipboard.copy(refined)\n            self.notifications.notify(f\"\u2705 {backend_name} - copiado\", f\"{refined[:80]}...\")\n            return LLMResponse(text=refined, backend=backend_name)\n        except Exception as e:\n            logger.error(f\"Error procesando texto con {backend_name}: {e}\")\n            self.clipboard.copy(text)\n            self.notifications.notify(f\"\u26a0\ufe0f {backend_name} fall\u00f3\", \"usando texto original...\")\n            return LLMResponse(text=text, backend=f\"{backend_name} (fallback)\")\n\n    async def translate_text(self, text: str, target_lang: str) -&gt; \"LLMResponse\":\n        \"\"\"Traduce el texto al idioma especificado usando el LLM.\"\"\"\n        from v2m.api.schemas import LLMResponse\n\n        backend_name = config.llm.backend\n        if not re.match(r\"^[a-zA-Z\\s\\-]{2,20}$\", target_lang):\n            logger.warning(f\"Idioma inv\u00e1lido: {target_lang}\")\n            self.notifications.notify(\"\u274c Error\", \"Idioma de destino inv\u00e1lido\")\n            return LLMResponse(text=text, backend=\"error\")\n        try:\n            if asyncio.iscoroutinefunction(self.llm_service.translate_text):\n                translated = await self.llm_service.translate_text(text, target_lang)\n            else:\n                translated = await asyncio.to_thread(self.llm_service.translate_text, text, target_lang)\n            self.clipboard.copy(translated)\n            self.notifications.notify(f\"\u2705 Traducci\u00f3n ({target_lang})\", f\"{translated[:80]}...\")\n            return LLMResponse(text=translated, backend=backend_name)\n        except Exception as e:\n            logger.error(f\"Error traduciendo con {backend_name}: {e}\")\n            self.notifications.notify(\"\u274c Error traducci\u00f3n\", \"Fallo al traducir\")\n            return LLMResponse(text=text, backend=f\"{backend_name} (error)\")\n</code></pre>"},{"location":"api/backend/workflows/#v2m.orchestration.llm_workflow.LLMWorkflow.clipboard","title":"<code>clipboard</code>  <code>property</code>","text":"<p>Adaptador de portapapeles (Linux).</p>"},{"location":"api/backend/workflows/#v2m.orchestration.llm_workflow.LLMWorkflow.llm_service","title":"<code>llm_service</code>  <code>property</code>","text":"<p>Servicio LLM configurado (Gemini, Ollama o Local).</p>"},{"location":"api/backend/workflows/#v2m.orchestration.llm_workflow.LLMWorkflow.notifications","title":"<code>notifications</code>  <code>property</code>","text":"<p>Servicio de notificaciones del sistema.</p>"},{"location":"api/backend/workflows/#v2m.orchestration.llm_workflow.LLMWorkflow.__init__","title":"<code>__init__()</code>","text":"<p>Inicializa el workflow de LLM.</p> Source code in <code>apps/daemon/backend/src/v2m/orchestration/llm_workflow.py</code> <pre><code>def __init__(self) -&gt; None:\n    \"\"\"Inicializa el workflow de LLM.\"\"\"\n    self._llm_service: Any | None = None\n    self._clipboard: LinuxClipboardAdapter | None = None\n    self._notifications: LinuxNotificationService | None = None\n</code></pre>"},{"location":"api/backend/workflows/#v2m.orchestration.llm_workflow.LLMWorkflow.process_text","title":"<code>process_text(text)</code>  <code>async</code>","text":"<p>Refina el texto usando el LLM y lo copia al portapapeles.</p> Source code in <code>apps/daemon/backend/src/v2m/orchestration/llm_workflow.py</code> <pre><code>async def process_text(self, text: str) -&gt; \"LLMResponse\":\n    \"\"\"Refina el texto usando el LLM y lo copia al portapapeles.\"\"\"\n    from v2m.api.schemas import LLMResponse\n\n    backend_name = config.llm.backend\n    try:\n        if asyncio.iscoroutinefunction(self.llm_service.process_text):\n            refined = await self.llm_service.process_text(text)\n        else:\n            refined = await asyncio.to_thread(self.llm_service.process_text, text)\n        self.clipboard.copy(refined)\n        self.notifications.notify(f\"\u2705 {backend_name} - copiado\", f\"{refined[:80]}...\")\n        return LLMResponse(text=refined, backend=backend_name)\n    except Exception as e:\n        logger.error(f\"Error procesando texto con {backend_name}: {e}\")\n        self.clipboard.copy(text)\n        self.notifications.notify(f\"\u26a0\ufe0f {backend_name} fall\u00f3\", \"usando texto original...\")\n        return LLMResponse(text=text, backend=f\"{backend_name} (fallback)\")\n</code></pre>"},{"location":"api/backend/workflows/#v2m.orchestration.llm_workflow.LLMWorkflow.translate_text","title":"<code>translate_text(text, target_lang)</code>  <code>async</code>","text":"<p>Traduce el texto al idioma especificado usando el LLM.</p> Source code in <code>apps/daemon/backend/src/v2m/orchestration/llm_workflow.py</code> <pre><code>async def translate_text(self, text: str, target_lang: str) -&gt; \"LLMResponse\":\n    \"\"\"Traduce el texto al idioma especificado usando el LLM.\"\"\"\n    from v2m.api.schemas import LLMResponse\n\n    backend_name = config.llm.backend\n    if not re.match(r\"^[a-zA-Z\\s\\-]{2,20}$\", target_lang):\n        logger.warning(f\"Idioma inv\u00e1lido: {target_lang}\")\n        self.notifications.notify(\"\u274c Error\", \"Idioma de destino inv\u00e1lido\")\n        return LLMResponse(text=text, backend=\"error\")\n    try:\n        if asyncio.iscoroutinefunction(self.llm_service.translate_text):\n            translated = await self.llm_service.translate_text(text, target_lang)\n        else:\n            translated = await asyncio.to_thread(self.llm_service.translate_text, text, target_lang)\n        self.clipboard.copy(translated)\n        self.notifications.notify(f\"\u2705 Traducci\u00f3n ({target_lang})\", f\"{translated[:80]}...\")\n        return LLMResponse(text=translated, backend=backend_name)\n    except Exception as e:\n        logger.error(f\"Error traduciendo con {backend_name}: {e}\")\n        self.notifications.notify(\"\u274c Error traducci\u00f3n\", \"Fallo al traducir\")\n        return LLMResponse(text=text, backend=f\"{backend_name} (error)\")\n</code></pre>"},{"location":"en/","title":"\ud83d\udde3\ufe0f Voice2Machine: Local Voice Dictation","text":""},{"location":"en/#purpose","title":"\ud83c\udfaf Purpose","text":"<p>The goal is simple:</p> <p>Be able to dictate text anywhere in your operating system.</p> <p>The idea is to transcribe audio using your local GPU for maximum speed and accuracy, regardless of the application you're using (code editor, browser, chat, etc.).</p> <p>This project transforms a simple script into a robust modular application based on a Backend Daemon (Python), designed with Hexagonal Architecture to ensure maintainability, scalability, and absolute privacy.</p>"},{"location":"en/#documentation","title":"\ud83d\udcda Documentation","text":"<p>The documentation is organized to serve different needs:</p>"},{"location":"en/#exploration","title":"\ud83d\ude80 Exploration","text":"<ul> <li>Quick Start: Start dictating in minutes.</li> <li>Glossary: Defines key terms like Daemon, Whisper, and REST API.</li> </ul>"},{"location":"en/#procedures","title":"\ud83d\udee0\ufe0f Procedures","text":"<ul> <li>Installation: Step-by-step guide for Ubuntu/Debian.</li> <li>Contributing: How to collaborate on the project.</li> </ul>"},{"location":"en/#reference","title":"\u2699\ufe0f Reference","text":"<ul> <li>Configuration: Adjust models, devices, and behaviors.</li> <li>Keyboard Shortcuts: Reference for global commands.</li> <li>REST API: HTTP endpoints documentation.</li> <li>Python API: Backend classes and methods reference.</li> </ul>"},{"location":"en/#concepts","title":"\ud83e\udde0 Concepts","text":"<ul> <li>Architecture: Hexagonal Design and system components.</li> <li>Decisions (ADR): Record of important technical decisions.</li> </ul>"},{"location":"en/#maintenance","title":"\ud83d\udd27 Maintenance","text":"<ul> <li>Troubleshooting: Diagnosis and fixing common errors.</li> <li>Changelog: Project change history.</li> </ul>"},{"location":"en/api_reference/","title":"REST API Reference","text":"<p>This section documents the Voice2Machine Daemon REST API (v0.2.0+).</p> <p>Updated Architecture</p> <p>Voice2Machine uses FastAPI for client-server communication, replacing the previous Unix Sockets IPC system. This allows testing endpoints directly with <code>curl</code> or any HTTP client.</p>"},{"location":"en/api_reference/#general-information","title":"General Information","text":"Property Value Base URL <code>http://localhost:8765</code> Protocol HTTP/1.1 + WebSocket Format JSON (UTF-8) Interactive Docs <code>http://localhost:8765/docs</code> (Swagger UI)"},{"location":"en/api_reference/#rest-endpoints","title":"REST Endpoints","text":""},{"location":"en/api_reference/#post-toggle","title":"POST <code>/toggle</code>","text":"<p>Recording toggle (start/stop). This is the main endpoint used by keyboard shortcuts.</p> Request <p><code>bash     curl -X POST http://localhost:8765/toggle | jq</code></p> Response (Starting) <p><code>json     {       \"status\": \"recording\",       \"message\": \"Recording started\",       \"text\": null     }</code></p> Response (Stopping) <p><code>json     {       \"status\": \"idle\",       \"message\": \"Transcription complete\",       \"text\": \"The transcribed text appears here...\"     }</code></p>"},{"location":"en/api_reference/#post-start","title":"POST <code>/start</code>","text":"<p>Explicitly start recording. Useful when you need separate start/stop control.</p> Request <p><code>bash     curl -X POST http://localhost:8765/start | jq</code></p> Response <p><code>json     {       \"status\": \"recording\",       \"message\": \"Recording started\",       \"text\": null     }</code></p>"},{"location":"en/api_reference/#post-stop","title":"POST <code>/stop</code>","text":"<p>Stop recording and transcribe captured audio.</p> Request <p><code>bash     curl -X POST http://localhost:8765/stop | jq</code></p> Response <p><code>json     {       \"status\": \"idle\",       \"message\": \"Transcription complete\",       \"text\": \"The transcribed text appears here...\"     }</code></p>"},{"location":"en/api_reference/#post-llmprocess","title":"POST <code>/llm/process</code>","text":"<p>Process text with LLM (cleanup, punctuation, formatting). Backend is selected based on <code>config.toml</code>.</p> Request <p><code>bash     curl -X POST http://localhost:8765/llm/process \\       -H \"Content-Type: application/json\" \\       -d '{\"text\": \"hello how are you hope youre well\"}' | jq</code></p> Response <p><code>json     {       \"text\": \"Hello, how are you? Hope you're well.\",       \"backend\": \"gemini\"     }</code></p>"},{"location":"en/api_reference/#post-llmtranslate","title":"POST <code>/llm/translate</code>","text":"<p>Translate text to another language using LLM.</p> Request <p><code>bash     curl -X POST http://localhost:8765/llm/translate \\       -H \"Content-Type: application/json\" \\       -d '{\"text\": \"Good morning\", \"target_lang\": \"es\"}' | jq</code></p> Response <p><code>json     {       \"text\": \"Buenos d\u00edas\",       \"backend\": \"gemini\"     }</code></p>"},{"location":"en/api_reference/#get-status","title":"GET <code>/status</code>","text":"<p>Returns current daemon state.</p> Request <p><code>bash     curl http://localhost:8765/status | jq</code></p> Response <p><code>json     {       \"state\": \"idle\",       \"recording\": false,       \"model_loaded\": true     }</code></p> <p>Possible States:</p> State Description <code>idle</code> Waiting for commands <code>recording</code> Recording audio <code>processing</code> Transcribing or processing with LLM"},{"location":"en/api_reference/#get-health","title":"GET <code>/health</code>","text":"<p>Health check for systemd/monitoring scripts.</p> Request <p><code>bash     curl http://localhost:8765/health | jq</code></p> Response <p><code>json     {       \"status\": \"ok\",       \"version\": \"0.2.0\"     }</code></p>"},{"location":"en/api_reference/#websocket","title":"WebSocket","text":""},{"location":"en/api_reference/#ws-wsevents","title":"WS <code>/ws/events</code>","text":"<p>Real-time event stream. Useful for showing provisional transcription while user speaks.</p> Connection (JavaScript) <pre><code>const ws = new WebSocket('ws://localhost:8765/ws/events');\n\n    ws.onmessage = (event) =&gt; {\n      const { event: eventType, data } = JSON.parse(event.data);\n      console.log(`Event: ${eventType}`, data);\n    };\n    ```\n\n=== \"Connection (Python)\"\n```python\nimport asyncio\nimport websockets\n\n    async def listen():\n        async with websockets.connect('ws://localhost:8765/ws/events') as ws:\n            async for message in ws:\n                print(message)\n\n    asyncio.run(listen())\n    ```\n\n**Emitted Events:**\n\n| Event                  | Fields                           | Description                                 |\n| ---------------------- | -------------------------------- | ------------------------------------------- |\n| `transcription_update` | `text: str`, `final: bool`       | Transcription update (provisional or final) |\n| `heartbeat`            | `timestamp: float`, `state: str` | Heartbeat to keep connection alive          |\n\n---\n\n## Data Models\n\n### ToggleResponse\n\n```python\nclass ToggleResponse(BaseModel):\n    status: str      # 'recording' | 'idle'\n    message: str     # Descriptive message\n    text: str | None # Transcribed text (only on stop)\n</code></pre>"},{"location":"en/api_reference/#statusresponse","title":"StatusResponse","text":"<pre><code>class StatusResponse(BaseModel):\n    state: str        # 'idle' | 'recording' | 'processing'\n    recording: bool   # True if recording\n    model_loaded: bool # True if Whisper is in VRAM\n</code></pre>"},{"location":"en/api_reference/#llmresponse","title":"LLMResponse","text":"<pre><code>class LLMResponse(BaseModel):\n    text: str    # Processed/translated text\n    backend: str # 'gemini' | 'ollama' | 'local'\n</code></pre>"},{"location":"en/api_reference/#error-codes","title":"Error Codes","text":"HTTP Code Meaning <code>200</code> Successful operation <code>422</code> Validation error (invalid payload) <code>500</code> Internal server error <p>Debugging</p> <p>Use the interactive documentation at <code>http://localhost:8765/docs</code> to test endpoints visually.</p>"},{"location":"en/architecture/","title":"\ud83e\udde9 System Architecture","text":"<p>Technical Philosophy</p> <p>Voice2Machine implements a strict Architecture based on Workflows and Features, prioritizing decoupling, testability, and technological independence. The system adheres to SOTA 2026 standards like static typing in Python (Protocol) and Frontend/Backend separation via REST API.</p>"},{"location":"en/architecture/#high-level-diagram","title":"\ud83c\udfd7\ufe0f High-Level Diagram","text":"<pre><code>graph TD\n    subgraph Clients [\"\ud83d\udd0c Clients (CLI / Scripts / GUI / Tauri)\"]\n        ClientApp[\"Any HTTP Client\"]\n    end\n\n    subgraph Backend [\"\ud83d\udc0d Backend Daemon (Python + FastAPI)\"]\n        API[\"FastAPI Package&lt;br&gt;(api/)\"]\n\n        subgraph Workflows [\"\ud83e\udde0 Workflows (Orchestration)\"]\n            RecWF[\"RecordingWorkflow\"]\n            LLMWF[\"LLMWorkflow\"]\n        end\n\n        subgraph Features [\"\ud83e\udde9 Features (Domain + Logic)\"]\n            AudioFeat[\"Audio Service\"]\n            TranscFeat[\"Transcription Service\"]\n            LLMFeat[\"LLM Service\"]\n        end\n\n        subgraph Shared [\"\u2699\ufe0f Shared (Foundation)\"]\n            Config[\"Config\"]\n            Errors[\"Errors\"]\n            Interfaces[\"Interfaces\"]\n        end\n    end\n\n    ClientApp &lt;--&gt;|REST + WebSocket| API\n    API --&gt; Workflows\n    Workflows --&gt; Features\n    Features --&gt; Shared\n\n    style Clients fill:#e3f2fd,stroke:#1565c0\n    style Backend fill:#e8f5e9,stroke:#2e7d32\n    style Workflows fill:#fff3e0,stroke:#ef6c00\n    style Features fill:#f3e5f5,stroke:#7b1fa2\n    style Shared fill:#eceff1,stroke:#455a64</code></pre>"},{"location":"en/architecture/#backend-components","title":"\ud83d\udce6 Backend Components","text":""},{"location":"en/architecture/#1-api-layer-fastapi","title":"1. API Layer (FastAPI)","text":"<p>Located in <code>apps/daemon/backend/src/v2m/api/</code>.</p> <ul> <li>Modules: <code>app.py</code>, <code>routes/</code>, <code>schemas.py</code></li> <li>REST Endpoints: <code>/toggle</code>, <code>/start</code>, <code>/stop</code>, <code>/status</code>, <code>/health</code></li> <li>WebSocket: <code>/ws/events</code> for real-time transcription streaming</li> <li>Auto-documentation: Swagger UI at <code>/docs</code></li> </ul> <p>Modern Structure</p> <p>Starting from v0.3.0, the API is organized as a complete package, separating routes and schemas for better maintainability.</p>"},{"location":"en/architecture/#2-workflows-orchestration","title":"2. Workflows (Orchestration)","text":"<p>Located in <code>apps/daemon/backend/src/v2m/orchestration/</code>.</p> <p>Instead of a monolithic Orchestrator, the system uses specialized Workflows for each business flow:</p> <ul> <li>RecordingWorkflow: Manages the complete capture and transcription lifecycle.</li> <li>LLMWorkflow: Coordinates text processing and translation.</li> </ul> <p>This approach allows each flow to evolve independently without affecting the rest of the system.</p>"},{"location":"en/architecture/#3-features-domains","title":"3. Features (Domains)","text":"<p>Located in <code>apps/daemon/backend/src/v2m/features/</code>.</p> <p>Each folder in <code>features/</code> represents a self-contained domain of knowledge including its own services and logic:</p> Feature Responsibility transcription Whisper implementations (<code>faster-whisper</code>). audio Audio capture and management of the Rust engine (<code>v2m_engine</code>). llm Integrations with Gemini, Ollama, and other providers."},{"location":"en/architecture/#4-shared-common-foundation","title":"4. Shared (Common Foundation)","text":"<p>Located in <code>apps/daemon/backend/src/v2m/shared/</code>.</p> <ul> <li>Interfaces: Global definitions via <code>typing.Protocol</code>.</li> <li>Config: <code>config.toml</code> management via Pydantic Settings.</li> <li>Errors: Shared exception hierarchies.</li> </ul>"},{"location":"en/architecture/#client-backend-communication","title":"\u26a1 Client-Backend Communication","text":"<p>Voice2Machine uses FastAPI REST + WebSocket for communication:</p>"},{"location":"en/architecture/#rest-synchronous","title":"REST (Synchronous)","text":"<pre><code># Toggle recording\ncurl -X POST http://localhost:8765/toggle | jq\n\n# Check status\ncurl http://localhost:8765/status | jq\n</code></pre>"},{"location":"en/architecture/#websocket-streaming","title":"WebSocket (Streaming)","text":"<pre><code>const ws = new WebSocket(\"ws://localhost:8765/ws/events\");\nws.onmessage = (e) =&gt; {\n  const { event, data } = JSON.parse(e.data);\n  if (event === \"transcription_update\") {\n    console.log(data.text, data.final);\n  }\n};\n</code></pre>"},{"location":"en/architecture/#native-extensions-rust","title":"\ud83e\udd80 Native Extensions (Rust)","text":"<p>For critical tasks where Python's GIL is a bottleneck, we use native extensions compiled in Rust (<code>v2m_engine</code>):</p> Component Function Audio I/O Direct WAV writing to disk (zero-copy) VAD Ultra-low latency voice detection (Silero ONNX) Ring Buffer Lock-free circular buffer for real-time audio"},{"location":"en/architecture/#data-flow","title":"\ud83d\udd04 Data Flow","text":"<pre><code>sequenceDiagram\n    participant User\n    participant Client as HTTP Client\n    participant API as FastAPI\n    participant WF as Workflows\n    participant Audio as AudioService\n    participant Whisper as TranscriptionService\n\n    User-&gt;&gt;Client: Press shortcut\n    Client-&gt;&gt;API: POST /toggle\n    API-&gt;&gt;WF: toggle() (RecordingWorkflow)\n\n    alt Not recording\n        WF-&gt;&gt;Audio: start_recording()\n        Audio--&gt;&gt;WF: OK\n        WF--&gt;&gt;API: status=recording\n    else Recording\n        WF-&gt;&gt;Audio: stop_recording()\n        Audio--&gt;&gt;WF: audio_buffer\n        WF-&gt;&gt;Whisper: transcribe(buffer)\n        Whisper--&gt;&gt;WF: text\n        WF--&gt;&gt;API: status=idle, text=...\n    end\n\n    API--&gt;&gt;Client: ToggleResponse\n    Client-&gt;&gt;User: Copy to clipboard</code></pre>"},{"location":"en/architecture/#2026-design-principles","title":"\ud83d\udee1\ufe0f 2026 Design Principles","text":"Principle Implementation Local-First No data leaves the machine unless a cloud provider is explicitly configured Privacy-By-Design Audio processed in memory, temp files deleted after transcription Resilience Automatic error recovery, subsystem restart if they fail Observability Structured logging (OpenTelemetry), real-time metrics Performance is Design Async FastAPI, Rust for hot paths, warm model in VRAM"},{"location":"en/changelog/","title":"Changelog","text":"<p>\ud83c\uddea\ud83c\uddf8 Espa\u00f1ol: Consulta el historial de cambios en espa\u00f1ol en .github/locales/es/CHANGELOG.md.</p> <p>title: Changelog description: Change log for the Voice2Machine project. ai_context: \"Versions, Change History, SemVer\" depends_on: [] status: stable</p>"},{"location":"en/changelog/#changelog","title":"Changelog","text":"<p>All notable changes to this project will be documented in this file.</p> <p>The format is based on Keep a Changelog, and this project adheres to Semantic Versioning.</p>"},{"location":"en/changelog/#added","title":"Added","text":"<ul> <li>Zero-Copy Audio Engine: New <code>ZeroCopyAudioRecorder</code> in Rust using shared memory (/dev/shm) for true zero-capacity transfers.</li> <li>Hallucination Detection: Heuristic filters and quality parameters (<code>no_speech</code>, <code>compression_ratio</code>) in <code>StreamingTranscriber</code> to reduce erroneous Whisper outputs.</li> <li>Performance Metrics: Inference latency tracking in logs for detailed diagnostics.</li> </ul>"},{"location":"en/changelog/#changed","title":"Changed","text":"<ul> <li>Advanced Whisper Config: Increased <code>beam_size</code> and <code>best_of</code> to 5 for higher transcription quality in the \"large-v3-turbo\" model.</li> <li>VAD Optimization: Adjusted default threshold to 0.35 to reduce false positives from ambient noise and breathing.</li> <li>Memory Management: Forced CUDA cache reset (<code>torch.cuda.empty_cache()</code>) when unloading models to effectively free VRAM.</li> <li>Code Hygiene: Import refactoring and linting error fixes (<code>ruff</code>) throughout the backend codebase.</li> </ul>"},{"location":"en/changelog/#planned","title":"Planned","text":"<ul> <li>Support for multiple simultaneous transcription languages</li> <li>Web dashboard for real-time monitoring</li> <li>Integration with more LLM providers</li> </ul>"},{"location":"en/changelog/#added_1","title":"Added","text":"<ul> <li>Feature-Based Architecture: Total restructuring into self-contained modules in <code>features/</code> (audio, llm, transcription).</li> <li>Orchestration via Workflows: Introduction of <code>RecordingWorkflow</code> and <code>LLMWorkflow</code> to decouple business logic from the monolithic legacy Orchestrator.</li> <li>Strict Protocols: Implementation of <code>typing.Protocol</code> for all internal services, allowing easy swapping of providers.</li> <li>Modular API: Package structure in <code>api/</code> with separate routes and schemas.</li> </ul>"},{"location":"en/changelog/#changed_1","title":"Changed","text":"<ul> <li>Elimination of Orchestrator: <code>services/orchestrator.py</code> has been decomposed and removed.</li> <li>Infrastructure Refactoring: The <code>infrastructure/</code> folder has been integrated into each corresponding <code>feature</code>.</li> <li>Core and Domain: Simplified and moved to <code>shared/</code> and local interfaces.</li> </ul>"},{"location":"en/changelog/#removed","title":"Removed","text":"<ul> <li>Legacy Audio Tests: Removal of obsolete tests for the Rust extension.</li> <li>System Monitor: System telemetry removed for core simplification.</li> </ul>"},{"location":"en/changelog/#added_2","title":"Added","text":"<ul> <li>FastAPI REST API: New HTTP API replacing the Unix Sockets-based IPC system</li> <li>WebSocket streaming: <code>/ws/events</code> endpoint for real-time provisional transcription</li> <li>Swagger documentation: Interactive UI at <code>/docs</code> for testing endpoints</li> <li>Orchestrator pattern: New coordination pattern that simplifies workflow</li> <li>Rust audio engine: Native <code>v2m_engine</code> extension for low-latency audio capture</li> <li>MkDocs documentation system: Structured documentation with Material theme</li> </ul>"},{"location":"en/changelog/#changed_2","title":"Changed","text":"<ul> <li>Simplified architecture: From CQRS/CommandBus to more direct Orchestrator pattern</li> <li>Communication: From binary Unix Domain Sockets to standard HTTP REST</li> <li>State model: Centralized management in <code>DaemonState</code> with lazy initialization</li> <li>Updated README.md with new architecture</li> </ul>"},{"location":"en/changelog/#removed_1","title":"Removed","text":"<ul> <li><code>daemon.py</code>: Replaced by <code>api.py</code> (FastAPI)</li> <li><code>client.py</code>: No longer needed, use <code>curl</code> or any HTTP client</li> <li>Binary IPC protocol: Replaced by standard JSON</li> </ul>"},{"location":"en/changelog/#fixed","title":"Fixed","text":"<ul> <li>Startup latency: Server starts in ~100ms, model loads in background</li> <li>Memory leaks in WebSocket connections</li> </ul>"},{"location":"en/changelog/#added_3","title":"Added","text":"<ul> <li>Initial Voice2Machine system version</li> <li>Local transcription support with Whisper (faster-whisper)</li> <li>Basic LLM integration (Ollama/Gemini)</li> <li>Unix Domain Sockets-based IPC system</li> <li>Hexagonal architecture with ports and adapters</li> <li>TOML-based configuration</li> </ul>"},{"location":"en/configuration/","title":"\u2699\ufe0f Configuration Guide","text":"<p>Configuration Management</p> <p>Configuration is primarily managed through the Frontend GUI (Gear icon \u2699\ufe0f). However, advanced users can directly edit the <code>config.toml</code> file.</p> <p>File location: <code>$XDG_CONFIG_HOME/v2m/config.toml</code> (usually <code>~/.config/v2m/config.toml</code>).</p>"},{"location":"en/configuration/#1-local-transcription-transcription","title":"1. Local Transcription (<code>[transcription]</code>)","text":"<p>The heart of the system. These parameters control the Faster-Whisper engine.</p> Parameter Type Default Description and Best Practice 2026 <code>model</code> <code>str</code> <code>distil-large-v3</code> Model to load. <code>distil-large-v3</code> offers extreme speed with SOTA accuracy. Options: <code>large-v3-turbo</code>, <code>medium</code>. <code>device</code> <code>str</code> <code>cuda</code> <code>cuda</code> (NVIDIA GPU) is mandatory for real-time experience. <code>cpu</code> is functional but not recommended. <code>compute_type</code> <code>str</code> <code>float16</code> Tensor precision. <code>float16</code> or <code>int8_float16</code> optimize VRAM and throughput on modern GPUs. <code>use_faster_whisper</code> <code>bool</code> <code>true</code> Enables the optimized CTranslate2 backend."},{"location":"en/configuration/#voice-activity-detection-vad","title":"Voice Activity Detection (VAD)","text":"<p>The system uses Silero VAD (Rust version in <code>v2m_engine</code>) to filter silence before invoking Whisper, saving GPU cycles.</p> <ul> <li><code>vad_filter</code> (<code>true</code>): Activates pre-filtering.</li> <li><code>vad_parameters</code>: Fine-tune sensitivity (silence threshold, minimum voice duration).</li> </ul>"},{"location":"en/configuration/#2-llm-services-llm","title":"2. LLM Services (<code>[llm]</code>)","text":"<p>Voice2Machine implements a Provider pattern to support multiple AI backends for text refinement.</p>"},{"location":"en/configuration/#global-configuration","title":"Global Configuration","text":"Parameter Description <code>provider</code> Active provider: <code>gemini</code> (Cloud) or <code>ollama</code> (Local). <code>model</code> Specific model name (e.g., <code>gemini-1.5-flash</code> or <code>llama3:8b</code>)."},{"location":"en/configuration/#specific-providers","title":"Specific Providers","text":""},{"location":"en/configuration/#google-gemini-provider-gemini","title":"Google Gemini (<code>provider = \"gemini\"</code>)","text":"<p>Requires API Key. Ideal for users without powerful GPU (VRAM &lt; 8GB).</p> <ul> <li>Recommended model: <code>gemini-1.5-flash-latest</code> (minimum latency).</li> <li>Temperature: <code>0.3</code> (conservative) for grammar correction.</li> </ul>"},{"location":"en/configuration/#ollama-provider-ollama","title":"Ollama (<code>provider = \"ollama\"</code>)","text":"<p>Total privacy. Requires running the Ollama server (<code>ollama serve</code>).</p> <ul> <li>Endpoint: <code>http://localhost:11434</code></li> <li>Recommended model: <code>qwen2.5:7b</code> or <code>llama3.1:8b</code>.</li> </ul>"},{"location":"en/configuration/#3-recording-recording","title":"3. Recording (<code>[recording]</code>)","text":"<p>Controls audio capture via <code>SoundDevice</code> and <code>v2m_engine</code>.</p> <ul> <li><code>sample_rate</code>: <code>16000</code> (Fixed, required by Whisper).</li> <li><code>channels</code>: <code>1</code> (Mono).</li> <li><code>device_index</code>: Microphone ID. If <code>null</code>, uses system default (PulseAudio/PipeWire).</li> </ul>"},{"location":"en/configuration/#4-system-system","title":"4. System (<code>[system]</code>)","text":"<p>Low-level configuration for the Daemon and communication.</p> <ul> <li><code>host</code>: Server host (<code>127.0.0.1</code> for local-only access).</li> <li><code>port</code>: HTTP port (<code>8765</code> by default).</li> <li><code>log_level</code>: <code>INFO</code> by default. Change to <code>DEBUG</code> for deep diagnostics.</li> </ul>"},{"location":"en/configuration/#secrets-and-security","title":"Secrets and Security","text":"<p>API keys are managed via environment variables or secure storage, never in plain text inside <code>config.toml</code> if possible.</p> <pre><code># Define in .env or system environment\nexport GEMINI_API_KEY=\"AIzaSy_YOUR_KEY_HERE\"\n</code></pre> <p>Important</p> <p>Restart the daemon (using <code>scripts/operations/daemon/restart_daemon.sh</code>) after manually editing the configuration file to apply changes.</p>"},{"location":"en/contributing/","title":"Contributing","text":"<p>title: Contributing Guide description: Instructions and standards for collaborating on Voice2Machine. status: stable last_update: 2026-01-23 language: US English</p> <p>\ud83c\uddea\ud83c\uddf8 Espa\u00f1ol: Si buscas la gu\u00eda en espa\u00f1ol, consulta locales/es/CONTRIBUTING.md.</p>"},{"location":"en/contributing/#contributing-guide","title":"\u2764\ufe0f Contributing Guide","text":"<p>Thank you for your interest in contributing to Voice2Machine! This project is built on collaboration and quality code.</p> <p>To maintain our \"State of the Art 2026\" standards, we follow strict but fair rules. Please read this before submitting your first Pull Request.</p>"},{"location":"en/contributing/#workflow","title":"\ud83d\ude80 Workflow","text":"<ol> <li>Discussion First: Before writing code, open an Issue to discuss the change. This avoids duplicate work or rejections due to architectural misalignment.</li> <li>Fork &amp; Branch:<ul> <li>Fork the repository.</li> <li>Create a descriptive branch: <code>feat/new-gpu-support</code> or <code>fix/transcription-error</code>.</li> </ul> </li> <li>Local Development: Follow the Installation guide to set up your development environment.</li> </ol>"},{"location":"en/contributing/#quality-standards","title":"\ud83d\udccf Quality Standards","text":""},{"location":"en/contributing/#code","title":"Code","text":"<ul> <li>Backend (Python):</li> <li>Strict static typing (100% Type Hints).</li> <li>Linter: <code>ruff check src/ --fix</code>.</li> <li>Formatter: <code>ruff format src/</code>.</li> <li>Tests: <code>pytest</code> must pass at 100%.</li> <li>Frontend (Tauri/React):</li> <li>Strict TypeScript (no <code>any</code>).</li> <li>Linter: <code>npm run lint</code>.</li> <li>Functional components and Hooks.</li> </ul>"},{"location":"en/contributing/#commits","title":"Commits","text":"<p>We use Conventional Commits. Your commit message must follow this format:</p> <pre><code>&lt;type&gt;(&lt;scope&gt;): &lt;short description&gt;\n\n[Optional detailed body]\n</code></pre> <p>Allowed Types:</p> <ul> <li><code>feat</code>: New functionality.</li> <li><code>fix</code>: Bug fix.</li> <li><code>docs</code>: Documentation only.</li> <li><code>refactor</code>: Code change that doesn't fix bugs or add features.</li> <li><code>test</code>: Add or fix tests.</li> <li><code>chore</code>: Maintenance, dependencies.</li> </ul> <p>Example:</p> <p><code>feat(whisper): upgrade to faster-whisper 1.0.0 for 20% speedup</code></p>"},{"location":"en/contributing/#documentation-docs-as-code","title":"Documentation (Docs as Code)","text":"<p>If you change functionality, you must update documentation in <code>docs/docs/</code>.</p> <ul> <li>Verify that <code>mkdocs serve</code> works locally.</li> <li>Follow the Style Guide.</li> </ul>"},{"location":"en/contributing/#pull-request-checklist","title":"\u2705 Pull Request Checklist","text":"<p>Before submitting your PR:</p> <ul> <li> I have run local tests and they pass.</li> <li> I have linted the code (<code>ruff</code>, <code>eslint</code>).</li> <li> I have updated relevant documentation.</li> <li> I have added an entry to <code>CHANGELOG.md</code> (if applicable).</li> <li> My code follows Hexagonal Architecture (no prohibited cross-imports).</li> </ul> <p>Help</p> <p>If you have questions about architecture or design, check the documents in <code>docs/docs/adr/</code> or ask in the corresponding Issue.</p>"},{"location":"en/glossary/","title":"Glossary","text":"<p>This glossary defines technical and domain terms used in Voice2Machine.</p>"},{"location":"en/glossary/#general-terms","title":"General Terms","text":""},{"location":"en/glossary/#local-first","title":"Local-First","text":"<p>Design philosophy where data (audio, text) is processed and stored exclusively on the user's device, without relying on the cloud.</p>"},{"location":"en/glossary/#daemon","title":"Daemon","text":"<p>Background process (written in Python) that manages recording, transcription, and communication with the frontend.</p>"},{"location":"en/glossary/#rest-api","title":"REST API","text":"<p>Communication mechanism between the Daemon (Python) and clients (scripts, frontends). We use FastAPI with standard HTTP endpoints and WebSocket for real-time events.</p>"},{"location":"en/glossary/#technical-components","title":"Technical Components","text":""},{"location":"en/glossary/#whisper","title":"Whisper","text":"<p>Speech recognition model (ASR) developed by OpenAI. Voice2Machine uses <code>faster-whisper</code>, an optimized implementation with CTranslate2.</p>"},{"location":"en/glossary/#workflows","title":"Workflows","text":"<p>Specialized coordination components that manage the complete lifecycle of a specific task (e.g., <code>RecordingWorkflow</code>, <code>LLMWorkflow</code>). They replace the old monolithic \"Orchestrator\" for better traceability and maintainability.</p>"},{"location":"en/glossary/#features","title":"Features","text":"<p>Self-contained modules that group domain logic and its infrastructure adapters (audio, llm, transcription). They represent the system's core capabilities.</p>"},{"location":"en/glossary/#backendprovider","title":"BackendProvider","text":"<p>Frontend component (React Context) that manages connection with the Daemon and distributes state to the UI.</p>"},{"location":"en/glossary/#telemetrycontext","title":"TelemetryContext","text":"<p>Sub-context in React optimized for high-frequency updates (GPU metrics, audio levels) to avoid unnecessary re-renders of the main UI.</p>"},{"location":"en/glossary/#modular-architecture","title":"Modular Architecture","text":"<p>Evolution of Hexagonal Architecture that organizes code around business modules (Features) and execution flows (Workflows), minimizing coupling and maximizing clarity.</p>"},{"location":"en/installation/","title":"\ud83d\udee0\ufe0f Installation and Setup","text":"<p>Prerequisite</p> <p>This project is optimized for Linux (Debian/Ubuntu). State of the Art 2026: We use hardware acceleration (CUDA) and a modular approach to ensure privacy and performance.</p> <p>This guide will take you from zero to a fully functional dictation system on your local machine.</p>"},{"location":"en/installation/#method-1-automatic-installation-recommended","title":"\ud83d\ude80 Method 1: Automatic Installation (Recommended)","text":"<p>We've created a script that handles all the \"dirty work\" for you: verifies your system, installs dependencies (apt), creates the virtual environment (venv), and configures credentials.</p> <pre><code># Run from the project root\n./apps/daemon/backend/scripts/setup/install.sh\n</code></pre> <p>What this script does:</p> <ol> <li>\ud83d\udce6 Installs system libraries (<code>ffmpeg</code>, <code>xclip</code>, <code>pulseaudio-utils</code>).</li> <li>\ud83d\udc0d Creates an isolated Python environment (<code>venv</code>).</li> <li>\u2699\ufe0f Installs project dependencies (<code>faster-whisper</code>, <code>torch</code>).</li> <li>\ud83d\udd11 Helps you configure your Gemini API Key (optional, for generative AI).</li> <li>\ud83d\udda5\ufe0f Verifies if you have a compatible NVIDIA GPU.</li> </ol>"},{"location":"en/installation/#method-2-manual-installation","title":"\ud83d\udee0\ufe0f Method 2: Manual Installation","text":"<p>If you prefer full control or the automatic script fails, follow these steps.</p>"},{"location":"en/installation/#1-system-dependencies-system-level","title":"1. System Dependencies (System Level)","text":"<p>We need tools to manipulate audio and clipboard at the OS level.</p> <pre><code>sudo apt update\nsudo apt install ffmpeg xclip pulseaudio-utils python3-venv build-essential python3-dev\n</code></pre>"},{"location":"en/installation/#2-python-environment","title":"2. Python Environment","text":"<p>We isolate libraries to avoid conflicts.</p> <pre><code># Navigate to the backend directory\ncd apps/daemon/backend\n\n# Create virtual environment\npython3 -m venv venv\n\n# Activate environment (Do this every time you work on the project!)\nsource venv/bin/activate\n\n# Install dependencies\npip install -e .\n</code></pre>"},{"location":"en/installation/#3-ai-configuration-optional","title":"3. AI Configuration (Optional)","text":"<p>To use \"Text Refinement\" features (rewriting with LLM), you need a Google Gemini API Key.</p> <ol> <li>Get your key at Google AI Studio.</li> <li>Create a <code>.env</code> file at the root:</li> </ol> <pre><code>echo 'GEMINI_API_KEY=\"your_api_key_here\"' &gt; .env\n</code></pre>"},{"location":"en/installation/#verification","title":"\u2705 Verification","text":"<p>Make sure everything works before continuing.</p>"},{"location":"en/installation/#1-verify-gpu-acceleration","title":"1. Verify GPU Acceleration","text":"<p>This confirms that Whisper can use your graphics card (essential for speed).</p> <pre><code>python apps/daemon/backend/scripts/diagnostics/check_cuda.py\n</code></pre>"},{"location":"en/installation/#2-system-diagnostics","title":"2. System Diagnostics","text":"<p>Verify that the daemon and audio services are ready.</p> <pre><code>python apps/daemon/backend/scripts/diagnostics/health_check.py\n</code></pre>"},{"location":"en/installation/#next-steps","title":"\u23ed\ufe0f Next Steps","text":"<p>Once installed, it's time to configure how you interact with the tool.</p> <ul> <li>Detailed Configuration - Adjust models and sensitivity.</li> <li>Keyboard Shortcuts - Configure your magic keys.</li> </ul>"},{"location":"en/keyboard_shortcuts/","title":"\u2328\ufe0f Keyboard Shortcuts and Scripts","text":"<p>Integration Philosophy</p> <p>Voice2Machine doesn't hijack your keyboard. It provides \"atomic\" scripts that you bind to your favorite window manager (GNOME, KDE, Hyprland, i3). This ensures universal compatibility and zero background resource consumption for listening to keys.</p>"},{"location":"en/keyboard_shortcuts/#main-scripts","title":"\ud83d\udd17 Main Scripts","text":"<p>To activate features, you must create global shortcuts that execute these scripts located in <code>scripts/</code>.</p>"},{"location":"en/keyboard_shortcuts/#1-dictation-toggle","title":"1. Dictation (Toggle)","text":"<ul> <li>Script: <code>scripts/v2m-toggle.sh</code></li> <li>Function: Recording toggle.</li> <li>Idle State: Starts recording \ud83d\udd34 (Confirmation sound).</li> <li>Recording State: Stops, transcribes, and pastes the text \ud83d\udfe2.</li> <li>Suggested Shortcut: <code>Super + V</code> or mouse side button.</li> </ul>"},{"location":"en/keyboard_shortcuts/#2-ai-refinement","title":"2. AI Refinement","text":"<ul> <li>Script: <code>scripts/v2m-llm.sh</code></li> <li>Function: Contextual text improvement.</li> <li>Reads current clipboard.</li> <li>Sends text to configured LLM provider (Gemini/Ollama).</li> <li>Replaces clipboard with improved version.</li> <li>Suggested Shortcut: <code>Super + G</code>.</li> </ul>"},{"location":"en/keyboard_shortcuts/#configuration-examples","title":"\ud83d\udc27 Configuration Examples","text":""},{"location":"en/keyboard_shortcuts/#gnome-ubuntu","title":"GNOME / Ubuntu","text":"<ol> <li>Go to Settings &gt; Keyboard &gt; Keyboard Shortcuts &gt; View and Customize.</li> <li>Select Custom Shortcuts.</li> <li>Add new:<ul> <li>Name: <code>V2M: Dictate</code></li> <li>Command: <code>/home/your_user/voice2machine/scripts/v2m-toggle.sh</code></li> <li>Shortcut: <code>Super+V</code></li> </ul> </li> </ol>"},{"location":"en/keyboard_shortcuts/#hyprland","title":"Hyprland","text":"<p>In your <code>hyprland.conf</code>:</p> <pre><code>bind = SUPER, V, exec, /home/$USER/voice2machine/scripts/v2m-toggle.sh\nbind = SUPER, G, exec, /home/$USER/voice2machine/scripts/v2m-llm.sh\n</code></pre>"},{"location":"en/keyboard_shortcuts/#i3-sway","title":"i3 / Sway","text":"<p>In your <code>config</code>:</p> <pre><code>bindsym Mod4+v exec --no-startup-id /home/$USER/voice2machine/scripts/v2m-toggle.sh\nbindsym Mod4+g exec --no-startup-id /home/$USER/voice2machine/scripts/v2m-llm.sh\n</code></pre>"},{"location":"en/keyboard_shortcuts/#troubleshooting","title":"\u26a0\ufe0f Troubleshooting","text":"<p>Execution Permissions</p> <p>If the shortcut seems \"dead\", verify that scripts have execution permissions: <code>bash     chmod +x scripts/v2m-toggle.sh scripts/v2m-llm.sh</code></p> <p>Wayland vs X11</p> <p>Scripts automatically detect your graphics server. - X11: Uses <code>xclip</code> and <code>xdotool</code>. - Wayland: Uses <code>wl-copy</code> and <code>wtype</code> (make sure you have them installed if using pure Wayland).</p> <p>Latency</p> <p>These scripts use HTTP requests to communicate with the daemon, ensuring activation latency &lt; 10ms. They don't start a heavy Python instance each time.</p>"},{"location":"en/quick_start/","title":"\ud83d\udd79\ufe0f Quick Start","text":"<p>Executive Summary</p> <p>Voice2Machine has two superpowers: Dictation (Voice \u2192 Text) and Refinement (Text \u2192 Better Text).</p> <p>This visual guide helps you understand the main workflows so you can be productive in minutes.</p>"},{"location":"en/quick_start/#1-dictation-flow-voice-text","title":"1. Dictation Flow (Voice \u2192 Text)","text":"<p>Ideal for: Writing emails, code, or quick messages without touching the keyboard.</p> <ol> <li>Focus: Click on the text field where you want to write.</li> <li>Activate shortcut (Configurable, by default running <code>v2m-toggle.sh</code>). You'll hear a start sound \ud83d\udd14.</li> <li>Speak clearly. Don't worry about being robotic, speak naturally.</li> <li>Press the shortcut again to stop. You'll hear an end sound \ud83d\udd15.</li> <li>The text will paste automatically into your active field (or remain in clipboard if auto-paste is disabled).</li> </ol> <pre><code>flowchart LR\n    A((\ud83c\udfa4 START)) --&gt;|Record| B{Local Whisper}\n    B --&gt;|Transcribe| C[\ud83d\udccb Clipboard / Paste]\n\n    style A fill:#ff6b6b,stroke:#333,stroke-width:2px,color:white\n    style B fill:#feca57,stroke:#333,stroke-width:2px\n    style C fill:#48dbfb,stroke:#333,stroke-width:2px</code></pre>"},{"location":"en/quick_start/#2-refinement-flow-text-ai-text","title":"2. Refinement Flow (Text \u2192 AI \u2192 Text)","text":"<p>Ideal for: Correcting grammar, translating, or giving professional formatting to a rough draft.</p> <ol> <li>Select and Copy (<code>Ctrl + C</code>) the text you want to improve.</li> <li>Activate the AI shortcut (running <code>v2m-llm.sh</code>).</li> <li>Wait a few seconds (the AI is thinking \ud83e\udde0).</li> <li>The improved text will replace your clipboard contents.</li> <li>Paste (<code>Ctrl + V</code>) the result.</li> </ol> <pre><code>flowchart LR\n    A[\ud83d\udccb Original Text] --&gt;|Copy| B((\ud83e\udde0 AI SHORTCUT))\n    B --&gt;|Process| C{Local LLM / Gemini}\n    C --&gt;|Improve| D[\u2728 Polished Text]\n\n    style A fill:#c8d6e5,stroke:#333,stroke-width:2px\n    style B fill:#5f27cd,stroke:#333,stroke-width:2px,color:white\n    style C fill:#feca57,stroke:#333,stroke-width:2px\n    style D fill:#1dd1a1,stroke:#333,stroke-width:2px</code></pre>"},{"location":"en/quick_start/#pro-tips","title":"\ud83d\udca1 Pro Tips","text":"<p>!!! tip \"Improve Your Accuracy\" - Speak fluently: Whisper understands context from complete sentences better than isolated words. - Hardware: A noise-canceling microphone dramatically improves results. - Configuration: You can adjust the LLM \"temperature\" in settings to make it more creative or more literal.</p> <p>Privacy Guaranteed</p> <p>Dictation is 100% local (runs on your GPU). Refinement can be local (Ollama) or cloud (Gemini), you have full control in settings.</p>"},{"location":"en/style_guide/","title":"Style Guide and Governance","text":"<p>This guide defines the standards for Voice2Machine documentation, aligned with \"State of the Art 2026\".</p>"},{"location":"en/style_guide/#fundamental-principles","title":"Fundamental Principles","text":"<ol> <li>Docs as Code: Documentation lives in the repository, is versioned with Git, and validated in CI/CD.</li> <li>Universal Accessibility: Strict WCAG 2.1 Level AA compliance.</li> <li>Localization: The source of truth (<code>docs/</code>) is in Native Latin American Spanish. Root files (<code>README.md</code>, <code>AGENTS.md</code>) are in English (USA) and Spanish.</li> </ol>"},{"location":"en/style_guide/#accessibility-wcag-21-aa","title":"Accessibility (WCAG 2.1 AA)","text":"<ul> <li>Alt Text: All images must have descriptive <code>alt text</code>.</li> <li>Heading Hierarchy: Don't skip levels (H1 -&gt; H2 -&gt; H3).</li> <li>Contrast: Diagrams and screenshots must have high contrast.</li> <li>Links: Use descriptive text (\"see installation guide\" instead of \"click here\").</li> </ul>"},{"location":"en/style_guide/#tone-and-voice","title":"Tone and Voice","text":"<ul> <li>Audience: Developers and technical users.</li> <li>Tone: Professional, concise, direct (\"Do this\" instead of \"You could do this\").</li> <li>Person: Second person (\"Configure your environment\") or impersonal (\"The environment is configured\").</li> <li>English: Standard American English. Avoid excessive local idioms.</li> </ul>"},{"location":"en/style_guide/#markdown-structure","title":"Markdown Structure","text":""},{"location":"en/style_guide/#admonitions-notes","title":"Admonitions (Notes)","text":"<p>Use admonition blocks to highlight information:</p> <pre><code>!!! note \"Note\"\nNeutral information.\n\n!!! tip \"Tip\"\nOptimization help.\n\n!!! warning \"Warning\"\nBe careful with this.\n\n!!! danger \"Danger\"\nRisk of data loss.\n</code></pre>"},{"location":"en/style_guide/#code","title":"Code","text":"<p>Code blocks with specified language:</p> <pre><code>def my_function():\n    pass\n</code></pre>"},{"location":"en/style_guide/#governance-process","title":"Governance Process","text":"<ol> <li>Changes: Any code change affecting functionality requires documentation update in the same PR.</li> <li>Review: Documentation PRs require human review.</li> <li>Maintenance: Quarterly obsolescence review.</li> </ol>"},{"location":"en/troubleshooting/","title":"\ud83d\udd27 Troubleshooting","text":"<p>Golden Rule</p> <p>For any problem, the first step is always to check system logs. <code>bash     # View logs in real-time     tail -f ~/.local/state/v2m/v2m.log</code></p>"},{"location":"en/troubleshooting/#audio-and-recording","title":"\ud83d\uded1 Audio and Recording","text":""},{"location":"en/troubleshooting/#no-sound-empty-transcription","title":"No sound / Empty transcription","text":"<ul> <li>Symptom: Recording starts and stops, but no text is generated.</li> <li>Diagnosis:   Run the audio diagnostic script:   <pre><code>python scripts/diagnose_audio.py\n</code></pre></li> <li>Solutions:</li> <li>Audio Driver: Voice2Machine uses <code>SoundDevice</code>. Make sure your system (PulseAudio/PipeWire) has an active default microphone.</li> <li>Permissions: On Linux, your user must belong to the <code>audio</code> group (<code>sudo usermod -aG audio $USER</code>).</li> </ul>"},{"location":"en/troubleshooting/#cut-off-or-incomplete-phrases","title":"Cut-off or incomplete phrases","text":"<ul> <li>Cause: The silence detector (VAD) is too aggressive.</li> <li>Solution:   Adjust settings in <code>config.toml</code> or via the GUI:</li> <li>Reduce the <code>threshold</code> (e.g., from <code>0.35</code> to <code>0.30</code>).</li> <li>Increase <code>min_silence_duration_ms</code> (e.g., to <code>800ms</code>).</li> </ul>"},{"location":"en/troubleshooting/#performance-and-gpu","title":"\ud83d\udc22 Performance and GPU","text":""},{"location":"en/troubleshooting/#slow-transcription-2-seconds","title":"Slow transcription (&gt; 2 seconds)","text":"<ul> <li>Probable Cause: Whisper is running on CPU instead of GPU.</li> <li>Verification:   <pre><code>python scripts/test_whisper_gpu.py\n</code></pre></li> <li>Solution:</li> <li>Install updated NVIDIA drivers (CUDA 12 compatible).</li> <li>Verify <code>config.toml</code> has <code>device = \"cuda\"</code>.</li> <li>If you don't have a dedicated GPU, switch model to <code>distil-medium.en</code> or <code>base</code>.</li> </ul>"},{"location":"en/troubleshooting/#error-cuda-out-of-memory","title":"Error <code>CUDA out of memory</code>","text":"<ul> <li>Cause: Your GPU doesn't have enough VRAM for the selected model.</li> <li>Solution:</li> <li>Change <code>compute_type</code> to <code>int8_float16</code> (reduces VRAM usage by half).</li> <li>Use a lighter model (<code>distil-large-v3</code> consumes less than original <code>large-v3</code>).</li> </ul>"},{"location":"en/troubleshooting/#connectivity-and-daemon","title":"\ud83d\udd0c Connectivity and Daemon","text":""},{"location":"en/troubleshooting/#connection-refused-in-gui-or-scripts","title":"\"Connection refused\" in GUI or Scripts","text":"<ul> <li>Cause: The backend process (Python) isn't running or the server crashed.</li> <li>Solution:</li> <li>Verify status:       <pre><code>pgrep -a python | grep v2m\n</code></pre></li> <li>If not running, start manually to see startup errors:       <pre><code>python -m v2m.main\n</code></pre></li> <li>If it says \"Address already in use\", kill the existing process:       <pre><code>pkill -f \"v2m.main\"\n</code></pre></li> </ul>"},{"location":"en/troubleshooting/#keyboard-shortcuts-dont-respond","title":"Keyboard shortcuts don't respond","text":"<ul> <li>Cause: Permission issue or incorrect path in window manager configuration.</li> <li>Solution:</li> <li>Run the script manually in terminal: <code>scripts/v2m-toggle.sh</code>.</li> <li>If it works, the error is in your shortcut configuration (e.g., relative path <code>~/</code> instead of <code>/home/...</code>).</li> <li>If it doesn't work, verify permissions: <code>chmod +x scripts/*.sh</code>.</li> </ul>"},{"location":"en/troubleshooting/#ai-errors-llm","title":"\ud83e\udde0 AI Errors (LLM)","text":""},{"location":"en/troubleshooting/#error-401403-with-gemini","title":"Error 401/403 with Gemini","text":"<ul> <li>Cause: Invalid or expired API Key.</li> <li>Solution: Regenerate your key at Google AI Studio and update the <code>.env</code> file or <code>GEMINI_API_KEY</code> environment variable.</li> </ul>"},{"location":"en/troubleshooting/#connection-refused-with-ollama","title":"\"Connection refused\" with Ollama","text":"<ul> <li>Cause: The Ollama server isn't running.</li> <li>Solution: Run <code>ollama serve</code> in another terminal.</li> </ul>"},{"location":"en/adr/","title":"Architecture Decision Records (ADRs)","text":"<p>An Architecture Decision Record (ADR) is a document that captures an important architectural decision, along with its context and consequences.</p>"},{"location":"en/adr/#index-of-decisions","title":"Index of Decisions","text":"ADR Title Status Date ADR-001 Migration from Unix Sockets IPC to FastAPI REST Accepted 2025-01 ADR-002 Replacement of CQRS/CommandBus with Orchestrator Accepted 2025-01 ADR-003 Selection of faster-whisper over whisper.cpp Accepted 2024-06 ADR-004 Hexagonal Architecture (Ports and Adapters) Accepted 2024-03 ADR-005 Rust Audio Engine (v2m_engine) Accepted 2025-01 ADR-006 Local-first: Processing Without Cloud Accepted 2024-03"},{"location":"en/adr/#when-to-write-an-adr","title":"When to Write an ADR?","text":"<p>Write an ADR when you make a significant decision that affects the project's structure, dependencies, interfaces, or technology.</p> <p>See ADR Template for the format.</p>"},{"location":"en/adr/001-fastapi-migration/","title":"ADR-001: Migration from Unix Sockets IPC to FastAPI REST","text":""},{"location":"en/adr/001-fastapi-migration/#status","title":"Status","text":"<p>Accepted</p>"},{"location":"en/adr/001-fastapi-migration/#date","title":"Date","text":"<p>2025-01-15</p>"},{"location":"en/adr/001-fastapi-migration/#context","title":"Context","text":"<p>The original Voice2Machine system used Unix Domain Sockets with a custom binary protocol for communication between the Daemon (Python) and clients (scripts, Tauri frontend).</p>"},{"location":"en/adr/001-fastapi-migration/#previous-system-limitations","title":"Previous system limitations:","text":"<ol> <li>Debugging complexity: Binary messages required specialized tools for inspection</li> <li>Learning curve: New developers needed to understand the proprietary protocol</li> <li>Standard tools incompatibility: Couldn't use <code>curl</code>, Postman, or browsers for testing</li> <li>Protocol maintenance: Every change required synchronized client and server updates</li> <li>Interactive documentation: No way to auto-generate docs</li> </ol>"},{"location":"en/adr/001-fastapi-migration/#requirements","title":"Requirements:","text":"<ul> <li>Maintain latency &lt; 50ms for critical operations (toggle)</li> <li>Allow real-time event streaming</li> <li>Simplify onboarding for new developers</li> <li>Facilitate testing and debugging</li> </ul>"},{"location":"en/adr/001-fastapi-migration/#decision","title":"Decision","text":"<p>Migrate to FastAPI as REST API framework, completely replacing the proprietary IPC system.</p>"},{"location":"en/adr/001-fastapi-migration/#implementation","title":"Implementation:","text":"<ul> <li>FastAPI + Uvicorn: Async HTTP server with performance comparable to Go/Rust</li> <li>WebSocket: For event streaming (provisional transcription)</li> <li>Pydantic V2: Automatic validation and OpenAPI schema generation</li> <li>Swagger UI: Interactive documentation at <code>/docs</code></li> </ul>"},{"location":"en/adr/001-fastapi-migration/#consequences","title":"Consequences","text":""},{"location":"en/adr/001-fastapi-migration/#positive","title":"Positive","text":"<ul> <li>\u2705 Trivial debugging: <code>curl -X POST localhost:8765/toggle | jq</code></li> <li>\u2705 Automatic documentation: Swagger UI included without extra effort</li> <li>\u2705 Standard ecosystem: Compatible with any HTTP client</li> <li>\u2705 Simplified testing: FastAPI TestClient for integration tests</li> <li>\u2705 Fast onboarding: A Junior can understand the API in minutes</li> </ul>"},{"location":"en/adr/001-fastapi-migration/#negative","title":"Negative","text":"<ul> <li>\u26a0\ufe0f HTTP overhead: ~2-5ms additional vs raw sockets (acceptable)</li> <li>\u26a0\ufe0f Exposed port: Requires firewall configuration (mitigated with <code>127.0.0.1</code> only)</li> <li>\u26a0\ufe0f Additional dependency: FastAPI + Uvicorn (~2MB)</li> </ul>"},{"location":"en/adr/001-fastapi-migration/#alternatives-considered","title":"Alternatives Considered","text":""},{"location":"en/adr/001-fastapi-migration/#grpc","title":"gRPC","text":"<ul> <li>Rejected: Requires additional tooling (protoc), similar learning curve to original IPC, no native Swagger UI.</li> </ul>"},{"location":"en/adr/001-fastapi-migration/#graphql","title":"GraphQL","text":"<ul> <li>Rejected: Unnecessary overhead for simple RPC-style operations, greater complexity.</li> </ul>"},{"location":"en/adr/001-fastapi-migration/#keep-unix-sockets","title":"Keep Unix Sockets","text":"<ul> <li>Rejected: Didn't solve debugging and onboarding problems.</li> </ul>"},{"location":"en/adr/001-fastapi-migration/#references","title":"References","text":"<ul> <li>FastAPI Documentation</li> <li>Uvicorn Performance</li> </ul>"},{"location":"en/adr/002-orchestrator-pattern/","title":"ADR-002: Replacement of CQRS/CommandBus with Orchestrator","text":""},{"location":"en/adr/002-orchestrator-pattern/#status","title":"Status","text":"<p>Accepted</p>"},{"location":"en/adr/002-orchestrator-pattern/#date","title":"Date","text":"<p>2025-01-15</p>"},{"location":"en/adr/002-orchestrator-pattern/#context","title":"Context","text":"<p>The original backend implemented the CQRS (Command Query Responsibility Segregation) pattern with a CommandBus to process user actions.</p>"},{"location":"en/adr/002-orchestrator-pattern/#identified-problems","title":"Identified problems:","text":"<ol> <li>Over-engineering: For a system with ~10 commands, CQRS overhead was disproportionate</li> <li>Excessive indirection: Command \u2192 CommandBus \u2192 Handler \u2192 Result \u2192 Response</li> <li>Boilerplate: Each new feature required creating Command DTO + Handler + registering in bus</li> <li>Complex debugging: Deep stack traces obscured the real flow</li> <li>Verbose testing: CommandBus mocks in every test</li> </ol>"},{"location":"en/adr/002-orchestrator-pattern/#requirements","title":"Requirements:","text":"<ul> <li>Maintain separation of concerns (don't couple API to infrastructure)</li> <li>Simplify control flow</li> <li>Reduce boilerplate for new features</li> <li>Facilitate testing and debugging</li> </ul>"},{"location":"en/adr/002-orchestrator-pattern/#decision","title":"Decision","text":"<p>Replace CQRS/CommandBus with a central Orchestrator that coordinates workflow directly.</p>"},{"location":"en/adr/002-orchestrator-pattern/#implementation","title":"Implementation:","text":"<pre><code>class Orchestrator:\n    async def toggle(self) -&gt; ToggleResponse: ...\n    async def start(self) -&gt; ToggleResponse: ...\n    async def stop(self) -&gt; ToggleResponse: ...\n</code></pre> <p>The Orchestrator:</p> <ul> <li>Exposes direct methods for each operation</li> <li>Coordinates adapters (AudioRecorder, WhisperAdapter, LLMProvider)</li> <li>Maintains system state</li> <li>Emits events to WebSocket clients</li> </ul>"},{"location":"en/adr/002-orchestrator-pattern/#consequences","title":"Consequences","text":""},{"location":"en/adr/002-orchestrator-pattern/#positive","title":"Positive","text":"<ul> <li>\u2705 Explicit flow: API \u2192 Orchestrator \u2192 Adapters (3 layers vs 6+)</li> <li>\u2705 Less code: Eliminated ~500 LOC of CommandBus infrastructure</li> <li>\u2705 Simple debugging: Clear and short stack traces</li> <li>\u2705 Direct testing: Mock adapters, not abstract buses</li> <li>\u2705 Onboarding: New devs understand the system in minutes</li> </ul>"},{"location":"en/adr/002-orchestrator-pattern/#negative","title":"Negative","text":"<ul> <li>\u26a0\ufe0f Less extensible: Adding a global \"middleware\" is less trivial</li> <li>\u26a0\ufe0f Orchestrator \"god object\": Risk of growing too large (mitigated with composition)</li> </ul>"},{"location":"en/adr/002-orchestrator-pattern/#alternatives-considered","title":"Alternatives Considered","text":""},{"location":"en/adr/002-orchestrator-pattern/#keep-simplified-cqrs","title":"Keep simplified CQRS","text":"<ul> <li>Rejected: Even simplified, the conceptual overhead wasn't justified.</li> </ul>"},{"location":"en/adr/002-orchestrator-pattern/#event-sourcing","title":"Event Sourcing","text":"<ul> <li>Rejected: Even greater over-engineering for current use case.</li> </ul>"},{"location":"en/adr/002-orchestrator-pattern/#simple-functions-no-class","title":"Simple Functions (no class)","text":"<ul> <li>Rejected: Lost state management and lifecycle.</li> </ul>"},{"location":"en/adr/002-orchestrator-pattern/#references","title":"References","text":"<ul> <li>Martin Fowler on CQRS</li> <li>When not to use CQRS</li> </ul>"},{"location":"en/adr/003-faster-whisper/","title":"ADR-003: Selection of faster-whisper over whisper.cpp","text":""},{"location":"en/adr/003-faster-whisper/#status","title":"Status","text":"<p>Accepted</p>"},{"location":"en/adr/003-faster-whisper/#date","title":"Date","text":"<p>2024-06-01</p>"},{"location":"en/adr/003-faster-whisper/#context","title":"Context","text":"<p>For local voice transcription, we needed to choose a Whisper model implementation from OpenAI that would maximize performance on consumer NVIDIA GPUs (RTX 3060-4090).</p>"},{"location":"en/adr/003-faster-whisper/#options-evaluated","title":"Options evaluated:","text":"<ol> <li>OpenAI Whisper (original): Reference implementation in PyTorch</li> <li>whisper.cpp: Pure C++ implementation with CUDA support</li> <li>faster-whisper: Implementation on CTranslate2 (optimized C++/CUDA)</li> </ol>"},{"location":"en/adr/003-faster-whisper/#requirements","title":"Requirements:","text":"<ul> <li>Latency &lt; 500ms for 5 seconds of audio (8x real-time minimum)</li> <li>Support for <code>large-v3</code> models and <code>distil</code> variants</li> <li>INT8/FP16 quantization to optimize VRAM</li> <li>Python API for backend integration</li> <li>Audio streaming/chunking</li> </ul>"},{"location":"en/adr/003-faster-whisper/#decision","title":"Decision","text":"<p>Adopt faster-whisper as the main transcription engine.</p>"},{"location":"en/adr/003-faster-whisper/#justification","title":"Justification:","text":"Criteria Whisper (PyTorch) whisper.cpp faster-whisper Speed 1x (baseline) 4x 4-8x VRAM (large-v3) 10GB 6GB 4-5GB Python API \u2705 Native \u274c Bindings \u2705 Excellent Quantization Limited \u2705 \u2705 INT8/FP16 Maintenance OpenAI Community Active (Systran)"},{"location":"en/adr/003-faster-whisper/#consequences","title":"Consequences","text":""},{"location":"en/adr/003-faster-whisper/#positive","title":"Positive","text":"<ul> <li>\u2705 4-8x faster than original Whisper with same accuracy</li> <li>\u2705 ~50% less VRAM: Allows using large-v3 on 6GB GPUs</li> <li>\u2705 Pythonic API: Natural integration with FastAPI async</li> <li>\u2705 Distil models support: <code>distil-large-v3</code> for minimum latency</li> </ul>"},{"location":"en/adr/003-faster-whisper/#negative","title":"Negative","text":"<ul> <li>\u26a0\ufe0f Additional dependency: CTranslate2 binary (~100MB)</li> <li>\u26a0\ufe0f Less portable: Requires compatible CUDA toolkit</li> <li>\u26a0\ufe0f Lag on new models: New OpenAI releases take ~2 weeks to be available</li> </ul>"},{"location":"en/adr/003-faster-whisper/#alternatives-considered","title":"Alternatives Considered","text":""},{"location":"en/adr/003-faster-whisper/#whispercpp","title":"whisper.cpp","text":"<ul> <li>Rejected: Immature Python bindings, more complex debugging.</li> </ul>"},{"location":"en/adr/003-faster-whisper/#openai-whisper","title":"OpenAI Whisper","text":"<ul> <li>Rejected: Too slow for real-time experience without enterprise hardware.</li> </ul>"},{"location":"en/adr/003-faster-whisper/#whisper-jax","title":"Whisper JAX","text":"<ul> <li>Rejected: Requires TPU or complex JAX on CUDA configuration.</li> </ul>"},{"location":"en/adr/003-faster-whisper/#references","title":"References","text":"<ul> <li>faster-whisper GitHub</li> <li>CTranslate2</li> <li>Whisper Benchmarks</li> </ul>"},{"location":"en/adr/004-hexagonal-architecture/","title":"ADR-004: Hexagonal Architecture (Ports and Adapters)","text":""},{"location":"en/adr/004-hexagonal-architecture/#status","title":"Status","text":"<p>Accepted</p>"},{"location":"en/adr/004-hexagonal-architecture/#date","title":"Date","text":"<p>2024-03-01</p>"},{"location":"en/adr/004-hexagonal-architecture/#context","title":"Context","text":"<p>Voice2Machine started as a monolithic script of ~200 lines. As functionality grew, we faced typical problems of coupled code:</p> <ol> <li>Difficult testing: Mocks of GPU, audio, external APIs</li> <li>Cascade changes: Modifying Whisper required touching 5+ files</li> <li>Vendor lock-in: Switching from Ollama to Gemini required massive refactor</li> <li>Diffuse responsibilities: Unclear where to put new logic</li> </ol>"},{"location":"en/adr/004-hexagonal-architecture/#requirements","title":"Requirements:","text":"<ul> <li>Framework-agnostic business core</li> <li>Interchangeable adapters (e.g., swap Whisper for another ASR)</li> <li>Testability without real hardware</li> <li>Clear boundaries between layers</li> </ul>"},{"location":"en/adr/004-hexagonal-architecture/#decision","title":"Decision","text":"<p>Adopt Hexagonal Architecture (Ports &amp; Adapters) as the structural pattern.</p>"},{"location":"en/adr/004-hexagonal-architecture/#folder-structure","title":"Folder structure:","text":"<pre><code>src/v2m/\n\u251c\u2500\u2500 core/           # Configuration, logging, base interfaces\n\u251c\u2500\u2500 domain/         # Models, ports (interfaces), errors\n\u251c\u2500\u2500 services/       # Orchestrator, coordination\n\u2514\u2500\u2500 infrastructure/ # Adapters (Whisper, Audio, LLM)\n</code></pre>"},{"location":"en/adr/004-hexagonal-architecture/#port-implementation","title":"Port implementation:","text":"<pre><code>from typing import Protocol, runtime_checkable\n\n@runtime_checkable\nclass TranscriptionService(Protocol):\n    async def transcribe(self, audio: bytes) -&gt; str: ...\n</code></pre> <p>Adapters implement ports:</p> <pre><code>class WhisperAdapter:\n    async def transcribe(self, audio: bytes) -&gt; str:\n        # Concrete implementation with faster-whisper\n</code></pre>"},{"location":"en/adr/004-hexagonal-architecture/#consequences","title":"Consequences","text":""},{"location":"en/adr/004-hexagonal-architecture/#positive","title":"Positive","text":"<ul> <li>\u2705 Isolated testing: Unit tests without GPU or network</li> <li>\u2705 Flexibility: Switching Gemini for Ollama is editing 1 file</li> <li>\u2705 Onboarding: Predictable and documented structure</li> <li>\u2705 Type safety: <code>Protocol</code> + mypy detects incompatibilities at compile time</li> </ul>"},{"location":"en/adr/004-hexagonal-architecture/#negative","title":"Negative","text":"<ul> <li>\u26a0\ufe0f More files: ~20 files vs ~5 from original script</li> <li>\u26a0\ufe0f Indirection: Must navigate between layers to understand complete flow</li> <li>\u26a0\ufe0f Initial overhead: More complex setup for simple features</li> </ul>"},{"location":"en/adr/004-hexagonal-architecture/#alternatives-considered","title":"Alternatives Considered","text":""},{"location":"en/adr/004-hexagonal-architecture/#clean-architecture-uncle-bob","title":"Clean Architecture (Uncle Bob)","text":"<ul> <li>Rejected: Too many layers (Entities, Use Cases, Interface Adapters, Frameworks) for the scope.</li> </ul>"},{"location":"en/adr/004-hexagonal-architecture/#mvcmvp","title":"MVC/MVP","text":"<ul> <li>Rejected: UI-oriented, doesn't apply well to a backend daemon.</li> </ul>"},{"location":"en/adr/004-hexagonal-architecture/#simple-modules","title":"Simple Modules","text":"<ul> <li>Rejected: In practice, we returned to original coupling.</li> </ul>"},{"location":"en/adr/004-hexagonal-architecture/#references","title":"References","text":"<ul> <li>Alistair Cockburn - Hexagonal Architecture</li> <li>Netflix - Ready for changes with Hexagonal Architecture</li> </ul>"},{"location":"en/adr/005-rust-audio-engine/","title":"ADR-005: Rust Audio Engine (v2m_engine)","text":""},{"location":"en/adr/005-rust-audio-engine/#status","title":"Status","text":"<p>Accepted</p>"},{"location":"en/adr/005-rust-audio-engine/#date","title":"Date","text":"<p>2025-01-10</p>"},{"location":"en/adr/005-rust-audio-engine/#context","title":"Context","text":"<p>Audio capture in pure Python presented critical limitations for a real-time dictation experience:</p>"},{"location":"en/adr/005-rust-audio-engine/#identified-problems","title":"Identified problems:","text":"<ol> <li>GIL blocking: Audio capture competed with transcription for the GIL</li> <li>Variable latency: 10-50ms jitter in audio buffering</li> <li>sounddevice overhead: Python callbacks added latency</li> <li>Inefficient VAD: Silero VAD in Python processed samples with overhead</li> </ol>"},{"location":"en/adr/005-rust-audio-engine/#requirements","title":"Requirements:","text":"<ul> <li>Capture latency &lt; 10ms</li> <li>Pre-processed VAD before Python</li> <li>Lock-free circular buffer</li> <li>Zero-copy when possible</li> </ul>"},{"location":"en/adr/005-rust-audio-engine/#decision","title":"Decision","text":"<p>Develop native Rust extension (<code>v2m_engine</code>) for critical audio tasks.</p>"},{"location":"en/adr/005-rust-audio-engine/#components","title":"Components:","text":"<pre><code>v2m_engine/\n\u251c\u2500\u2500 src/\n\u2502   \u251c\u2500\u2500 audio_capture.rs  # CPAL-based capture\n\u2502   \u251c\u2500\u2500 ring_buffer.rs    # Lock-free circular buffer\n\u2502   \u251c\u2500\u2500 vad.rs            # Silero ONNX inference\n\u2502   \u2514\u2500\u2500 lib.rs            # PyO3 bindings\n</code></pre>"},{"location":"en/adr/005-rust-audio-engine/#python-interface","title":"Python interface:","text":"<pre><code>from v2m_engine import AudioCapture, VADProcessor\n\ncapture = AudioCapture(sample_rate=16000, buffer_size=4096)\nvad = VADProcessor(threshold=0.5)\n\nasync with capture.stream() as audio:\n    if vad.contains_speech(audio):\n        await transcriber.process(audio)\n</code></pre>"},{"location":"en/adr/005-rust-audio-engine/#consequences","title":"Consequences","text":""},{"location":"en/adr/005-rust-audio-engine/#positive","title":"Positive","text":"<ul> <li>\u2705 Latency &lt; 5ms: CPAL + lock-free buffers</li> <li>\u2705 GIL-free: Audio thread independent from Python</li> <li>\u2705 Efficient VAD: Native ONNX runtime, 10x faster</li> <li>\u2705 Zero-copy: NumPy arrays share memory with Rust</li> </ul>"},{"location":"en/adr/005-rust-audio-engine/#negative","title":"Negative","text":"<ul> <li>\u26a0\ufe0f Build complexity: Requires Rust toolchain + maturin</li> <li>\u26a0\ufe0f Cross-language debugging: Mixed Python/Rust stack traces</li> <li>\u26a0\ufe0f Portability: Platform-specific binaries</li> </ul>"},{"location":"en/adr/005-rust-audio-engine/#alternatives-considered","title":"Alternatives Considered","text":""},{"location":"en/adr/005-rust-audio-engine/#pure-python-sounddevice-numpy","title":"Pure Python (sounddevice + numpy)","text":"<ul> <li>Rejected: GIL blocking and unacceptable latency.</li> </ul>"},{"location":"en/adr/005-rust-audio-engine/#cython","title":"Cython","text":"<ul> <li>Rejected: Still tied to GIL, limited benefits.</li> </ul>"},{"location":"en/adr/005-rust-audio-engine/#c-extension","title":"C++ Extension","text":"<ul> <li>Rejected: Rust offers memory safety without GC, better tooling (cargo).</li> </ul>"},{"location":"en/adr/005-rust-audio-engine/#references","title":"References","text":"<ul> <li>PyO3 - Rust bindings for Python</li> <li>CPAL - Cross-platform Audio Library</li> <li>Lock-free Ring Buffer</li> </ul>"},{"location":"en/adr/006-local-first/","title":"ADR-006: Local-first: Processing Without Cloud","text":""},{"location":"en/adr/006-local-first/#status","title":"Status","text":"<p>Accepted</p>"},{"location":"en/adr/006-local-first/#date","title":"Date","text":"<p>2024-03-01</p>"},{"location":"en/adr/006-local-first/#context","title":"Context","text":"<p>Existing dictation services (Google Speech-to-Text, Whisper API, Dragon) require sending audio to external servers.</p>"},{"location":"en/adr/006-local-first/#problems-with-cloud-based-dictation","title":"Problems with cloud-based dictation:","text":"<ol> <li>Privacy: Sensitive audio (medical, legal, personal) leaves the machine</li> <li>Network latency: 100-500ms additional RTT</li> <li>Availability: Requires internet connection</li> <li>Costs: Transcription APIs charge per minute</li> <li>Rate limits: Throttling on intensive use</li> </ol>"},{"location":"en/adr/006-local-first/#user-requirements","title":"User requirements:","text":"<ul> <li>Absolute privacy: No data should leave the machine</li> <li>Offline operation: System must work without internet</li> <li>Minimum latency: &lt; 500ms end-to-end</li> <li>Zero cost: No subscriptions or usage fees</li> </ul>"},{"location":"en/adr/006-local-first/#decision","title":"Decision","text":"<p>Adopt \"Local-first\" philosophy where all voice processing occurs on the user's device.</p>"},{"location":"en/adr/006-local-first/#implementation","title":"Implementation:","text":"Component Local Solution Transcription faster-whisper on local GPU LLM (optional) Ollama with local models Audio Processed in RAM Storage Only temporary files, deleted post-use"},{"location":"en/adr/006-local-first/#configurable-exceptions","title":"Configurable exceptions:","text":"<p>User can opt-in to cloud services for text refinement:</p> <ul> <li>Google Gemini API (for LLM)</li> <li>But never for raw audio</li> </ul>"},{"location":"en/adr/006-local-first/#consequences","title":"Consequences","text":""},{"location":"en/adr/006-local-first/#positive","title":"Positive","text":"<ul> <li>\u2705 Guaranteed privacy: Audio never leaves the device</li> <li>\u2705 No network latency: All processing local</li> <li>\u2705 Works offline: No internet required for dictation</li> <li>\u2705 Predictable cost: Only hardware (GPU), no subscriptions</li> <li>\u2705 Compliance: Compatible with regulations (HIPAA, GDPR)</li> </ul>"},{"location":"en/adr/006-local-first/#negative","title":"Negative","text":"<ul> <li>\u26a0\ufe0f Requires GPU: Without NVIDIA GPU, degraded performance</li> <li>\u26a0\ufe0f Local LLM models: Lower quality than GPT-4/Gemini Pro</li> <li>\u26a0\ufe0f Manual updates: Models don't auto-update</li> </ul>"},{"location":"en/adr/006-local-first/#alternatives-considered","title":"Alternatives Considered","text":""},{"location":"en/adr/006-local-first/#hybrid-local-stt-cloud-llm-default","title":"Hybrid (local STT + cloud LLM default)","text":"<ul> <li>Rejected: Violates privacy-by-default principle.</li> </ul>"},{"location":"en/adr/006-local-first/#cloud-first-with-local-cache","title":"Cloud-first with local cache","text":"<ul> <li>Rejected: Unnecessary complexity, audio still needs to be uploaded.</li> </ul>"},{"location":"en/adr/006-local-first/#federated-learning","title":"Federated Learning","text":"<ul> <li>Rejected: Over-engineering for current scope.</li> </ul>"},{"location":"en/adr/006-local-first/#references","title":"References","text":"<ul> <li>Local-first Software</li> <li>Ink &amp; Switch - Seven Ideals</li> <li>GDPR and Voice Data</li> </ul>"},{"location":"en/adr/template/","title":"ADR-XXX: Short Decision Title","text":""},{"location":"en/adr/template/#status","title":"Status","text":"<p>[Proposed | Accepted | Rejected | Obsolete]</p>"},{"location":"en/adr/template/#context","title":"Context","text":"<p>Describe the context and problem we are solving.</p> <ul> <li>What is the current limitation?</li> <li>What technical or business requirements drive this?</li> </ul>"},{"location":"en/adr/template/#decision","title":"Decision","text":"<p>Describe the decision made.</p> <ul> <li>\"We will use X technology for Y component...\"</li> </ul>"},{"location":"en/adr/template/#consequences","title":"Consequences","text":"<p>What becomes easier or harder because of this change?</p>"},{"location":"en/adr/template/#positive","title":"Positive","text":"<p>-</p>"},{"location":"en/adr/template/#negative","title":"Negative","text":"<p>-</p>"},{"location":"en/adr/template/#alternatives-considered","title":"Alternatives Considered","text":"<ul> <li>Option A: Why rejected.</li> <li>Option B: Why rejected.</li> </ul>"},{"location":"en/api/","title":"Python API - Index","text":"<p>This section provides auto-generated documentation of Python classes and functions from the Voice2Machine backend.</p> <p>Generated with mkdocstrings</p> <p>This documentation is automatically extracted from source code docstrings. For the most up-to-date version, always check the code at <code>apps/daemon/backend/src/v2m/</code>.</p>"},{"location":"en/api/#main-modules","title":"Main Modules","text":""},{"location":"en/api/#interfaces","title":"Interfaces","text":"<p>Protocols and contracts that define the expected behavior of adapters.</p>"},{"location":"en/api/#domain","title":"Domain","text":"<p>Domain models, ports, and error types.</p>"},{"location":"en/api/#services","title":"Services","text":"<p>Application services including the main Orchestrator.</p>"},{"location":"en/api/#quick-navigation","title":"Quick Navigation","text":"Class/Function Description <code>Orchestrator</code> Central workflow coordinator <code>TranscriptionService</code> Port for transcription services <code>AudioRecorder</code> Interface for audio capture <code>LLMProvider</code> Base interface for AI providers"},{"location":"en/api/domain/","title":"Domain","text":"<p>This page documents domain models and data types.</p>"},{"location":"en/api/domain/#data-models-pydantic-v2","title":"Data Models (Pydantic V2)","text":"<p>Voice2Machine data models use Pydantic V2 for strict validation and fast serialization.</p>"},{"location":"en/api/domain/#api-models-schemas","title":"API Models (Schemas)","text":"<p>Located in <code>v2m.api.schemas</code>, they define input/output contracts for the REST API and WebSockets.</p> Class Purpose <code>StatusResponse</code> Current daemon state (idle, recording, etc) <code>ToggleResponse</code> Result of starting/stopping recording <code>TranscriptionUpdate</code> Streaming event with provisional text <code>LLMResponse</code> Text processing result"},{"location":"en/api/domain/#correctionresult","title":"CorrectionResult","text":"<p>Structured output model for text refinement (used by LLM Workflows).</p> <pre><code>class CorrectionResult(BaseModel):\n    corrected_text: str = Field(description=\"Corrected text\")\n    explanation: str | None = Field(default=None, description=\"Changes made\")\n</code></pre>"},{"location":"en/api/domain/#exceptions","title":"Exceptions","text":"<p>The system uses an exception hierarchy based on <code>ApplicationError</code>.</p> Exception Context <code>AudioError</code> Hardware or audio buffer failures <code>TranscriptionError</code> Whisper model or VRAM failures <code>LLMProviderError</code> Connection or quota errors with Gemini/Ollama <code>ConfigError</code> Validation error in <code>config.toml</code>"},{"location":"en/api/interfaces/","title":"Protocols and Contracts","text":"<p>This page documents the protocols (interfaces) that define system contracts in \"State of the Art 2026\".</p>"},{"location":"en/api/interfaces/#workflows-protocol","title":"Workflows (Protocol)","text":"<p>Workflows follow a standard protocol to ensure the API can interact with them generically.</p> <pre><code>from typing import Protocol, runtime_checkable\n\n@runtime_checkable\nclass Workflow(Protocol):\n    async def execute(self, *args, **kwargs):\n        \"\"\"Main entry point for workflow execution.\"\"\"\n        ...\n</code></pre>"},{"location":"en/api/interfaces/#feature-adapters","title":"Feature Adapters","text":"<p>Each <code>Feature</code> defines its own interface using Python protocols to allow switching implementations (e.g., swapping <code>Gemini</code> for <code>Ollama</code>).</p>"},{"location":"en/api/interfaces/#llmprovider","title":"LLMProvider","text":"<pre><code>class LLMProvider(Protocol):\n    async def process(self, text: str, system_prompt: str) -&gt; str:\n        \"\"\"Process text using a language model.\"\"\"\n</code></pre>"},{"location":"en/api/interfaces/#audiosource","title":"AudioSource","text":"<pre><code>class AudioSource(Protocol):\n    def read(self, frames: int) -&gt; np.ndarray:\n        \"\"\"Reads audio frames from buffer.\"\"\"\n</code></pre>"},{"location":"en/api/services/","title":"Services and Application Logic","text":"<p>In the 2026 model, application logic is divided into Workflows and Features.</p>"},{"location":"en/api/services/#workflows","title":"Workflows","text":"<p>Located in <code>v2m.orchestration</code>. They are the high-level coordinators.</p> <ul> <li>RecordingWorkflow: Manages the audio -&gt; VAD -&gt; transcription -&gt; paste cycle.</li> <li>LLMWorkflow: Manages the text -&gt; refinement/translation -&gt; paste cycle.</li> </ul>"},{"location":"en/api/services/#features","title":"Features","text":"<p>Located in <code>v2m.features</code>. They are the functional building blocks of the system.</p> Feature Purpose Main Implementation Audio Capture and pre-processing <code>v2m_engine</code> (Rust) Transcription Audio-to-text conversion <code>faster-whisper</code> LLM Intelligent processing <code>gemini</code> / <code>ollama</code>"},{"location":"en/api/services/#lazy-initialization","title":"Lazy Initialization","text":"<p>To minimize resource consumption (especially VRAM), heavy services are initialized only when first required.</p>"},{"location":"en/api/backend/","title":"Backend API (Python)","text":"<p>This section contains documentation automatically generated from the Voice2Machine backend source code.</p> <p>Auto-generated</p> <p>This documentation syncs automatically with code docstrings. Source of truth: <code>apps/daemon/backend/src/v2m/</code></p>"},{"location":"en/api/backend/#main-modules","title":"Main Modules","text":""},{"location":"en/api/backend/#orchestration","title":"Orchestration","text":"<ul> <li>Workflows - Business workflow coordinators (Recording, LLM)</li> <li>REST API - FastAPI endpoints and data models (<code>api/</code> package)</li> </ul>"},{"location":"en/api/backend/#foundation","title":"Foundation","text":"<ul> <li>Config - Typed configuration system (<code>shared/config/</code>)</li> </ul>"},{"location":"en/api/backend/#features","title":"Features","text":"<ul> <li>Transcription - Whisper and inference engines</li> <li>LLM Services - Gemini, Ollama, and Local Providers</li> </ul>"},{"location":"en/api/backend/#layer-navigation","title":"Layer Navigation","text":"<pre><code>graph TD\n    A[REST API] --&gt; B[Workflows]\n    B --&gt; C[Features]\n    C --&gt; D[Audio]\n    C --&gt; E[Transcription]\n    C --&gt; F[LLM]\n\n    style A fill:#e3f2fd\n    style B fill:#fff3e0\n    style C fill:#f3e5f5</code></pre> Layer Responsibility API HTTP endpoints, validation, serialization Orchestration Business workflow coordination (Workflows) Features Domain logic and specialized adapters"},{"location":"en/api/backend/#code-status","title":"Code Status","text":"Metric Value Python Files 27 Docstring Coverage ~70% Style Google Style"},{"location":"en/api/backend/api/","title":"REST API (Backend)","text":"<p>Documentation for FastAPI endpoints and data models.</p>"},{"location":"en/api/backend/api/#requestresponse-models-schemas","title":"Request/Response Models (Schemas)","text":"<p>options: show_source: false</p> <p>options: show_source: false</p> <p>options: show_source: false</p> <p>options: show_source: false</p> <p>options: show_source: false</p>"},{"location":"en/api/backend/api/#global-state","title":"Global State","text":"<p>options: show_source: true members: - init - recording - llm - broadcast_event</p>"},{"location":"en/api/backend/api/#recording","title":"Recording","text":"<p>options: show_source: true</p> <p>options: show_source: true</p> <p>options: show_source: true</p>"},{"location":"en/api/backend/api/#status","title":"Status","text":"<p>options: show_source: true</p> <p>options: show_source: true</p>"},{"location":"en/api/backend/config/","title":"Configuration","text":"<p>Typed configuration system using Pydantic Settings.</p>"},{"location":"en/api/backend/config/#main-settings","title":"Main Settings","text":"<p>options: show_source: false members: - paths - transcription - llm - notifications</p>"},{"location":"en/api/backend/config/#paths-configuration","title":"Paths Configuration","text":"<p>options: show_source: false</p>"},{"location":"en/api/backend/config/#transcription-configuration","title":"Transcription Configuration","text":"<p>options: show_source: false</p> <p>options: show_source: false</p> <p>options: show_source: false</p>"},{"location":"en/api/backend/config/#llm-configuration","title":"LLM Configuration","text":"<p>options: show_source: false</p> <p>options: show_source: false</p> <p>options: show_source: false</p> <p>options: show_source: false</p>"},{"location":"en/api/backend/config/#notifications-configuration","title":"Notifications Configuration","text":"<p>options: show_source: false</p>"},{"location":"en/api/backend/llm/","title":"LLM Services","text":"<p>Language model providers for text processing.</p>"},{"location":"en/api/backend/llm/#google-gemini-cloud","title":"Google Gemini (Cloud)","text":"<p>LLM service connecting to Google Gemini API for text processing and translations.</p> <p>Location: <code>v2m/features/llm/gemini_service.py</code></p> <p>Main methods:</p> <ul> <li><code>process_text(text: str) -&gt; str</code> - Refines text with punctuation and grammar</li> <li><code>translate_text(text: str, target_lang: str) -&gt; str</code> - Translates text</li> </ul>"},{"location":"en/api/backend/llm/#ollama-local","title":"Ollama (Local)","text":"<p>Local LLM service connecting to Ollama server for total privacy.</p> <p>Location: <code>v2m/features/llm/ollama_service.py</code></p> <p>Configuration: <code>http://localhost:11434</code></p>"},{"location":"en/api/backend/llm/#local-llamacpp","title":"Local (llama.cpp)","text":"<p>Embedded LLM service using llama-cpp-python directly.</p> <p>Location: <code>v2m/features/llm/local_service.py</code></p>"},{"location":"en/api/backend/llm/#design-pattern","title":"Design Pattern","text":"<p>All LLM services implement a common interface (Protocol):</p> <pre><code>class ILLMService(Protocol):\n    async def process_text(self, text: str) -&gt; str:\n        \"\"\"Refines text with grammar and punctuation.\"\"\"\n        ...\n\n    async def translate_text(self, text: str, target_lang: str) -&gt; str:\n        \"\"\"Translates text to specified language.\"\"\"\n        ...\n</code></pre> <p>The <code>LLMWorkflow</code> selects the backend based on <code>config.llm.provider</code>:</p> <ul> <li><code>\"gemini\"</code> \u2192 GeminiLLMService</li> <li><code>\"ollama\"</code> \u2192 OllamaLLMService</li> <li><code>\"local\"</code> \u2192 LocalLLMService</li> </ul>"},{"location":"en/api/backend/transcription/","title":"Transcription","text":"<p>Audio to text transcription services using faster-whisper.</p>"},{"location":"en/api/backend/transcription/#persistentwhisperworker","title":"PersistentWhisperWorker","text":"<p>Persistent worker that keeps the Whisper model loaded in VRAM between sessions.</p> <p>Location: <code>v2m/features/transcription/persistent_model.py</code></p>"},{"location":"en/api/backend/transcription/#features","title":"Features","text":"<ul> <li>Lazy Loading: Model loads the first time it's needed</li> <li>Keep-Warm Policy: Keeps model in VRAM based on configuration</li> <li>GPU Optimized: Uses <code>float16</code> or <code>int8_float16</code> for maximum performance</li> </ul>"},{"location":"en/api/backend/transcription/#methods","title":"Methods","text":"<pre><code>class PersistentWhisperWorker:\n    def initialize_sync(self) -&gt; None:\n        \"\"\"Loads model into VRAM (synchronous, for warmup).\"\"\"\n\n    async def transcribe(self, audio: np.ndarray) -&gt; str:\n        \"\"\"Transcribes audio to text.\"\"\"\n\n    async def unload(self) -&gt; None:\n        \"\"\"Releases model from VRAM.\"\"\"\n</code></pre>"},{"location":"en/api/backend/transcription/#streamingtranscriber","title":"StreamingTranscriber","text":"<p>Real-time transcriber providing provisional feedback while the user speaks.</p> <p>Location: <code>v2m/features/audio/streaming_transcriber.py</code></p>"},{"location":"en/api/backend/transcription/#data-flow","title":"Data Flow","text":"<pre><code>graph LR\n    A[AudioRecorder] --&gt; B[VAD]\n    B --&gt; C[StreamingTranscriber]\n    C --&gt; D[WhisperWorker]\n    D --&gt; E[WebSocket]\n\n    style C fill:#e3f2fd</code></pre>"},{"location":"en/api/backend/transcription/#integration","title":"Integration","text":"<p>The <code>StreamingTranscriber</code> emits events via WebSocket:</p> <ul> <li><code>transcription_update</code>: Provisional text during recording</li> <li><code>transcription_final</code>: Final text on stop</li> </ul>"},{"location":"en/api/backend/workflows/","title":"Workflows (Orchestration)","text":"<p>Workflows are components in charge of coordinating the different system functionalities to complete complex business tasks.</p>"},{"location":"en/api/backend/workflows/#recordingworkflow","title":"RecordingWorkflow","text":"<p>Manages the entire recording process, from initial capture to final transcription.</p> <p>options: show_source: true</p>"},{"location":"en/api/backend/workflows/#llmworkflow","title":"LLMWorkflow","text":"<p>Coordinates text processing using language providers (LLM), including refinement and translation.</p> <p>options: show_source: true</p>"},{"location":"en/contribucion/","title":"Contributing","text":"<p>title: Gu\u00eda de Contribuci\u00f3n description: Instrucciones y est\u00e1ndares para colaborar en Voice2Machine. status: stable last_update: 2026-01-23 language: Native Latin American Spanish</p>"}]}
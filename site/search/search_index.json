{"config":{"lang":["es","en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"\ud83d\udde3\ufe0f Voice2Machine: Dictado por Voz Local","text":""},{"location":"#proposito","title":"\ud83c\udfaf Prop\u00f3sito","text":"<p>El objetivo es simple:</p> <p>Poder dictar texto en cualquier lugar del sistema operativo.</p> <p>La idea es transcribir audio utilizando tu GPU local para obtener la m\u00e1xima velocidad y precisi\u00f3n, sin importar la aplicaci\u00f3n que est\u00e9s usando (editor de c\u00f3digo, navegador, chat, etc.).</p> <p>Este proyecto transforma un script simple en una aplicaci\u00f3n modular robusta basada en un Backend Daemon (Python), dise\u00f1ada bajo Arquitectura Hexagonal para garantizar mantenibilidad, escalabilidad y privacidad absoluta.</p>"},{"location":"#documentacion","title":"\ud83d\udcda Documentaci\u00f3n","text":"<p>La documentaci\u00f3n est\u00e1 organizada para servir a diferentes necesidades:</p>"},{"location":"#exploracion","title":"\ud83d\ude80 Exploraci\u00f3n","text":"<ul> <li>Gu\u00eda R\u00e1pida: Comienza a dictar en minutos.</li> <li>Glosario: Define t\u00e9rminos clave como Daemon, Whisper y API REST.</li> </ul>"},{"location":"#procedimientos","title":"\ud83d\udee0\ufe0f Procedimientos","text":"<ul> <li>Instalaci\u00f3n: Gu\u00eda paso a paso para Ubuntu/Debian.</li> <li>Contribuci\u00f3n: C\u00f3mo colaborar en el proyecto.</li> </ul>"},{"location":"#referencia","title":"\u2699\ufe0f Referencia","text":"<ul> <li>Configuraci\u00f3n: Ajusta modelos, dispositivos y comportamientos.</li> <li>Atajos de Teclado: Referencia de comandos globales.</li> <li>API REST: Documentaci\u00f3n de endpoints HTTP y WebSocket.</li> <li>API Python: Referencia de clases y m\u00e9todos del backend.</li> </ul>"},{"location":"#conceptos","title":"\ud83e\udde0 Conceptos","text":"<ul> <li>Arquitectura: Dise\u00f1o Hexagonal y componentes del sistema.</li> <li>Decisiones (ADR): Registro de decisiones t\u00e9cnicas importantes.</li> </ul>"},{"location":"#mantenimiento","title":"\ud83d\udd27 Mantenimiento","text":"<ul> <li>Soluci\u00f3n de Problemas: Diagn\u00f3stico y correcci\u00f3n de errores comunes.</li> <li>Changelog: Historial de cambios del proyecto.</li> </ul>"},{"location":"arquitectura/","title":"\ud83e\udde9 Arquitectura del Sistema","text":"<p>Filosof\u00eda T\u00e9cnica</p> <p>Voice2Machine implementa una Arquitectura Hexagonal (Ports &amp; Adapters) estricta, priorizando el desacoplamiento, la testabilidad y la independencia tecnol\u00f3gica. El sistema se adhiere a est\u00e1ndares SOTA 2026 como tipos est\u00e1ticos en Python (Protocol) y separaci\u00f3n Frontend/Backend mediante API REST.</p>"},{"location":"arquitectura/#diagrama-de-alto-nivel","title":"\ud83c\udfd7\ufe0f Diagrama de Alto Nivel","text":"<pre><code>graph TD\n    subgraph Clients [\"\ud83d\udd0c Clientes (CLI / Scripts / GUI / Tauri)\"]\n        ClientApp[\"Cualquier cliente HTTP\"]\n    end\n\n    subgraph Backend [\"\ud83d\udc0d Backend Daemon (Python + FastAPI)\"]\n        API[\"FastAPI Server&lt;br&gt;(api.py)\"]\n\n        subgraph Hexagon [\"Hexagon (Core)\"]\n            Orchestrator[\"Orchestrator&lt;br&gt;(Coordinaci\u00f3n)\"]\n            Domain[\"Domain&lt;br&gt;(Interfaces/Models)\"]\n        end\n\n        subgraph Infra [\"Infrastructure (Adapters)\"]\n            Whisper[\"Whisper Adapter&lt;br&gt;(faster-whisper)\"]\n            Audio[\"Audio Engine&lt;br&gt;(Rust v2m_engine)\"]\n            LLM[\"LLM Providers&lt;br&gt;(Gemini/Ollama)\"]\n        end\n    end\n\n    ClientApp &lt;--&gt;|REST + WebSocket| API\n    API --&gt; Orchestrator\n    Orchestrator --&gt; Domain\n    Whisper -.-&gt;|Implements| Domain\n    Audio -.-&gt;|Implements| Domain\n    LLM -.-&gt;|Implements| Domain\n\n    style Clients fill:#e3f2fd,stroke:#1565c0\n    style Backend fill:#e8f5e9,stroke:#2e7d32\n    style Hexagon fill:#fff3e0,stroke:#ef6c00\n    style Infra fill:#f3e5f5,stroke:#7b1fa2</code></pre>"},{"location":"arquitectura/#componentes-del-backend","title":"\ud83d\udce6 Componentes del Backend","text":""},{"location":"arquitectura/#1-api-layer-fastapi","title":"1. API Layer (FastAPI)","text":"<p>Ubicado en <code>apps/daemon/backend/src/v2m/api.py</code>.</p> <ul> <li>Endpoints REST: <code>/toggle</code>, <code>/start</code>, <code>/stop</code>, <code>/status</code>, <code>/health</code></li> <li>WebSocket: <code>/ws/events</code> para streaming de transcripci\u00f3n en tiempo real</li> <li>Documentaci\u00f3n autom\u00e1tica: Swagger UI en <code>/docs</code></li> </ul> <p>Migraci\u00f3n Completada</p> <p>El sistema anterior usaba Unix Domain Sockets con protocolo binario personalizado. Desde v0.2.0, usamos FastAPI para simplicidad y compatibilidad con cualquier cliente HTTP.</p>"},{"location":"arquitectura/#2-orchestrator-coordinacion","title":"2. Orchestrator (Coordinaci\u00f3n)","text":"<p>Ubicado en <code>apps/daemon/backend/src/v2m/services/orchestrator.py</code>.</p> <p>El Orchestrator es el punto central de coordinaci\u00f3n que:</p> <ul> <li>Gestiona el ciclo de vida completo: grabaci\u00f3n \u2192 transcripci\u00f3n \u2192 post-procesamiento</li> <li>Mantiene el estado del sistema (idle, recording, processing)</li> <li>Coordina la comunicaci\u00f3n entre adaptadores sin acoplarlos directamente</li> <li>Emite eventos a clientes WebSocket conectados</li> </ul> <pre><code>class Orchestrator:\n    async def toggle(self) -&gt; ToggleResponse: ...\n    async def start(self) -&gt; ToggleResponse: ...\n    async def stop(self) -&gt; ToggleResponse: ...\n    async def warmup(self) -&gt; None: ...\n</code></pre>"},{"location":"arquitectura/#3-core-el-hexagono","title":"3. Core (El Hex\u00e1gono)","text":"<p>Ubicado en <code>apps/daemon/backend/src/v2m/core/</code> y <code>domain/</code>.</p> <ul> <li>Puertos (Interfaces): Definidos usando <code>typing.Protocol</code> + <code>@runtime_checkable</code> para chequeo estructural en tiempo de ejecuci\u00f3n</li> <li>Modelos de Dominio: DTOs con Pydantic V2 para validaci\u00f3n autom\u00e1tica</li> <li>Contratos estrictos: Los adaptadores implementan interfaces, no clases concretas</li> </ul>"},{"location":"arquitectura/#4-infrastructure-adapters","title":"4. Infrastructure (Adapters)","text":"<p>Ubicado en <code>apps/daemon/backend/src/v2m/infrastructure/</code>.</p> Adapter Responsabilidad WhisperAdapter Transcripci\u00f3n con <code>faster-whisper</code>. Lazy loading para ahorrar VRAM AudioRecorder Captura de audio usando extensi\u00f3n Rust (<code>v2m_engine</code>) LLMProviders Factory para Gemini/Ollama seg\u00fan configuraci\u00f3n SystemMonitor Telemetr\u00eda de GPU/CPU en tiempo real"},{"location":"arquitectura/#comunicacion-cliente-backend","title":"\u26a1 Comunicaci\u00f3n Cliente-Backend","text":"<p>Voice2Machine utiliza FastAPI REST + WebSocket para la comunicaci\u00f3n:</p>"},{"location":"arquitectura/#rest-sincrono","title":"REST (S\u00edncrono)","text":"<pre><code># Toggle grabaci\u00f3n\ncurl -X POST http://localhost:8765/toggle | jq\n\n# Verificar estado\ncurl http://localhost:8765/status | jq\n</code></pre>"},{"location":"arquitectura/#websocket-streaming","title":"WebSocket (Streaming)","text":"<pre><code>const ws = new WebSocket(\"ws://localhost:8765/ws/events\");\nws.onmessage = (e) =&gt; {\n  const { event, data } = JSON.parse(e.data);\n  if (event === \"transcription_update\") {\n    console.log(data.text, data.final);\n  }\n};\n</code></pre>"},{"location":"arquitectura/#extensiones-nativas-rust","title":"\ud83e\udd80 Extensiones Nativas (Rust)","text":"<p>Para tareas cr\u00edticas donde el GIL de Python es un cuello de botella, utilizamos extensiones nativas compiladas en Rust (<code>v2m_engine</code>):</p> Componente Funci\u00f3n Audio I/O Escritura de WAVs directa a disco (zero-copy) VAD Detecci\u00f3n de voz de ultra-baja latencia (Silero ONNX) Buffer Ring Buffer circular lock-free para audio en tiempo real"},{"location":"arquitectura/#flujo-de-datos","title":"\ud83d\udd04 Flujo de Datos","text":"<pre><code>sequenceDiagram\n    participant User\n    participant Client as Cliente HTTP\n    participant API as FastAPI\n    participant Orch as Orchestrator\n    participant Audio as AudioRecorder\n    participant Whisper as WhisperAdapter\n\n    User-&gt;&gt;Client: Presiona atajo\n    Client-&gt;&gt;API: POST /toggle\n    API-&gt;&gt;Orch: toggle()\n\n    alt No grabando\n        Orch-&gt;&gt;Audio: start_recording()\n        Audio--&gt;&gt;Orch: OK\n        Orch--&gt;&gt;API: status=recording\n    else Grabando\n        Orch-&gt;&gt;Audio: stop_recording()\n        Audio--&gt;&gt;Orch: audio_buffer\n        Orch-&gt;&gt;Whisper: transcribe(buffer)\n        Whisper--&gt;&gt;Orch: texto\n        Orch--&gt;&gt;API: status=idle, text=...\n    end\n\n    API--&gt;&gt;Client: ToggleResponse\n    Client-&gt;&gt;User: Copia al clipboard</code></pre>"},{"location":"arquitectura/#principios-de-diseno-2026","title":"\ud83d\udee1\ufe0f Principios de Dise\u00f1o 2026","text":"Principio Implementaci\u00f3n Local-First Ning\u00fan dato sale de la m\u00e1quina a menos que se configure expl\u00edcitamente un proveedor cloud Privacy-By-Design Audio procesado en memoria, archivos temporales eliminados despu\u00e9s de transcripci\u00f3n Resiliencia Recuperaci\u00f3n autom\u00e1tica de errores, reinicio de subsistemas si fallan Observabilidad Logging estructurado (OpenTelemetry), m\u00e9tricas en tiempo real Performance is Design FastAPI async, Rust para hot paths, modelo warm en VRAM"},{"location":"atajos_teclado/","title":"\u2328\ufe0f Atajos de Teclado y Scripts","text":"<p>Filosof\u00eda de Integraci\u00f3n</p> <p>Voice2Machine no secuestra tu teclado. Proporciona scripts \"at\u00f3micos\" que t\u00fa vinculas a tu gestor de ventanas favorito (GNOME, KDE, Hyprland, i3). Esto garantiza compatibilidad universal y cero consumo de recursos en segundo plano para escuchar teclas.</p>"},{"location":"atajos_teclado/#scripts-principales","title":"\ud83d\udd17 Scripts Principales","text":"<p>Para activar las funciones, debes crear atajos globales que ejecuten estos scripts ubicados en <code>scripts/</code>.</p>"},{"location":"atajos_teclado/#1-dictado-toggle","title":"1. Dictado (Toggle)","text":"<ul> <li>Script: <code>scripts/v2m-toggle.sh</code></li> <li>Funci\u00f3n: Interruptor de grabaci\u00f3n.<ul> <li>Estado Inactivo: Inicia grabaci\u00f3n \ud83d\udd34 (Sonido de confirmaci\u00f3n).</li> <li>Estado Grabando: Detiene, transcribe y pega el texto \ud83d\udfe2.</li> </ul> </li> <li>Atajo Sugerido: <code>Super + V</code> o bot\u00f3n lateral del mouse.</li> </ul>"},{"location":"atajos_teclado/#2-refinado-con-ia","title":"2. Refinado con IA","text":"<ul> <li>Script: <code>scripts/v2m-llm.sh</code></li> <li>Funci\u00f3n: Mejora de texto contextual.<ul> <li>Lee el portapapeles actual.</li> <li>Env\u00eda el texto al proveedor LLM configurado (Gemini/Ollama).</li> <li>Reemplaza el portapapeles con la versi\u00f3n mejorada.</li> </ul> </li> <li>Atajo Sugerido: <code>Super + G</code>.</li> </ul>"},{"location":"atajos_teclado/#ejemplos-de-configuracion","title":"\ud83d\udc27 Ejemplos de Configuraci\u00f3n","text":""},{"location":"atajos_teclado/#gnome-ubuntu","title":"GNOME / Ubuntu","text":"<ol> <li>Ve a Configuraci\u00f3n &gt; Teclado &gt; Atajos de teclado &gt; Ver y personalizar.</li> <li>Selecciona Atajos personalizados.</li> <li>A\u00f1ade nuevo:<ul> <li>Nombre: <code>V2M: Dictar</code></li> <li>Comando: <code>/home/tu_usuario/voice2machine/scripts/v2m-toggle.sh</code></li> <li>Atajo: <code>Super+V</code></li> </ul> </li> </ol>"},{"location":"atajos_teclado/#hyprland","title":"Hyprland","text":"<p>En tu <code>hyprland.conf</code>:</p> <pre><code>bind = SUPER, V, exec, /home/$USER/voice2machine/scripts/v2m-toggle.sh\nbind = SUPER, G, exec, /home/$USER/voice2machine/scripts/v2m-llm.sh\n</code></pre>"},{"location":"atajos_teclado/#i3-sway","title":"i3 / Sway","text":"<p>En tu <code>config</code>:</p> <pre><code>bindsym Mod4+v exec --no-startup-id /home/$USER/voice2machine/scripts/v2m-toggle.sh\nbindsym Mod4+g exec --no-startup-id /home/$USER/voice2machine/scripts/v2m-llm.sh\n</code></pre>"},{"location":"atajos_teclado/#solucion-de-problemas","title":"\u26a0\ufe0f Soluci\u00f3n de Problemas","text":"<p>Permisos de Ejecuci\u00f3n</p> <p>Si el atajo parece \"muerto\", verifica que los scripts tengan permiso de ejecuci\u00f3n: <pre><code>chmod +x scripts/v2m-toggle.sh scripts/v2m-llm.sh\n</code></pre></p> <p>Wayland vs X11</p> <p>Los scripts detectan autom\u00e1ticamente tu servidor gr\u00e1fico. - X11: Usa <code>xclip</code> y <code>xdotool</code>. - Wayland: Usa <code>wl-copy</code> y <code>wtype</code> (aseg\u00farate de tenerlos instalados si usas Wayland puro).</p> <p>Latencia</p> <p>Estos scripts usan comunicaci\u00f3n por sockets crudos (raw sockets) para hablar con el demonio, asegurando una latencia de activaci\u00f3n &lt; 10ms. No inician una instancia de Python pesada cada vez.</p>"},{"location":"changelog/","title":"Changelog","text":"<p>Todos los cambios notables en este proyecto ser\u00e1n documentados en este archivo.</p> <p>El formato se basa en Keep a Changelog, y este proyecto se adhiere a Semantic Versioning.</p>"},{"location":"changelog/#020-2025-01-20","title":"[0.2.0] - 2025-01-20","text":""},{"location":"changelog/#added","title":"Added","text":"<ul> <li>FastAPI REST API: Nueva API HTTP que reemplaza el sistema IPC basado en Unix Sockets</li> <li>WebSocket streaming: Endpoint <code>/ws/events</code> para transcripci\u00f3n provisional en tiempo real</li> <li>Documentaci\u00f3n Swagger: UI interactiva en <code>/docs</code> para probar endpoints</li> <li>Orchestrator pattern: Nuevo patr\u00f3n de coordinaci\u00f3n que simplifica el flujo de trabajo</li> <li>Rust audio engine: Extensi\u00f3n nativa <code>v2m_engine</code> para captura de audio de baja latencia</li> <li>Sistema de documentaci\u00f3n MkDocs: Documentaci\u00f3n estructurada con Material theme</li> </ul>"},{"location":"changelog/#changed","title":"Changed","text":"<ul> <li>Arquitectura simplificada: De CQRS/CommandBus a Orchestrator pattern m\u00e1s directo</li> <li>Comunicaci\u00f3n: De Unix Domain Sockets binarios a HTTP REST est\u00e1ndar</li> <li>Modelo de estado: Gesti\u00f3n centralizada en <code>DaemonState</code> con lazy initialization</li> <li>Actualizaci\u00f3n de README.md con nueva arquitectura</li> </ul>"},{"location":"changelog/#removed","title":"Removed","text":"<ul> <li><code>daemon.py</code>: Reemplazado por <code>api.py</code> (FastAPI)</li> <li><code>client.py</code>: Ya no necesario, usar <code>curl</code> o cualquier cliente HTTP</li> <li>Protocolo IPC binario: Reemplazado por JSON est\u00e1ndar</li> </ul>"},{"location":"changelog/#fixed","title":"Fixed","text":"<ul> <li>Latencia de arranque: El servidor inicia en ~100ms, modelo carga en background</li> <li>Memory leaks en WebSocket connections</li> </ul>"},{"location":"changelog/#unreleased","title":"[Unreleased]","text":""},{"location":"changelog/#planned","title":"Planned","text":"<ul> <li>Soporte para m\u00faltiples idiomas de transcripci\u00f3n simult\u00e1neos</li> <li>Dashboard web para monitoreo en tiempo real</li> <li>Integraci\u00f3n con m\u00e1s proveedores LLM</li> </ul>"},{"location":"changelog/#010-2024-03-20","title":"[0.1.0] - 2024-03-20","text":""},{"location":"changelog/#added_1","title":"Added","text":"<ul> <li>Versi\u00f3n inicial del sistema Voice2Machine</li> <li>Soporte para transcripci\u00f3n local con Whisper (faster-whisper)</li> <li>Integraci\u00f3n b\u00e1sica con LLMs (Ollama/Gemini)</li> <li>Sistema IPC basado en Unix Domain Sockets</li> <li>Arquitectura hexagonal con puertos y adaptadores</li> <li>Configuraci\u00f3n mediante TOML</li> </ul>"},{"location":"configuracion/","title":"\u2699\ufe0f Gu\u00eda de Configuraci\u00f3n","text":"<p>Gesti\u00f3n de Configuraci\u00f3n</p> <p>La configuraci\u00f3n se gestiona principalmente a trav\u00e9s de la interfaz gr\u00e1fica del Frontend (Icono de engranaje \u2699\ufe0f). Sin embargo, los usuarios avanzados pueden editar directamente el archivo <code>config.toml</code>.</p> <p>Ubicaci\u00f3n del archivo: <code>$XDG_CONFIG_HOME/v2m/config.toml</code> (usualmente <code>~/.config/v2m/config.toml</code>).</p>"},{"location":"configuracion/#1-transcripcion-local-transcription","title":"1. Transcripci\u00f3n Local (<code>[transcription]</code>)","text":"<p>El coraz\u00f3n del sistema. Estos par\u00e1metros controlan el motor Faster-Whisper.</p> Par\u00e1metro Tipo Default Descripci\u00f3n y \"Best Practice\" 2026 <code>model</code> <code>str</code> <code>distil-large-v3</code> Modelo a cargar. <code>distil-large-v3</code> ofrece velocidad extrema con precisi\u00f3n SOTA. Opciones: <code>large-v3-turbo</code>, <code>medium</code>. <code>device</code> <code>str</code> <code>cuda</code> <code>cuda</code> (GPU NVIDIA) es mandatorio para experiencia en tiempo real. <code>cpu</code> es funcional pero no recomendado. <code>compute_type</code> <code>str</code> <code>float16</code> Precisi\u00f3n de tensores. <code>float16</code> o <code>int8_float16</code> optimizan VRAM y throughput en GPUs modernas. <code>use_faster_whisper</code> <code>bool</code> <code>true</code> Habilita el backend optimizado CTranslate2."},{"location":"configuracion/#deteccion-de-voz-vad","title":"Detecci\u00f3n de Voz (VAD)","text":"<p>El sistema utiliza Silero VAD (versi\u00f3n Rust en <code>v2m_engine</code>) para filtrar silencio antes de invocar a Whisper, ahorrando GPU.</p> <ul> <li><code>vad_filter</code> (<code>true</code>): Activa el pre-filtrado.</li> <li><code>vad_parameters</code>: Ajuste fino de sensibilidad (umbral de silencio, duraci\u00f3n m\u00ednima de voz).</li> </ul>"},{"location":"configuracion/#2-servicios-llm-llm","title":"2. Servicios LLM (<code>[llm]</code>)","text":"<p>Voice2Machine implementa un patr\u00f3n de Proveedor para soportar m\u00faltiples backends de IA para el refinado de texto.</p>"},{"location":"configuracion/#configuracion-global","title":"Configuraci\u00f3n Global","text":"Par\u00e1metro Descripci\u00f3n <code>provider</code> Proveedor activo: <code>gemini</code> (Nube) u <code>ollama</code> (Local). <code>model</code> Nombre del modelo espec\u00edfico (ej. <code>gemini-1.5-flash</code> o <code>llama3:8b</code>)."},{"location":"configuracion/#proveedores-especificos","title":"Proveedores Espec\u00edficos","text":""},{"location":"configuracion/#google-gemini-provider-gemini","title":"Google Gemini (<code>provider = \"gemini\"</code>)","text":"<p>Requiere API Key. Ideal para usuarios sin GPU potente (VRAM &lt; 8GB).</p> <ul> <li>Modelo recomendado: <code>gemini-1.5-flash-latest</code> (latencia m\u00ednima).</li> <li>Temperatura: <code>0.3</code> (conservador) para correcci\u00f3n gramatical.</li> </ul>"},{"location":"configuracion/#ollama-provider-ollama","title":"Ollama (<code>provider = \"ollama\"</code>)","text":"<p>Privacidad total. Requiere correr el servidor de Ollama (<code>ollama serve</code>).</p> <ul> <li>Endpoint: <code>http://localhost:11434</code></li> <li>Modelo recomendado: <code>qwen2.5:7b</code> o <code>llama3.1:8b</code>.</li> </ul>"},{"location":"configuracion/#3-grabacion-recording","title":"3. Grabaci\u00f3n (<code>[recording]</code>)","text":"<p>Controla la captura de audio mediante <code>SoundDevice</code> y <code>v2m_engine</code>.</p> <ul> <li><code>sample_rate</code>: <code>16000</code> (Fijo, requerido por Whisper).</li> <li><code>channels</code>: <code>1</code> (Mono).</li> <li><code>device_index</code>: ID del micr\u00f3fono. Si es <code>null</code>, usa el default del sistema (PulseAudio/PipeWire).</li> </ul>"},{"location":"configuracion/#4-sistema-system","title":"4. Sistema (<code>[system]</code>)","text":"<p>Configuraci\u00f3n de bajo nivel para el Daemon y comunicaci\u00f3n.</p> <ul> <li><code>host</code>: Host del servidor (<code>127.0.0.1</code> para acceso solo local).</li> <li><code>port</code>: Puerto HTTP (<code>8765</code> por defecto).</li> <li><code>log_level</code>: <code>INFO</code> por defecto. Cambiar a <code>DEBUG</code> para diagn\u00f3sticos profundos.</li> </ul>"},{"location":"configuracion/#secretos-y-seguridad","title":"Secretos y Seguridad","text":"<p>Las claves de API se gestionan mediante variables de entorno o almacenamiento seguro, nunca en texto plano dentro de <code>config.toml</code> si es posible.</p> <pre><code># Definir en .env o entorno del sistema\nexport GEMINI_API_KEY=\"AIzaSy_TU_CLAVE_AQUI\"\n</code></pre> <p>Importante</p> <p>Reinicia el demonio (<code>python -m v2m.main</code>) despu\u00e9s de editar manualmente el archivo de configuraci\u00f3n para aplicar los cambios.</p>"},{"location":"contribucion/","title":"\u2764\ufe0f Gu\u00eda de Contribuci\u00f3n","text":"<p>\u00a1Gracias por tu inter\u00e9s en contribuir a Voice2Machine! Este proyecto se construye sobre la colaboraci\u00f3n y el c\u00f3digo de calidad.</p> <p>Para mantener nuestros est\u00e1ndares \"State of the Art 2026\", seguimos reglas estrictas pero justas. Por favor, lee esto antes de enviar tu primer Pull Request.</p>"},{"location":"contribucion/#flujo-de-trabajo","title":"\ud83d\ude80 Flujo de Trabajo","text":"<ol> <li>Discusi\u00f3n Primero: Antes de escribir c\u00f3digo, abre un Issue para discutir el cambio. Esto evita trabajo duplicado o rechazos por desalineaci\u00f3n arquitect\u00f3nica.</li> <li>Fork &amp; Branch:<ul> <li>Haz fork del repositorio.</li> <li>Crea una rama descriptiva: <code>feat/nuevo-soporte-gpu</code> o <code>fix/error-transcripcion</code>.</li> </ul> </li> <li>Desarrollo Local: Sigue la gu\u00eda de Instalaci\u00f3n para configurar tu entorno de desarrollo.</li> </ol>"},{"location":"contribucion/#estandares-de-calidad","title":"\ud83d\udccf Est\u00e1ndares de Calidad","text":""},{"location":"contribucion/#codigo","title":"C\u00f3digo","text":"<ul> <li>Backend (Python):<ul> <li>Tipado est\u00e1tico estricto (100% Type Hints).</li> <li>Linter: <code>ruff check src/ --fix</code>.</li> <li>Formateador: <code>ruff format src/</code>.</li> <li>Tests: <code>pytest</code> debe pasar al 100%.</li> </ul> </li> <li>Frontend (Tauri/React):<ul> <li>TypeScript estricto (no <code>any</code>).</li> <li>Linter: <code>npm run lint</code>.</li> <li>Componentes funcionales y Hooks.</li> </ul> </li> </ul>"},{"location":"contribucion/#commits","title":"Commits","text":"<p>Usamos Conventional Commits. Tu mensaje de commit debe seguir este formato:</p> <pre><code>&lt;tipo&gt;(&lt;alcance&gt;): &lt;descripci\u00f3n corta&gt;\n\n[Cuerpo opcional detallado]\n</code></pre> <p>Tipos permitidos: - <code>feat</code>: Nueva funcionalidad. - <code>fix</code>: Correcci\u00f3n de bug. - <code>docs</code>: Solo documentaci\u00f3n. - <code>refactor</code>: Cambio de c\u00f3digo que no arregla bugs ni a\u00f1ade features. - <code>test</code>: A\u00f1adir o corregir tests. - <code>chore</code>: Mantenimiento, dependencias.</p> <p>Ejemplo:</p> <p><code>feat(whisper): upgrade to faster-whisper 1.0.0 for 20% speedup</code></p>"},{"location":"contribucion/#documentacion-docs-as-code","title":"Documentaci\u00f3n (Docs as Code)","text":"<p>Si cambias funcionalidad, debes actualizar la documentaci\u00f3n en <code>docs/docs/es/</code>. - Verifica que <code>mkdocs serve</code> funcione localmente. - Sigue la Gu\u00eda de Estilo.</p>"},{"location":"contribucion/#checklist-de-pull-request","title":"\u2705 Checklist de Pull Request","text":"<p>Antes de enviar tu PR:</p> <ul> <li> He ejecutado los tests locales y pasan.</li> <li> He lintado el c\u00f3digo (<code>ruff</code>, <code>eslint</code>).</li> <li> He actualizado la documentaci\u00f3n relevante.</li> <li> He a\u00f1adido una entrada al <code>CHANGELOG.md</code> (si aplica).</li> <li> Mi c\u00f3digo sigue la Arquitectura Hexagonal (sin imports cruzados prohibidos).</li> </ul> <p>Ayuda</p> <p>Si tienes dudas sobre arquitectura o dise\u00f1o, consulta los documentos en <code>docs/docs/es/adr/</code> o pregunta en el Issue correspondiente.</p>"},{"location":"glosario/","title":"Glosario","text":"<p>Este glosario define t\u00e9rminos t\u00e9cnicos y de dominio utilizados en Voice2Machine.</p>"},{"location":"glosario/#terminos-generales","title":"T\u00e9rminos Generales","text":""},{"location":"glosario/#local-first","title":"Local-First","text":"<p>Filosof\u00eda de dise\u00f1o donde los datos (audio, texto) se procesan y almacenan exclusivamente en el dispositivo del usuario, sin depender de la nube.</p>"},{"location":"glosario/#daemon","title":"Daemon","text":"<p>Proceso en segundo plano (escrito en Python) que gestiona la grabaci\u00f3n, transcripci\u00f3n y comunicaci\u00f3n con el frontend.</p>"},{"location":"glosario/#api-rest","title":"API REST","text":"<p>Mecanismo de comunicaci\u00f3n entre el Daemon (Python) y los clientes (scripts, frontends). Utilizamos FastAPI con endpoints HTTP est\u00e1ndar y WebSocket para eventos en tiempo real.</p>"},{"location":"glosario/#componentes-tecnicos","title":"Componentes T\u00e9cnicos","text":""},{"location":"glosario/#whisper","title":"Whisper","text":"<p>Modelo de reconocimiento de voz (ASR) desarrollado por OpenAI. Voice2Machine utiliza <code>faster-whisper</code>, una implementaci\u00f3n optimizada con CTranslate2.</p>"},{"location":"glosario/#orchestrator","title":"Orchestrator","text":"<p>Componente central de coordinaci\u00f3n que gestiona el ciclo de vida completo del flujo de trabajo: grabaci\u00f3n \u2192 transcripci\u00f3n \u2192 post-procesamiento. Reemplaza el patr\u00f3n anterior CQRS/CommandBus con un enfoque m\u00e1s directo y simple.</p>"},{"location":"glosario/#backendprovider","title":"BackendProvider","text":"<p>Componente del frontend (React Context) que gestiona la conexi\u00f3n con el Daemon y distribuye el estado a la UI.</p>"},{"location":"glosario/#telemetrycontext","title":"TelemetryContext","text":"<p>Sub-contexto de React optimizado para actualizaciones de alta frecuencia (m\u00e9tricas de GPU, niveles de audio) para evitar re-renderizados innecesarios de la UI principal.</p>"},{"location":"glosario/#arquitectura-hexagonal","title":"Arquitectura Hexagonal","text":"<p>Tambi\u00e9n conocida como \"Puertos y Adaptadores\". Patr\u00f3n de dise\u00f1o donde la l\u00f3gica de negocio central (el hex\u00e1gono) est\u00e1 aislada de las preocupaciones externas (bases de datos, APIs, UI) a trav\u00e9s de interfaces bien definidas (puertos) e implementaciones (adaptadores).</p>"},{"location":"guia_rapida/","title":"\ud83d\udd79\ufe0f Gu\u00eda R\u00e1pida","text":"<p>Resumen Ejecutivo</p> <p>Voice2Machine tiene dos superpoderes: Dictado (Voz \u2192 Texto) y Refinado (Texto \u2192 Mejor Texto).</p> <p>Esta gu\u00eda visual te ayuda a entender los flujos de trabajo principales para que seas productivo en minutos.</p>"},{"location":"guia_rapida/#1-flujo-de-dictado-voz-texto","title":"1. Flujo de Dictado (Voz \u2192 Texto)","text":"<p>Ideal para: Escribir correos, c\u00f3digo o mensajes r\u00e1pidos sin tocar el teclado.</p> <ol> <li>Foco: Haz clic en el campo de texto donde quieres escribir.</li> <li>Activa el atajo (Configurable, por defecto ejecutando <code>v2m-toggle.sh</code>). Escuchar\u00e1s un sonido de inicio \ud83d\udd14.</li> <li>Habla claramente. No te preocupes por ser un robot, habla natural.</li> <li>Pulsa el atajo de nuevo para detener. Escuchar\u00e1s un sonido de fin \ud83d\udd15.</li> <li>El texto se pegar\u00e1 autom\u00e1ticamente en tu campo activo (o quedar\u00e1 en el portapapeles si la auto-escritura est\u00e1 desactivada).</li> </ol> <pre><code>flowchart LR\n    A((\ud83c\udfa4 INICIO)) --&gt;|Grabar| B{Whisper Local}\n    B --&gt;|Transcribir| C[\ud83d\udccb Portapapeles / Pegado]\n\n    style A fill:#ff6b6b,stroke:#333,stroke-width:2px,color:white\n    style B fill:#feca57,stroke:#333,stroke-width:2px\n    style C fill:#48dbfb,stroke:#333,stroke-width:2px</code></pre>"},{"location":"guia_rapida/#2-flujo-de-refinado-texto-ia-texto","title":"2. Flujo de Refinado (Texto \u2192 IA \u2192 Texto)","text":"<p>Ideal para: Corregir gram\u00e1tica, traducir o dar formato profesional a un borrador sucio.</p> <ol> <li>Selecciona y Copia (<code>Ctrl + C</code>) el texto que quieres mejorar.</li> <li>Activa el atajo de IA (ejecutando <code>v2m-llm.sh</code>).</li> <li>Espera unos segundos (la IA est\u00e1 pensando \ud83e\udde0).</li> <li>El texto mejorado reemplazar\u00e1 el contenido de tu portapapeles.</li> <li>Pega (<code>Ctrl + V</code>) el resultado.</li> </ol> <pre><code>flowchart LR\n    A[\ud83d\udccb Texto Original] --&gt;|Copiar| B((\ud83e\udde0 ATAJO IA))\n    B --&gt;|Procesar| C{Local LLM / Gemini}\n    C --&gt;|Mejorar| D[\u2728 Texto Pulido]\n\n    style A fill:#c8d6e5,stroke:#333,stroke-width:2px\n    style B fill:#5f27cd,stroke:#333,stroke-width:2px,color:white\n    style C fill:#feca57,stroke:#333,stroke-width:2px\n    style D fill:#1dd1a1,stroke:#333,stroke-width:2px</code></pre>"},{"location":"guia_rapida/#consejos-pro","title":"\ud83d\udca1 Consejos Pro","text":"<p>Mejora tu Precisi\u00f3n</p> <ul> <li>Habla fluido: Whisper entiende mejor el contexto de frases completas que palabras sueltas.</li> <li>Hardware: Un micr\u00f3fono con cancelaci\u00f3n de ruido mejora dr\u00e1sticamente los resultados.</li> <li>Configuraci\u00f3n: Puedes ajustar la \"temperatura\" del LLM en la configuraci\u00f3n para hacerlo m\u00e1s creativo o m\u00e1s literal.</li> </ul> <p>Privacidad Garantizada</p> <p>El Dictado es 100% local (ejecutado en tu GPU). El Refinado puede ser local (Ollama) o nube (Gemini), t\u00fa tienes el control total en la configuraci\u00f3n.</p>"},{"location":"instalacion/","title":"\ud83d\udee0\ufe0f Instalaci\u00f3n y Configuraci\u00f3n","text":"<p>Prerrequisito</p> <p>Este proyecto est\u00e1 optimizado para Linux (Debian/Ubuntu). Estado del Arte 2026: Utilizamos aceleraci\u00f3n por hardware (CUDA) y un enfoque modular para garantizar privacidad y rendimiento.</p> <p>Esta gu\u00eda te llevar\u00e1 desde cero hasta un sistema de dictado completamente funcional en tu m\u00e1quina local.</p>"},{"location":"instalacion/#metodo-1-instalacion-automatica-recomendado","title":"\ud83d\ude80 M\u00e9todo 1: Instalaci\u00f3n Autom\u00e1tica (Recomendado)","text":"<p>Hemos creado un script que maneja todo el \"trabajo sucio\" por ti: verifica tu sistema, instala dependencias (apt), crea el entorno virtual (venv) y configura las credenciales.</p> <pre><code># Ejecutar desde la ra\u00edz del proyecto\n./apps/daemon/backend/scripts/setup/install.sh\n</code></pre> <p>Lo que hace este script:</p> <ol> <li>\ud83d\udce6 Instala librer\u00edas del sistema (<code>ffmpeg</code>, <code>xclip</code>, <code>pulseaudio-utils</code>).</li> <li>\ud83d\udc0d Crea un entorno Python aislado (<code>venv</code>).</li> <li>\u2699\ufe0f Instala las dependencias del proyecto (<code>faster-whisper</code>, <code>torch</code>).</li> <li>\ud83d\udd11 Te ayuda a configurar tu API Key de Gemini (opcional, para IA generativa).</li> <li>\ud83d\udda5\ufe0f Verifica si tienes una GPU NVIDIA compatible.</li> </ol>"},{"location":"instalacion/#metodo-2-instalacion-manual","title":"\ud83d\udee0\ufe0f M\u00e9todo 2: Instalaci\u00f3n Manual","text":"<p>Si prefieres tener el control total o el script autom\u00e1tico falla, sigue estos pasos.</p>"},{"location":"instalacion/#1-dependencias-del-sistema-system-level","title":"1. Dependencias del Sistema (System Level)","text":"<p>Necesitamos herramientas para manipular audio y el portapapeles a nivel del SO.</p> <pre><code>sudo apt update\nsudo apt install ffmpeg xclip pulseaudio-utils python3-venv build-essential python3-dev\n</code></pre>"},{"location":"instalacion/#2-entorno-python","title":"2. Entorno Python","text":"<p>Aislamos las librer\u00edas para evitar conflictos.</p> <pre><code># Navegar al directorio del backend\ncd apps/daemon/backend\n\n# Crear entorno virtual\npython3 -m venv venv\n\n# Activar entorno (\u00a1Haz esto cada vez que trabajes en el proyecto!)\nsource venv/bin/activate\n\n# Instalar dependencias\npip install -e .\n</code></pre>"},{"location":"instalacion/#3-configuracion-de-ia-opcional","title":"3. Configuraci\u00f3n de IA (Opcional)","text":"<p>Para usar las funciones de \"Refinado de Texto\" (reescritura con LLM), necesitas una API Key de Google Gemini.</p> <ol> <li>Consigue tu clave en Google AI Studio.</li> <li>Crea un archivo <code>.env</code> en la ra\u00edz:</li> </ol> <pre><code>echo 'GEMINI_API_KEY=\"tu_clave_api_aqui\"' &gt; .env\n</code></pre>"},{"location":"instalacion/#verificacion","title":"\u2705 Verificaci\u00f3n","text":"<p>Aseg\u00farate de que todo funciona antes de continuar.</p>"},{"location":"instalacion/#1-verificar-aceleracion-gpu","title":"1. Verificar Aceleraci\u00f3n GPU","text":"<p>Esto confirma que Whisper puede usar tu tarjeta gr\u00e1fica (esencial para velocidad).</p> <pre><code>python apps/daemon/backend/scripts/diagnostics/check_cuda.py\n</code></pre>"},{"location":"instalacion/#2-diagnostico-del-sistema","title":"2. Diagn\u00f3stico del Sistema","text":"<p>Verifica que el demonio y los servicios de audio est\u00e9n listos.</p> <pre><code>python apps/daemon/backend/scripts/diagnostics/health_check.py\n</code></pre>"},{"location":"instalacion/#siguientes-pasos","title":"\u23ed\ufe0f Siguientes Pasos","text":"<p>Una vez instalado, es hora de configurar c\u00f3mo interact\u00faas con la herramienta.</p> <ul> <li>Configuraci\u00f3n Detallada - Ajusta modelos y sensibilidad.</li> <li>Atajos de Teclado - Configura tus teclas m\u00e1gicas.</li> </ul>"},{"location":"referencia_api/","title":"Referencia de API REST","text":"<p>Esta secci\u00f3n documenta la API REST del Daemon Voice2Machine (v0.2.0+).</p> <p>Arquitectura Actualizada</p> <p>Voice2Machine utiliza FastAPI para la comunicaci\u00f3n cliente-servidor, reemplazando el sistema anterior basado en Unix Sockets IPC. Esto permite probar endpoints directamente con <code>curl</code> o cualquier cliente HTTP.</p>"},{"location":"referencia_api/#informacion-general","title":"Informaci\u00f3n General","text":"Propiedad Valor Base URL <code>http://localhost:8765</code> Protocolo HTTP/1.1 + WebSocket Formato JSON (UTF-8) Documentaci\u00f3n Interactiva <code>http://localhost:8765/docs</code> (Swagger UI)"},{"location":"referencia_api/#endpoints-rest","title":"Endpoints REST","text":""},{"location":"referencia_api/#post-toggle","title":"POST <code>/toggle</code>","text":"<p>Toggle de grabaci\u00f3n (iniciar/detener). Este es el endpoint principal que usa el atajo de teclado.</p> Request <p><code>bash     curl -X POST http://localhost:8765/toggle | jq</code></p> Response (Iniciando) <p><code>json     {       \"status\": \"recording\",       \"message\": \"Grabaci\u00f3n iniciada\",       \"text\": null     }</code></p> Response (Deteniendo) <p><code>json     {       \"status\": \"idle\",       \"message\": \"Transcripci\u00f3n completada\",       \"text\": \"El texto transcrito aparece aqu\u00ed...\"     }</code></p>"},{"location":"referencia_api/#post-start","title":"POST <code>/start</code>","text":"<p>Inicia grabaci\u00f3n expl\u00edcitamente. \u00datil cuando necesitas control separado de inicio/fin.</p> Request <p><code>bash     curl -X POST http://localhost:8765/start | jq</code></p> Response <p><code>json     {       \"status\": \"recording\",       \"message\": \"Grabaci\u00f3n iniciada\",       \"text\": null     }</code></p>"},{"location":"referencia_api/#post-stop","title":"POST <code>/stop</code>","text":"<p>Detiene grabaci\u00f3n y transcribe el audio capturado.</p> Request <p><code>bash     curl -X POST http://localhost:8765/stop | jq</code></p> Response <p><code>json     {       \"status\": \"idle\",       \"message\": \"Transcripci\u00f3n completada\",       \"text\": \"El texto transcrito aparece aqu\u00ed...\"     }</code></p>"},{"location":"referencia_api/#post-llmprocess","title":"POST <code>/llm/process</code>","text":"<p>Procesa texto con LLM (limpieza, puntuaci\u00f3n, formato). El backend se selecciona seg\u00fan <code>config.toml</code>.</p> Request <p><code>bash     curl -X POST http://localhost:8765/llm/process \\       -H \"Content-Type: application/json\" \\       -d '{\"text\": \"hola como estas espero que bien\"}' | jq</code></p> Response <p><code>json     {       \"text\": \"Hola, \u00bfc\u00f3mo est\u00e1s? Espero que bien.\",       \"backend\": \"gemini\"     }</code></p>"},{"location":"referencia_api/#post-llmtranslate","title":"POST <code>/llm/translate</code>","text":"<p>Traduce texto a otro idioma usando LLM.</p> Request <p><code>bash     curl -X POST http://localhost:8765/llm/translate \\       -H \"Content-Type: application/json\" \\       -d '{\"text\": \"Buenos d\u00edas\", \"target_lang\": \"en\"}' | jq</code></p> Response <p><code>json     {       \"text\": \"Good morning\",       \"backend\": \"gemini\"     }</code></p>"},{"location":"referencia_api/#get-status","title":"GET <code>/status</code>","text":"<p>Retorna el estado actual del daemon.</p> Request <p><code>bash     curl http://localhost:8765/status | jq</code></p> Response <p><code>json     {       \"state\": \"idle\",       \"recording\": false,       \"model_loaded\": true     }</code></p> <p>Estados posibles:</p> Estado Descripci\u00f3n <code>idle</code> Esperando comandos <code>recording</code> Grabando audio <code>processing</code> Transcribiendo o procesando con LLM"},{"location":"referencia_api/#get-health","title":"GET <code>/health</code>","text":"<p>Health check para systemd/scripts de monitoreo.</p> Request <p><code>bash     curl http://localhost:8765/health | jq</code></p> Response <p><code>json     {       \"status\": \"ok\",       \"version\": \"0.2.0\"     }</code></p>"},{"location":"referencia_api/#websocket","title":"WebSocket","text":""},{"location":"referencia_api/#ws-wsevents","title":"WS <code>/ws/events</code>","text":"<p>Stream de eventos en tiempo real. \u00datil para mostrar transcripci\u00f3n provisional mientras el usuario habla.</p> Conexi\u00f3n (JavaScript) <pre><code>const ws = new WebSocket('ws://localhost:8765/ws/events');\n\n    ws.onmessage = (event) =&gt; {\n      const { event: eventType, data } = JSON.parse(event.data);\n      console.log(`Evento: ${eventType}`, data);\n    };\n    ```\n\n=== \"Conexi\u00f3n (Python)\"\n```python\nimport asyncio\nimport websockets\n\n    async def listen():\n        async with websockets.connect('ws://localhost:8765/ws/events') as ws:\n            async for message in ws:\n                print(message)\n\n    asyncio.run(listen())\n    ```\n\n**Eventos emitidos:**\n\n| Evento                 | Campos                           | Descripci\u00f3n                                          |\n| ---------------------- | -------------------------------- | ---------------------------------------------------- |\n| `transcription_update` | `text: str`, `final: bool`       | Actualizaci\u00f3n de transcripci\u00f3n (provisional o final) |\n| `heartbeat`            | `timestamp: float`, `state: str` | Latido para mantener conexi\u00f3n viva                   |\n\n---\n\n## Modelos de Datos\n\n### ToggleResponse\n\n```python\nclass ToggleResponse(BaseModel):\n    status: str      # 'recording' | 'idle'\n    message: str     # Mensaje descriptivo\n    text: str | None # Texto transcrito (solo en stop)\n</code></pre>"},{"location":"referencia_api/#statusresponse","title":"StatusResponse","text":"<pre><code>class StatusResponse(BaseModel):\n    state: str        # 'idle' | 'recording' | 'processing'\n    recording: bool   # True si est\u00e1 grabando\n    model_loaded: bool # True si Whisper est\u00e1 en VRAM\n</code></pre>"},{"location":"referencia_api/#llmresponse","title":"LLMResponse","text":"<pre><code>class LLMResponse(BaseModel):\n    text: str    # Texto procesado/traducido\n    backend: str # 'gemini' | 'ollama' | 'local'\n</code></pre>"},{"location":"referencia_api/#codigos-de-error","title":"C\u00f3digos de Error","text":"C\u00f3digo HTTP Significado <code>200</code> Operaci\u00f3n exitosa <code>422</code> Error de validaci\u00f3n (payload inv\u00e1lido) <code>500</code> Error interno del servidor <p>Depuraci\u00f3n</p> <p>Usa la documentaci\u00f3n interactiva en <code>http://localhost:8765/docs</code> para probar endpoints visualmente.</p>"},{"location":"style_guide/","title":"Gu\u00eda de Estilo y Gobernanza","text":"<p>Esta gu\u00eda define los est\u00e1ndares para la documentaci\u00f3n de Voice2Machine, alineados con el \"Estado del Arte 2026\".</p>"},{"location":"style_guide/#principios-fundamentales","title":"Principios Fundamentales","text":"<ol> <li>Docs as Code: La documentaci\u00f3n vive en el repositorio, se versiona con Git y se valida en CI/CD.</li> <li>Accesibilidad Universal: Cumplimiento estricto de WCAG 2.1 Level AA.</li> <li>Localizaci\u00f3n: La fuente de la verdad (<code>docs/</code>) est\u00e1 en Espa\u00f1ol Latinoamericano Nativo. Los archivos ra\u00edz (<code>README.md</code>, <code>AGENTS.md</code>) est\u00e1n en Ingl\u00e9s (USA) y Espa\u00f1ol.</li> </ol>"},{"location":"style_guide/#accesibilidad-wcag-21-aa","title":"Accesibilidad (WCAG 2.1 AA)","text":"<ul> <li>Texto Alternativo: Todas las im\u00e1genes deben tener <code>alt text</code> descriptivo.</li> <li>Jerarqu\u00eda de Encabezados: No saltar niveles (H1 -&gt; H2 -&gt; H3).</li> <li>Contraste: Diagramas y capturas deben tener alto contraste.</li> <li>Enlaces: Usar texto descriptivo (\"ver gu\u00eda de instalaci\u00f3n\" en lugar de \"clic aqu\u00ed\").</li> </ul>"},{"location":"style_guide/#tono-y-voz","title":"Tono y Voz","text":"<ul> <li>Audiencia: Desarrolladores y usuarios t\u00e9cnicos.</li> <li>Tono: Profesional, conciso, directo (\"Haga esto\" en lugar de \"Podr\u00eda hacer esto\").</li> <li>Persona: Segunda persona (\"Configura tu entorno\") o impersonal (\"Se configura el entorno\").</li> <li>Espa\u00f1ol: Neutro/Latinoamericano. Evitar modismos locales excesivos.</li> </ul>"},{"location":"style_guide/#estructura-de-markdown","title":"Estructura de Markdown","text":""},{"location":"style_guide/#admonitions-notas","title":"Admonitions (Notas)","text":"<p>Usa bloques de admonition para resaltar informaci\u00f3n:</p> <pre><code>!!! note \"Nota\"\n    Informaci\u00f3n neutral.\n\n!!! tip \"Consejo\"\n    Ayuda para optimizar.\n\n!!! warning \"Advertencia\"\n    Cuidado con esto.\n\n!!! danger \"Peligro\"\n    Riesgo de p\u00e9rdida de datos.\n</code></pre>"},{"location":"style_guide/#codigo","title":"C\u00f3digo","text":"<p>Bloques de c\u00f3digo con lenguaje especificado:</p> <pre><code>def mi_funcion():\n    pass\n</code></pre>"},{"location":"style_guide/#proceso-de-gobernanza","title":"Proceso de Gobernanza","text":"<ol> <li>Cambios: Todo cambio de c\u00f3digo que afecte funcionalidad requiere actualizaci\u00f3n de docs en el mismo PR.</li> <li>Revisi\u00f3n: Los PRs de documentaci\u00f3n requieren revisi\u00f3n humana.</li> <li>Mantenimiento: Revisi\u00f3n trimestral de obsolescencia.</li> </ol>"},{"location":"troubleshooting/","title":"\ud83d\udd27 Soluci\u00f3n de Problemas (Troubleshooting)","text":"<p>Regla de Oro</p> <p>Ante cualquier problema, el primer paso siempre es consultar los logs del sistema. <code>bash     # Ver logs en tiempo real     tail -f ~/.local/state/v2m/v2m.log</code></p>"},{"location":"troubleshooting/#audio-y-grabacion","title":"\ud83d\uded1 Audio y Grabaci\u00f3n","text":""},{"location":"troubleshooting/#no-se-escucha-nada-transcripcion-vacia","title":"No se escucha nada / Transcripci\u00f3n vac\u00eda","text":"<ul> <li>S\u00edntoma: La grabaci\u00f3n inicia y termina, pero no se genera texto.</li> <li>Diagn\u00f3stico:   Ejecuta el script de diagn\u00f3stico de audio:   <pre><code>python scripts/diagnose_audio.py\n</code></pre></li> <li>Soluciones:</li> <li>Driver de Audio: Voice2Machine usa <code>SoundDevice</code>. Aseg\u00farate de que tu sistema (PulseAudio/PipeWire) tenga un micr\u00f3fono predeterminado activo.</li> <li>Permisos: En Linux, tu usuario debe pertenecer al grupo <code>audio</code> (<code>sudo usermod -aG audio $USER</code>).</li> </ul>"},{"location":"troubleshooting/#frases-cortadas-o-incompletas","title":"Frases cortadas o incompletas","text":"<ul> <li>Causa: El detector de silencio (VAD) es demasiado agresivo.</li> <li>Soluci\u00f3n:   Ajusta la configuraci\u00f3n en <code>config.toml</code> o desde la GUI:</li> <li>Reduce el <code>threshold</code> (ej. de <code>0.35</code> a <code>0.30</code>).</li> <li>Aumenta el <code>min_silence_duration_ms</code> (ej. a <code>800ms</code>).</li> </ul>"},{"location":"troubleshooting/#rendimiento-y-gpu","title":"\ud83d\udc22 Rendimiento y GPU","text":""},{"location":"troubleshooting/#transcripcion-lenta-2-segundos","title":"Transcripci\u00f3n lenta (&gt; 2 segundos)","text":"<ul> <li>Causa Probable: Whisper est\u00e1 ejecut\u00e1ndose en CPU en lugar de GPU.</li> <li>Verificaci\u00f3n:   <pre><code>python scripts/test_whisper_gpu.py\n</code></pre></li> <li>Soluci\u00f3n:</li> <li>Instala drivers NVIDIA actualizados (compatible con CUDA 12).</li> <li>Verifica que <code>config.toml</code> tenga <code>device = \"cuda\"</code>.</li> <li>Si no tienes GPU dedicada, cambia el modelo a <code>distil-medium.en</code> o <code>base</code>.</li> </ul>"},{"location":"troubleshooting/#error-cuda-out-of-memory","title":"Error <code>CUDA out of memory</code>","text":"<ul> <li>Causa: Tu GPU no tiene suficiente VRAM para el modelo seleccionado.</li> <li>Soluci\u00f3n:</li> <li>Cambia <code>compute_type</code> a <code>int8_float16</code> (reduce uso de VRAM a la mitad).</li> <li>Usa un modelo m\u00e1s ligero (<code>distil-large-v3</code> consume menos que <code>large-v3</code> original).</li> </ul>"},{"location":"troubleshooting/#conectividad-y-demonio","title":"\ud83d\udd0c Conectividad y Demonio","text":""},{"location":"troubleshooting/#connection-refused-en-gui-o-scripts","title":"\"Connection refused\" en GUI o Scripts","text":"<ul> <li>Causa: El proceso backend (Python) no est\u00e1 corriendo o el puerto est\u00e1 ocupado.</li> <li>Soluci\u00f3n:</li> <li>Verifica el estado:       <pre><code>pgrep -a python | grep v2m\n</code></pre></li> <li>Si no corre, in\u00edcialo manualmente para ver errores de arranque:       <pre><code>python -m v2m.main\n</code></pre></li> <li>Si dice \"Address already in use\", mata el proceso existente:       <pre><code>pkill -f \"v2m.main\"\n</code></pre></li> </ul>"},{"location":"troubleshooting/#atajos-de-teclado-no-responden","title":"Atajos de teclado no responden","text":"<ul> <li>Causa: Problema de permisos o ruta incorrecta en la configuraci\u00f3n del gestor de ventanas.</li> <li>Soluci\u00f3n:</li> <li>Ejecuta el script manualmente en terminal: <code>scripts/v2m-toggle.sh</code>.</li> <li>Si funciona, el error est\u00e1 en tu configuraci\u00f3n de atajos (ej. ruta relativa <code>~/</code> en lugar de <code>/home/...</code>).</li> <li>Si no funciona, verifica permisos: <code>chmod +x scripts/*.sh</code>.</li> </ul>"},{"location":"troubleshooting/#errores-de-ia-llm","title":"\ud83e\udde0 Errores de IA (LLM)","text":""},{"location":"troubleshooting/#error-401403-con-gemini","title":"Error 401/403 con Gemini","text":"<ul> <li>Causa: API Key inv\u00e1lida o expirada.</li> <li>Soluci\u00f3n: Regenera tu clave en Google AI Studio y actualiza el archivo <code>.env</code> o la variable de entorno <code>GEMINI_API_KEY</code>.</li> </ul>"},{"location":"troubleshooting/#connection-refused-con-ollama","title":"\"Connection refused\" con Ollama","text":"<ul> <li>Causa: El servidor de Ollama no est\u00e1 corriendo.</li> <li>Soluci\u00f3n: Ejecuta <code>ollama serve</code> en otra terminal.</li> </ul>"},{"location":"adr/","title":"Architecture Decision Records (ADRs)","text":"<p>Un Registro de Decisiones de Arquitectura (ADR) es un documento que captura una decisi\u00f3n de arquitectura importante, junto con su contexto y consecuencias.</p>"},{"location":"adr/#indice-de-decisiones","title":"\u00cdndice de Decisiones","text":"ADR T\u00edtulo Estado Fecha ADR-001 Migraci\u00f3n de IPC Unix Sockets a FastAPI REST Aceptada 2025-01 ADR-002 Reemplazo de CQRS/CommandBus por Orchestrator Aceptada 2025-01 ADR-003 Selecci\u00f3n de faster-whisper sobre whisper.cpp Aceptada 2024-06 ADR-004 Arquitectura Hexagonal (Puertos y Adaptadores) Aceptada 2024-03 ADR-005 Motor de Audio en Rust (v2m_engine) Aceptada 2025-01 ADR-006 Local-first: Procesamiento sin Cloud Aceptada 2024-03"},{"location":"adr/#cuando-escribir-un-adr","title":"\u00bfCu\u00e1ndo escribir un ADR?","text":"<p>Escribe un ADR cuando tomes una decisi\u00f3n significativa que afecte a la estructura, dependencias, interfaces o tecnolog\u00eda del proyecto.</p> <p>Ver Plantilla de ADR para el formato.</p>"},{"location":"adr/001-fastapi-migration/","title":"ADR-001: Migraci\u00f3n de IPC Unix Sockets a FastAPI REST","text":""},{"location":"adr/001-fastapi-migration/#estado","title":"Estado","text":"<p>Aceptada</p>"},{"location":"adr/001-fastapi-migration/#fecha","title":"Fecha","text":"<p>2025-01-15</p>"},{"location":"adr/001-fastapi-migration/#contexto","title":"Contexto","text":"<p>El sistema original de Voice2Machine utilizaba Unix Domain Sockets con un protocolo binario personalizado para la comunicaci\u00f3n entre el Daemon (Python) y los clientes (scripts, frontend Tauri).</p>"},{"location":"adr/001-fastapi-migration/#limitaciones-del-sistema-anterior","title":"Limitaciones del sistema anterior:","text":"<ol> <li>Complejidad de debugging: Los mensajes binarios requer\u00edan herramientas especializadas para inspecci\u00f3n</li> <li>Curva de aprendizaje: Los nuevos desarrolladores necesitaban entender el protocolo propietario</li> <li>Incompatibilidad con herramientas est\u00e1ndar: No se pod\u00eda usar <code>curl</code>, Postman, o navegadores para testing</li> <li>Mantenimiento del protocolo: Cada cambio requer\u00eda actualizar cliente y servidor sincronizadamente</li> <li>Documentaci\u00f3n interactiva: No hab\u00eda forma de generar docs autom\u00e1ticamente</li> </ol>"},{"location":"adr/001-fastapi-migration/#requisitos","title":"Requisitos:","text":"<ul> <li>Mantener latencia &lt; 50ms para operaciones cr\u00edticas (toggle)</li> <li>Permitir streaming de eventos en tiempo real</li> <li>Simplificar onboarding de nuevos desarrolladores</li> <li>Facilitar testing y debugging</li> </ul>"},{"location":"adr/001-fastapi-migration/#decision","title":"Decisi\u00f3n","text":"<p>Migrar a FastAPI como framework de API REST, reemplazando completamente el sistema IPC propietario.</p>"},{"location":"adr/001-fastapi-migration/#implementacion","title":"Implementaci\u00f3n:","text":"<ul> <li>FastAPI + Uvicorn: Servidor HTTP async con rendimiento comparable a Go/Rust</li> <li>WebSocket: Para streaming de eventos (transcripci\u00f3n provisional)</li> <li>Pydantic V2: Validaci\u00f3n autom\u00e1tica y generaci\u00f3n de OpenAPI schema</li> <li>Swagger UI: Documentaci\u00f3n interactiva en <code>/docs</code></li> </ul>"},{"location":"adr/001-fastapi-migration/#consecuencias","title":"Consecuencias","text":""},{"location":"adr/001-fastapi-migration/#positivas","title":"Positivas","text":"<ul> <li>\u2705 Debugging trivial: <code>curl -X POST localhost:8765/toggle | jq</code></li> <li>\u2705 Documentaci\u00f3n autom\u00e1tica: Swagger UI incluido sin esfuerzo adicional</li> <li>\u2705 Ecosistema est\u00e1ndar: Compatible con cualquier cliente HTTP</li> <li>\u2705 Testing simplificado: FastAPI TestClient para tests de integraci\u00f3n</li> <li>\u2705 Onboarding r\u00e1pido: Un Junior puede entender la API en minutos</li> </ul>"},{"location":"adr/001-fastapi-migration/#negativas","title":"Negativas","text":"<ul> <li>\u26a0\ufe0f Overhead HTTP: ~2-5ms adicionales vs sockets raw (aceptable)</li> <li>\u26a0\ufe0f Puerto expuesto: Requiere configuraci\u00f3n de firewall (mitigado con <code>127.0.0.1</code> only)</li> <li>\u26a0\ufe0f Dependencia adicional: FastAPI + Uvicorn (~2MB)</li> </ul>"},{"location":"adr/001-fastapi-migration/#alternativas-consideradas","title":"Alternativas Consideradas","text":""},{"location":"adr/001-fastapi-migration/#grpc","title":"gRPC","text":"<ul> <li>Rechazado: Requiere tooling adicional (protoc), curva de aprendizaje similar al IPC original, no hay Swagger UI nativo.</li> </ul>"},{"location":"adr/001-fastapi-migration/#graphql","title":"GraphQL","text":"<ul> <li>Rechazado: Overhead innecesario para operaciones simples RPC-style, mayor complejidad.</li> </ul>"},{"location":"adr/001-fastapi-migration/#mantener-unix-sockets","title":"Mantener Unix Sockets","text":"<ul> <li>Rechazado: No resolv\u00eda los problemas de debugging y onboarding.</li> </ul>"},{"location":"adr/001-fastapi-migration/#referencias","title":"Referencias","text":"<ul> <li>FastAPI Documentation</li> <li>Uvicorn Performance</li> </ul>"},{"location":"adr/002-orchestrator-pattern/","title":"ADR-002: Reemplazo de CQRS/CommandBus por Orchestrator","text":""},{"location":"adr/002-orchestrator-pattern/#estado","title":"Estado","text":"<p>Aceptada</p>"},{"location":"adr/002-orchestrator-pattern/#fecha","title":"Fecha","text":"<p>2025-01-15</p>"},{"location":"adr/002-orchestrator-pattern/#contexto","title":"Contexto","text":"<p>El backend original implementaba el patr\u00f3n CQRS (Command Query Responsibility Segregation) con un CommandBus para procesar acciones del usuario.</p>"},{"location":"adr/002-orchestrator-pattern/#problemas-identificados","title":"Problemas identificados:","text":"<ol> <li>Sobre-ingenier\u00eda: Para un sistema con ~10 comandos, el overhead de CQRS era desproporcionado</li> <li>Indirecci\u00f3n excesiva: Command \u2192 CommandBus \u2192 Handler \u2192 Result \u2192 Response</li> <li>Boilerplate: Cada nueva funcionalidad requer\u00eda crear Command DTO + Handler + registrar en bus</li> <li>Debugging complejo: Stack traces profundos oscurec\u00edan el flujo real</li> <li>Testing verbose: Mocks de CommandBus en cada test</li> </ol>"},{"location":"adr/002-orchestrator-pattern/#requisitos","title":"Requisitos:","text":"<ul> <li>Mantener separaci\u00f3n de concerns (no acoplar API a infraestructura)</li> <li>Simplificar el flujo de control</li> <li>Reducir boilerplate para nuevas features</li> <li>Facilitar testing y debugging</li> </ul>"},{"location":"adr/002-orchestrator-pattern/#decision","title":"Decisi\u00f3n","text":"<p>Reemplazar CQRS/CommandBus por un Orchestrator central que coordina el flujo de trabajo directamente.</p>"},{"location":"adr/002-orchestrator-pattern/#implementacion","title":"Implementaci\u00f3n:","text":"<pre><code>class Orchestrator:\n    async def toggle(self) -&gt; ToggleResponse: ...\n    async def start(self) -&gt; ToggleResponse: ...\n    async def stop(self) -&gt; ToggleResponse: ...\n</code></pre> <p>El Orchestrator:</p> <ul> <li>Expone m\u00e9todos directos para cada operaci\u00f3n</li> <li>Coordina adaptadores (AudioRecorder, WhisperAdapter, LLMProvider)</li> <li>Mantiene el estado del sistema</li> <li>Emite eventos a WebSocket clients</li> </ul>"},{"location":"adr/002-orchestrator-pattern/#consecuencias","title":"Consecuencias","text":""},{"location":"adr/002-orchestrator-pattern/#positivas","title":"Positivas","text":"<ul> <li>\u2705 Flujo expl\u00edcito: API \u2192 Orchestrator \u2192 Adapters (3 capas vs 6+)</li> <li>\u2705 Menos c\u00f3digo: Eliminamos ~500 LOC de CommandBus infrastructure</li> <li>\u2705 Debugging simple: Stack traces claros y cortos</li> <li>\u2705 Testing directo: Mockeamos adaptadores, no buses abstractos</li> <li>\u2705 Onboarding: Nuevos devs entienden el sistema en minutos</li> </ul>"},{"location":"adr/002-orchestrator-pattern/#negativas","title":"Negativas","text":"<ul> <li>\u26a0\ufe0f Menos extensible: Agregar un \"middleware\" global es menos trivial</li> <li>\u26a0\ufe0f Orchestrator \"god object\": Riesgo de que crezca demasiado (mitigado con composici\u00f3n)</li> </ul>"},{"location":"adr/002-orchestrator-pattern/#alternativas-consideradas","title":"Alternativas Consideradas","text":""},{"location":"adr/002-orchestrator-pattern/#mantener-cqrs-simplificado","title":"Mantener CQRS simplificado","text":"<ul> <li>Rechazado: Incluso simplificado, el overhead conceptual no se justificaba.</li> </ul>"},{"location":"adr/002-orchestrator-pattern/#event-sourcing","title":"Event Sourcing","text":"<ul> <li>Rechazado: Sobre-ingenier\u00eda a\u00fan mayor para el caso de uso actual.</li> </ul>"},{"location":"adr/002-orchestrator-pattern/#simple-functions-sin-clase","title":"Simple Functions (sin clase)","text":"<ul> <li>Rechazado: Perd\u00edamos gesti\u00f3n de estado y lifecycle.</li> </ul>"},{"location":"adr/002-orchestrator-pattern/#referencias","title":"Referencias","text":"<ul> <li>Martin Fowler on CQRS</li> <li>When not to use CQRS</li> </ul>"},{"location":"adr/003-faster-whisper/","title":"ADR-003: Selecci\u00f3n de faster-whisper sobre whisper.cpp","text":""},{"location":"adr/003-faster-whisper/#estado","title":"Estado","text":"<p>Aceptada</p>"},{"location":"adr/003-faster-whisper/#fecha","title":"Fecha","text":"<p>2024-06-01</p>"},{"location":"adr/003-faster-whisper/#contexto","title":"Contexto","text":"<p>Para la transcripci\u00f3n de voz local, necesit\u00e1bamos elegir una implementaci\u00f3n del modelo Whisper de OpenAI que maximizara rendimiento en GPUs NVIDIA consumer (RTX 3060-4090).</p>"},{"location":"adr/003-faster-whisper/#opciones-evaluadas","title":"Opciones evaluadas:","text":"<ol> <li>OpenAI Whisper (original): Implementaci\u00f3n de referencia en PyTorch</li> <li>whisper.cpp: Implementaci\u00f3n en C++ puro con soporte CUDA</li> <li>faster-whisper: Implementaci\u00f3n sobre CTranslate2 (C++/CUDA optimizado)</li> </ol>"},{"location":"adr/003-faster-whisper/#requisitos","title":"Requisitos:","text":"<ul> <li>Latencia &lt; 500ms para 5 segundos de audio (8x real-time m\u00ednimo)</li> <li>Soporte para modelos <code>large-v3</code> y variantes <code>distil</code></li> <li>Cuantizaci\u00f3n INT8/FP16 para optimizar VRAM</li> <li>API Python para integraci\u00f3n con el backend</li> <li>Streaming/chunking de audio</li> </ul>"},{"location":"adr/003-faster-whisper/#decision","title":"Decisi\u00f3n","text":"<p>Adoptar faster-whisper como motor de transcripci\u00f3n principal.</p>"},{"location":"adr/003-faster-whisper/#justificacion","title":"Justificaci\u00f3n:","text":"Criterio Whisper (PyTorch) whisper.cpp faster-whisper Velocidad 1x (baseline) 4x 4-8x VRAM (large-v3) 10GB 6GB 4-5GB Python API \u2705 Nativa \u274c Bindings \u2705 Excelente Cuantizaci\u00f3n Limited \u2705 \u2705 INT8/FP16 Mantenimiento OpenAI Comunidad Activo (Systran)"},{"location":"adr/003-faster-whisper/#consecuencias","title":"Consecuencias","text":""},{"location":"adr/003-faster-whisper/#positivas","title":"Positivas","text":"<ul> <li>\u2705 4-8x m\u00e1s r\u00e1pido que Whisper original con misma precisi\u00f3n</li> <li>\u2705 ~50% menos VRAM: Permite usar large-v3 en GPUs de 6GB</li> <li>\u2705 API Pythonica: Integraci\u00f3n natural con FastAPI async</li> <li>\u2705 Soporte distil models: <code>distil-large-v3</code> para latencia m\u00ednima</li> </ul>"},{"location":"adr/003-faster-whisper/#negativas","title":"Negativas","text":"<ul> <li>\u26a0\ufe0f Dependencia adicional: CTranslate2 binario (~100MB)</li> <li>\u26a0\ufe0f Menos portable: Requiere CUDA toolkit compatible</li> <li>\u26a0\ufe0f Lag en nuevos modelos: Nuevos releases de OpenAI tardan ~2 semanas en estar disponibles</li> </ul>"},{"location":"adr/003-faster-whisper/#alternativas-consideradas","title":"Alternativas Consideradas","text":""},{"location":"adr/003-faster-whisper/#whispercpp","title":"whisper.cpp","text":"<ul> <li>Rechazado: Bindings Python inmaduros, debugging m\u00e1s complejo.</li> </ul>"},{"location":"adr/003-faster-whisper/#openai-whisper","title":"OpenAI Whisper","text":"<ul> <li>Rechazado: Demasiado lento para experiencia real-time sin hardware enterprise.</li> </ul>"},{"location":"adr/003-faster-whisper/#whisper-jax","title":"Whisper JAX","text":"<ul> <li>Rechazado: Requiere TPU o configuraci\u00f3n compleja de JAX en CUDA.</li> </ul>"},{"location":"adr/003-faster-whisper/#referencias","title":"Referencias","text":"<ul> <li>faster-whisper GitHub</li> <li>CTranslate2</li> <li>Whisper Benchmarks</li> </ul>"},{"location":"adr/004-hexagonal-architecture/","title":"ADR-004: Arquitectura Hexagonal (Puertos y Adaptadores)","text":""},{"location":"adr/004-hexagonal-architecture/#estado","title":"Estado","text":"<p>Aceptada</p>"},{"location":"adr/004-hexagonal-architecture/#fecha","title":"Fecha","text":"<p>2024-03-01</p>"},{"location":"adr/004-hexagonal-architecture/#contexto","title":"Contexto","text":"<p>Voice2Machine comenz\u00f3 como un script monol\u00edtico de ~200 l\u00edneas. Al crecer en funcionalidad, enfrentamos problemas t\u00edpicos de c\u00f3digo acoplado:</p> <ol> <li>Testing dif\u00edcil: Mocks de GPU, audio, API externa</li> <li>Cambios cascada: Modificar Whisper requer\u00eda tocar 5+ archivos</li> <li>Vendor lock-in: Cambiar de Ollama a Gemini requer\u00eda refactor masivo</li> <li>Responsabilidades difusas: No estaba claro d\u00f3nde poner nueva l\u00f3gica</li> </ol>"},{"location":"adr/004-hexagonal-architecture/#requisitos","title":"Requisitos:","text":"<ul> <li>N\u00facleo de negocio agn\u00f3stico a frameworks</li> <li>Adaptadores intercambiables (ej: cambiar Whisper por otro ASR)</li> <li>Testabilidad sin hardware real</li> <li>Boundaries claros entre capas</li> </ul>"},{"location":"adr/004-hexagonal-architecture/#decision","title":"Decisi\u00f3n","text":"<p>Adoptar Arquitectura Hexagonal (Ports &amp; Adapters) como patr\u00f3n estructural.</p>"},{"location":"adr/004-hexagonal-architecture/#estructura-de-carpetas","title":"Estructura de carpetas:","text":"<pre><code>src/v2m/\n\u251c\u2500\u2500 core/           # Configuraci\u00f3n, logging, interfaces base\n\u251c\u2500\u2500 domain/         # Modelos, puertos (interfaces), errores\n\u251c\u2500\u2500 services/       # Orchestrator, coordinaci\u00f3n\n\u2514\u2500\u2500 infrastructure/ # Adaptadores (Whisper, Audio, LLM)\n</code></pre>"},{"location":"adr/004-hexagonal-architecture/#implementacion-de-puertos","title":"Implementaci\u00f3n de puertos:","text":"<pre><code>from typing import Protocol, runtime_checkable\n\n@runtime_checkable\nclass TranscriptionService(Protocol):\n    async def transcribe(self, audio: bytes) -&gt; str: ...\n</code></pre> <p>Los adaptadores implementan los puertos:</p> <pre><code>class WhisperAdapter:\n    async def transcribe(self, audio: bytes) -&gt; str:\n        # Implementaci\u00f3n concreta con faster-whisper\n</code></pre>"},{"location":"adr/004-hexagonal-architecture/#consecuencias","title":"Consecuencias","text":""},{"location":"adr/004-hexagonal-architecture/#positivas","title":"Positivas","text":"<ul> <li>\u2705 Testing aislado: Tests unitarios sin GPU ni red</li> <li>\u2705 Flexibilidad: Cambiar Gemini por Ollama es editar 1 archivo</li> <li>\u2705 Onboarding: Estructura predecible y documentada</li> <li>\u2705 Type safety: <code>Protocol</code> + mypy detecta incompatibilidades en compile time</li> </ul>"},{"location":"adr/004-hexagonal-architecture/#negativas","title":"Negativas","text":"<ul> <li>\u26a0\ufe0f M\u00e1s archivos: ~20 archivos vs ~5 del script original</li> <li>\u26a0\ufe0f Indirecci\u00f3n: Hay que navegar entre capas para entender flujo completo</li> <li>\u26a0\ufe0f Overhead inicial: Setup m\u00e1s complejo para features simples</li> </ul>"},{"location":"adr/004-hexagonal-architecture/#alternativas-consideradas","title":"Alternativas Consideradas","text":""},{"location":"adr/004-hexagonal-architecture/#clean-architecture-uncle-bob","title":"Clean Architecture (Uncle Bob)","text":"<ul> <li>Rechazado: Demasiadas capas (Entities, Use Cases, Interface Adapters, Frameworks) para el scope.</li> </ul>"},{"location":"adr/004-hexagonal-architecture/#mvcmvp","title":"MVC/MVP","text":"<ul> <li>Rechazado: Orientado a UI, no aplica bien a un daemon backend.</li> </ul>"},{"location":"adr/004-hexagonal-architecture/#simple-modules","title":"Simple Modules","text":"<ul> <li>Rechazado: En la pr\u00e1ctica volv\u00edamos al acoplamiento original.</li> </ul>"},{"location":"adr/004-hexagonal-architecture/#referencias","title":"Referencias","text":"<ul> <li>Alistair Cockburn - Hexagonal Architecture</li> <li>Netflix - Ready for changes with Hexagonal Architecture</li> </ul>"},{"location":"adr/005-rust-audio-engine/","title":"ADR-005: Motor de Audio en Rust (v2m_engine)","text":""},{"location":"adr/005-rust-audio-engine/#estado","title":"Estado","text":"<p>Aceptada</p>"},{"location":"adr/005-rust-audio-engine/#fecha","title":"Fecha","text":"<p>2025-01-10</p>"},{"location":"adr/005-rust-audio-engine/#contexto","title":"Contexto","text":"<p>La captura de audio en Python puro presentaba limitaciones cr\u00edticas para una experiencia de dictado real-time:</p>"},{"location":"adr/005-rust-audio-engine/#problemas-identificados","title":"Problemas identificados:","text":"<ol> <li>GIL blocking: La captura de audio compet\u00eda con transcripci\u00f3n por el GIL</li> <li>Latencia variable: Jitter de 10-50ms en buffering de audio</li> <li>Overhead de sounddevice: Callbacks Python a\u00f1ad\u00edan latencia</li> <li>VAD ineficiente: Silero VAD en Python procesaba samples con overhead</li> </ol>"},{"location":"adr/005-rust-audio-engine/#requisitos","title":"Requisitos:","text":"<ul> <li>Latencia de captura &lt; 10ms</li> <li>VAD pre-procesado antes de Python</li> <li>Buffer circular lock-free</li> <li>Zero-copy cuando sea posible</li> </ul>"},{"location":"adr/005-rust-audio-engine/#decision","title":"Decisi\u00f3n","text":"<p>Desarrollar extensi\u00f3n nativa en Rust (<code>v2m_engine</code>) para tareas cr\u00edticas de audio.</p>"},{"location":"adr/005-rust-audio-engine/#componentes","title":"Componentes:","text":"<pre><code>v2m_engine/\n\u251c\u2500\u2500 src/\n\u2502   \u251c\u2500\u2500 audio_capture.rs  # CPAL-based capture\n\u2502   \u251c\u2500\u2500 ring_buffer.rs    # Lock-free circular buffer\n\u2502   \u251c\u2500\u2500 vad.rs            # Silero ONNX inference\n\u2502   \u2514\u2500\u2500 lib.rs            # PyO3 bindings\n</code></pre>"},{"location":"adr/005-rust-audio-engine/#interfaz-python","title":"Interfaz Python:","text":"<pre><code>from v2m_engine import AudioCapture, VADProcessor\n\ncapture = AudioCapture(sample_rate=16000, buffer_size=4096)\nvad = VADProcessor(threshold=0.5)\n\nasync with capture.stream() as audio:\n    if vad.contains_speech(audio):\n        await transcriber.process(audio)\n</code></pre>"},{"location":"adr/005-rust-audio-engine/#consecuencias","title":"Consecuencias","text":""},{"location":"adr/005-rust-audio-engine/#positivas","title":"Positivas","text":"<ul> <li>\u2705 Latencia &lt; 5ms: CPAL + lock-free buffers</li> <li>\u2705 GIL-free: Audio thread independiente de Python</li> <li>\u2705 VAD eficiente: ONNX runtime nativo, 10x m\u00e1s r\u00e1pido</li> <li>\u2705 Zero-copy: NumPy arrays comparten memoria con Rust</li> </ul>"},{"location":"adr/005-rust-audio-engine/#negativas","title":"Negativas","text":"<ul> <li>\u26a0\ufe0f Complejidad de build: Requiere Rust toolchain + maturin</li> <li>\u26a0\ufe0f Debugging cross-language: Stack traces mixtos Python/Rust</li> <li>\u26a0\ufe0f Portabilidad: Binarios espec\u00edficos por plataforma</li> </ul>"},{"location":"adr/005-rust-audio-engine/#alternativas-consideradas","title":"Alternativas Consideradas","text":""},{"location":"adr/005-rust-audio-engine/#pure-python-sounddevice-numpy","title":"Pure Python (sounddevice + numpy)","text":"<ul> <li>Rechazado: GIL blocking y latencia inaceptable.</li> </ul>"},{"location":"adr/005-rust-audio-engine/#cython","title":"Cython","text":"<ul> <li>Rechazado: Todav\u00eda atado al GIL, beneficios limitados.</li> </ul>"},{"location":"adr/005-rust-audio-engine/#c-extension","title":"C++ Extension","text":"<ul> <li>Rechazado: Rust ofrece memory safety sin GC, mejor tooling (cargo).</li> </ul>"},{"location":"adr/005-rust-audio-engine/#referencias","title":"Referencias","text":"<ul> <li>PyO3 - Rust bindings for Python</li> <li>CPAL - Cross-platform Audio Library</li> <li>Lock-free Ring Buffer</li> </ul>"},{"location":"adr/006-local-first/","title":"ADR-006: Local-first: Procesamiento sin Cloud","text":""},{"location":"adr/006-local-first/#estado","title":"Estado","text":"<p>Aceptada</p>"},{"location":"adr/006-local-first/#fecha","title":"Fecha","text":"<p>2024-03-01</p>"},{"location":"adr/006-local-first/#contexto","title":"Contexto","text":"<p>Los servicios de dictado existentes (Google Speech-to-Text, Whisper API, Dragon) requieren enviar audio a servidores externos.</p>"},{"location":"adr/006-local-first/#problemas-con-cloud-based-dictation","title":"Problemas con cloud-based dictation:","text":"<ol> <li>Privacidad: Audio sensible (m\u00e9dico, legal, personal) sale de la m\u00e1quina</li> <li>Latencia de red: 100-500ms RTT adicionales</li> <li>Disponibilidad: Requiere conexi\u00f3n a internet</li> <li>Costos: APIs de transcripci\u00f3n cobran por minuto</li> <li>Rate limits: Throttling en uso intensivo</li> </ol>"},{"location":"adr/006-local-first/#requisitos-del-usuario","title":"Requisitos del usuario:","text":"<ul> <li>Privacidad absoluta: Ning\u00fan dato debe salir de la m\u00e1quina</li> <li>Funcionamiento offline: El sistema debe operar sin internet</li> <li>Latencia m\u00ednima: &lt; 500ms end-to-end</li> <li>Costo cero: Sin suscripciones ni pagos por uso</li> </ul>"},{"location":"adr/006-local-first/#decision","title":"Decisi\u00f3n","text":"<p>Adoptar filosof\u00eda \"Local-first\" donde todo el procesamiento de voz ocurre en el dispositivo del usuario.</p>"},{"location":"adr/006-local-first/#implementacion","title":"Implementaci\u00f3n:","text":"Componente Soluci\u00f3n Local Transcripci\u00f3n faster-whisper en GPU local LLM (opcional) Ollama con modelos locales Audio Procesado en memoria RAM Almacenamiento Solo archivos temporales, eliminados post-uso"},{"location":"adr/006-local-first/#excepciones-configurables","title":"Excepciones configurables:","text":"<p>El usuario puede optar-in a servicios cloud para el refinamiento de texto:</p> <ul> <li>Google Gemini API (para LLM)</li> <li>Pero nunca para el audio crudo</li> </ul>"},{"location":"adr/006-local-first/#consecuencias","title":"Consecuencias","text":""},{"location":"adr/006-local-first/#positivas","title":"Positivas","text":"<ul> <li>\u2705 Privacidad garantizada: Audio nunca sale del dispositivo</li> <li>\u2705 Sin latencia de red: Todo procesamiento local</li> <li>\u2705 Funciona offline: No requiere internet para dictar</li> <li>\u2705 Costo predecible: Solo hardware (GPU), sin suscripciones</li> <li>\u2705 Compliance: Compatible con regulaciones (HIPAA, GDPR)</li> </ul>"},{"location":"adr/006-local-first/#negativas","title":"Negativas","text":"<ul> <li>\u26a0\ufe0f Requiere GPU: Sin GPU NVIDIA, rendimiento degradado</li> <li>\u26a0\ufe0f Modelos locales LLM: Calidad inferior a GPT-4/Gemini Pro</li> <li>\u26a0\ufe0f Actualizaciones manuales: Modelos no se auto-actualizan</li> </ul>"},{"location":"adr/006-local-first/#alternativas-consideradas","title":"Alternativas Consideradas","text":""},{"location":"adr/006-local-first/#hybrid-local-stt-cloud-llm-default","title":"Hybrid (local STT + cloud LLM default)","text":"<ul> <li>Rechazado: Viola principio de privacidad-por-defecto.</li> </ul>"},{"location":"adr/006-local-first/#cloud-first-con-cache-local","title":"Cloud-first con cache local","text":"<ul> <li>Rechazado: Complejidad innecesaria, audio a\u00fan debe subirse.</li> </ul>"},{"location":"adr/006-local-first/#federated-learning","title":"Federated Learning","text":"<ul> <li>Rechazado: Sobre-ingenier\u00eda para el scope actual.</li> </ul>"},{"location":"adr/006-local-first/#referencias","title":"Referencias","text":"<ul> <li>Local-first Software</li> <li>Ink &amp; Switch - Seven Ideals</li> <li>GDPR and Voice Data</li> </ul>"},{"location":"adr/template/","title":"ADR-XXX: T\u00edtulo Corto de la Decisi\u00f3n","text":""},{"location":"adr/template/#estado","title":"Estado","text":"<p>[Propuesto | Aceptado | Rechazado | Obsoleto]</p>"},{"location":"adr/template/#contexto","title":"Contexto","text":"<p>Describe el contexto y el problema que estamos resolviendo. - \u00bfCu\u00e1l es la limitaci\u00f3n actual? - \u00bfQu\u00e9 requisitos t\u00e9cnicos o de negocio impulsan esto?</p>"},{"location":"adr/template/#decision","title":"Decisi\u00f3n","text":"<p>Describir la decisi\u00f3n tomada. - \"Usaremos X tecnolog\u00eda para Y componente...\"</p>"},{"location":"adr/template/#consecuencias","title":"Consecuencias","text":"<p>\u00bfQu\u00e9 se vuelve m\u00e1s f\u00e1cil o dif\u00edcil debido a este cambio?</p>"},{"location":"adr/template/#positivas","title":"Positivas","text":"<p>-</p>"},{"location":"adr/template/#negativas","title":"Negativas","text":"<p>-</p>"},{"location":"adr/template/#alternativas-consideradas","title":"Alternativas Consideradas","text":"<ul> <li>Opci\u00f3n A: Por qu\u00e9 se rechaz\u00f3.</li> <li>Opci\u00f3n B: Por qu\u00e9 se rechaz\u00f3.</li> </ul>"},{"location":"api/","title":"API Python - \u00cdndice","text":"<p>Esta secci\u00f3n proporciona documentaci\u00f3n auto-generada de las clases y funciones Python del backend de Voice2Machine.</p> <p>Generado con mkdocstrings</p> <p>Esta documentaci\u00f3n se extrae autom\u00e1ticamente de los docstrings del c\u00f3digo fuente. Para la versi\u00f3n m\u00e1s actualizada, consulta siempre el c\u00f3digo en <code>apps/daemon/backend/src/v2m/</code>.</p>"},{"location":"api/#modulos-principales","title":"M\u00f3dulos Principales","text":""},{"location":"api/#interfaces","title":"Interfaces","text":"<p>Protocolos y contratos que definen el comportamiento esperado de los adaptadores.</p>"},{"location":"api/#dominio","title":"Dominio","text":"<p>Modelos de dominio, puertos y tipos de error.</p>"},{"location":"api/#servicios","title":"Servicios","text":"<p>Servicios de aplicaci\u00f3n incluyendo el Orchestrator principal.</p>"},{"location":"api/#navegacion-rapida","title":"Navegaci\u00f3n R\u00e1pida","text":"Clase/Funci\u00f3n Descripci\u00f3n <code>Orchestrator</code> Coordinador central del flujo de trabajo <code>TranscriptionService</code> Puerto para servicios de transcripci\u00f3n <code>AudioRecorder</code> Interfaz para captura de audio <code>LLMProvider</code> Interfaz base para proveedores de IA"},{"location":"api/domain/","title":"Dominio","text":"<p>Esta p\u00e1gina documenta los modelos de dominio y tipos de datos del sistema.</p>"},{"location":"api/domain/#modelos-de-datos","title":"Modelos de Datos","text":""},{"location":"api/domain/#correctionresult","title":"CorrectionResult","text":"<p>Modelo de salida estructurada para refinamiento de texto.</p> <p>Este modelo fuerza a los LLMs a responder en un formato JSON predecible, facilitando el parsing y reduciendo alucinaciones de formato.</p> <pre><code>from pydantic import BaseModel, Field\n\nclass CorrectionResult(BaseModel):\n    corrected_text: str = Field(\n        description=\"Texto corregido con gram\u00e1tica y coherencia mejoradas\"\n    )\n    explanation: str | None = Field(\n        default=None,\n        description=\"Cambios realizados al texto original\"\n    )\n</code></pre> <p>Ejemplo de uso:</p> <pre><code>result = CorrectionResult(\n    corrected_text=\"Hola, \u00bfc\u00f3mo est\u00e1s?\",\n    explanation=\"A\u00f1adida puntuaci\u00f3n y signos de interrogaci\u00f3n\"\n)\n</code></pre>"},{"location":"api/domain/#errores-de-dominio","title":"Errores de Dominio","text":"<p>El sistema define excepciones espec\u00edficas para diferentes tipos de errores:</p> Excepci\u00f3n Descripci\u00f3n <code>TranscriptionError</code> Error durante la transcripci\u00f3n de audio <code>AudioCaptureError</code> Error en la captura de audio (micr\u00f3fono no disponible) <code>LLMError</code> Error al comunicarse con el proveedor LLM <code>ConfigurationError</code> Error en la configuraci\u00f3n del sistema"},{"location":"api/interfaces/","title":"Interfaces (Protocolos)","text":"<p>Esta p\u00e1gina documenta los protocolos (interfaces) que definen los contratos del sistema.</p>"},{"location":"api/interfaces/#orchestrator","title":"Orchestrator","text":"<p>El Orchestrator es el componente central que coordina todo el flujo de trabajo.</p>"},{"location":"api/interfaces/#metodos-principales","title":"M\u00e9todos Principales","text":"<pre><code>class Orchestrator:\n    async def toggle(self) -&gt; ToggleResponse:\n        \"\"\"Toggle de grabaci\u00f3n (iniciar/detener).\"\"\"\n\n    async def start(self) -&gt; ToggleResponse:\n        \"\"\"Inicia la grabaci\u00f3n de audio.\"\"\"\n\n    async def stop(self) -&gt; ToggleResponse:\n        \"\"\"Detiene la grabaci\u00f3n y transcribe el audio.\"\"\"\n\n    async def warmup(self) -&gt; None:\n        \"\"\"Pre-carga el modelo Whisper en VRAM.\"\"\"\n\n    async def shutdown(self) -&gt; None:\n        \"\"\"Libera recursos al apagar el servidor.\"\"\"\n\n    def get_status(self) -&gt; StatusResponse:\n        \"\"\"Retorna estado actual del daemon.\"\"\"\n\n    async def process_text(self, text: str) -&gt; LLMResponse:\n        \"\"\"Procesa texto con LLM (limpieza, puntuaci\u00f3n).\"\"\"\n\n    async def translate_text(self, text: str, target_lang: str) -&gt; LLMResponse:\n        \"\"\"Traduce texto con LLM.\"\"\"\n</code></pre>"},{"location":"api/interfaces/#response-models","title":"Response Models","text":""},{"location":"api/interfaces/#toggleresponse","title":"ToggleResponse","text":"<pre><code>class ToggleResponse(BaseModel):\n    status: str      # 'recording' | 'idle'\n    message: str     # Mensaje descriptivo\n    text: str | None # Texto transcrito (solo en stop)\n</code></pre>"},{"location":"api/interfaces/#statusresponse","title":"StatusResponse","text":"<pre><code>class StatusResponse(BaseModel):\n    state: str        # 'idle' | 'recording' | 'processing'\n    recording: bool   # True si est\u00e1 grabando\n    model_loaded: bool # True si Whisper est\u00e1 en VRAM\n</code></pre>"},{"location":"api/interfaces/#llmresponse","title":"LLMResponse","text":"<pre><code>class LLMResponse(BaseModel):\n    text: str    # Texto procesado/traducido\n    backend: str # 'gemini' | 'ollama' | 'local'\n</code></pre>"},{"location":"api/services/","title":"Servicios","text":"<p>Esta p\u00e1gina documenta los servicios de aplicaci\u00f3n del backend.</p>"},{"location":"api/services/#orchestrator","title":"Orchestrator","text":"<p>El Orchestrator es el servicio central que coordina todo el flujo de trabajo de Voice2Machine.</p>"},{"location":"api/services/#responsabilidades","title":"Responsabilidades","text":"<ul> <li>Gestiona el ciclo de vida completo: grabaci\u00f3n \u2192 transcripci\u00f3n \u2192 post-procesamiento</li> <li>Mantiene el estado del sistema (idle, recording, processing)</li> <li>Coordina la comunicaci\u00f3n entre adaptadores sin acoplarlos directamente</li> <li>Emite eventos a clientes WebSocket conectados</li> </ul>"},{"location":"api/services/#lazy-initialization","title":"Lazy Initialization","text":"<p>Todos los sub-servicios se crean cuando se necesitan por primera vez:</p> <pre><code>@property\ndef worker(self) -&gt; WhisperWorker:\n    \"\"\"Obtiene el worker de Whisper (lazy init).\"\"\"\n    if self._worker is None:\n        self._worker = WhisperWorker()\n    return self._worker\n</code></pre>"},{"location":"api/services/#servicios-coordinados","title":"Servicios Coordinados","text":"Servicio Descripci\u00f3n <code>WhisperWorker</code> Transcripci\u00f3n con faster-whisper <code>AudioRecorder</code> Captura de audio (Rust extension) <code>StreamingTranscriber</code> Transcripci\u00f3n en tiempo real <code>ClipboardService</code> Acceso al portapapeles del sistema <code>NotificationService</code> Notificaciones de escritorio <code>LLMService</code> Procesamiento con Gemini/Ollama"},{"location":"api/backend/","title":"API Backend (Python)","text":"<p>Esta secci\u00f3n contiene documentaci\u00f3n generada autom\u00e1ticamente desde el c\u00f3digo fuente del backend de Voice2Machine.</p> <p>Auto-generada</p> <p>Esta documentaci\u00f3n se sincroniza autom\u00e1ticamente con los docstrings del c\u00f3digo. La fuente de verdad es: <code>apps/daemon/backend/src/v2m/</code></p>"},{"location":"api/backend/#modulos-principales","title":"M\u00f3dulos Principales","text":""},{"location":"api/backend/#servicio-de-coordinacion","title":"Servicio de Coordinaci\u00f3n","text":"<ul> <li>Orchestrator - Coordinador central del sistema</li> <li>API REST - Endpoints FastAPI y modelos de datos</li> </ul>"},{"location":"api/backend/#configuracion","title":"Configuraci\u00f3n","text":"<ul> <li>Config - Sistema de configuraci\u00f3n tipada</li> </ul>"},{"location":"api/backend/#infraestructura","title":"Infraestructura","text":"<ul> <li>Transcripci\u00f3n - Whisper y streaming</li> <li>LLM Services - Gemini, Ollama, Local</li> </ul>"},{"location":"api/backend/#navegacion-por-capas","title":"Navegaci\u00f3n por Capas","text":"<pre><code>graph TD\n    A[API REST] --&gt; B[Orchestrator]\n    B --&gt; C[Infrastructure]\n    C --&gt; D[Whisper]\n    C --&gt; E[Audio Recorder]\n    C --&gt; F[LLM Providers]\n\n    style A fill:#e3f2fd\n    style B fill:#fff3e0\n    style C fill:#f3e5f5</code></pre> Capa Responsabilidad API Endpoints HTTP, validaci\u00f3n, serializaci\u00f3n Services Coordinaci\u00f3n de flujo de trabajo Infrastructure Adaptadores a servicios externos"},{"location":"api/backend/#estado-del-codigo","title":"Estado del C\u00f3digo","text":"M\u00e9trica Valor Archivos Python 27 Cobertura docstrings ~70% Estilo Google Style"},{"location":"api/backend/api/","title":"API REST","text":"<p>Documentaci\u00f3n de los endpoints FastAPI y modelos de datos.</p>"},{"location":"api/backend/api/#modelos-de-requestresponse","title":"Modelos de Request/Response","text":"<p>options: show_source: false</p> <p>options: show_source: false</p> <p>options: show_source: false</p> <p>options: show_source: false</p> <p>options: show_source: false</p>"},{"location":"api/backend/api/#v2m.api.ToggleResponse","title":"<code>v2m.api.ToggleResponse</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Respuesta del endpoint /toggle.</p> Source code in <code>apps/daemon/backend/src/v2m/api.py</code> <pre><code>class ToggleResponse(BaseModel):\n    \"\"\"Respuesta del endpoint /toggle.\"\"\"\n\n    status: str = Field(description=\"Estado actual: 'recording' o 'idle'\")\n    message: str = Field(description=\"Mensaje descriptivo para el usuario\")\n    text: str | None = Field(default=None, description=\"Texto transcrito (solo en stop)\")\n</code></pre>"},{"location":"api/backend/api/#v2m.api.StatusResponse","title":"<code>v2m.api.StatusResponse</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Respuesta del endpoint /status.</p> Source code in <code>apps/daemon/backend/src/v2m/api.py</code> <pre><code>class StatusResponse(BaseModel):\n    \"\"\"Respuesta del endpoint /status.\"\"\"\n\n    state: str = Field(description=\"Estado del daemon: 'idle', 'recording', 'processing'\")\n    recording: bool = Field(description=\"True si est\u00e1 grabando actualmente\")\n    model_loaded: bool = Field(description=\"True si el modelo Whisper est\u00e1 cargado\")\n</code></pre>"},{"location":"api/backend/api/#v2m.api.LLMResponse","title":"<code>v2m.api.LLMResponse</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Respuesta de endpoints LLM.</p> Source code in <code>apps/daemon/backend/src/v2m/api.py</code> <pre><code>class LLMResponse(BaseModel):\n    \"\"\"Respuesta de endpoints LLM.\"\"\"\n\n    text: str = Field(description=\"Texto procesado/traducido\")\n    backend: str = Field(description=\"Backend usado: 'gemini', 'ollama', 'local'\")\n</code></pre>"},{"location":"api/backend/api/#v2m.api.ProcessTextRequest","title":"<code>v2m.api.ProcessTextRequest</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Request para /llm/process.</p> Source code in <code>apps/daemon/backend/src/v2m/api.py</code> <pre><code>class ProcessTextRequest(BaseModel):\n    \"\"\"Request para /llm/process.\"\"\"\n\n    text: str = Field(min_length=1, max_length=10000, description=\"Texto a procesar\")\n</code></pre>"},{"location":"api/backend/api/#v2m.api.TranslateTextRequest","title":"<code>v2m.api.TranslateTextRequest</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Request para /llm/translate.</p> Source code in <code>apps/daemon/backend/src/v2m/api.py</code> <pre><code>class TranslateTextRequest(BaseModel):\n    \"\"\"Request para /llm/translate.\"\"\"\n\n    text: str = Field(min_length=1, max_length=10000, description=\"Texto a traducir\")\n    target_lang: str = Field(default=\"en\", description=\"Idioma destino (ej. 'en', 'es')\")\n</code></pre>"},{"location":"api/backend/api/#estado-global","title":"Estado Global","text":"<p>options: show_source: true members: - init - orchestrator - broadcast_event</p>"},{"location":"api/backend/api/#v2m.api.DaemonState","title":"<code>v2m.api.DaemonState</code>","text":"<p>Estado global del daemon.</p> <p>Lazy initialization: los servicios se crean cuando se necesitan por primera vez. Esto permite que el servidor arranque r\u00e1pido y cargue el modelo en background.</p> Source code in <code>apps/daemon/backend/src/v2m/api.py</code> <pre><code>class DaemonState:\n    \"\"\"Estado global del daemon.\n\n    Lazy initialization: los servicios se crean cuando se necesitan por primera vez.\n    Esto permite que el servidor arranque r\u00e1pido y cargue el modelo en background.\n    \"\"\"\n\n    def __init__(self) -&gt; None:\n        \"\"\"Inicializa el estado global con servicios diferidos.\n\n        Los servicios pesados (Orchestrator, modelo Whisper) no se crean aqu\u00ed.\n        Se instancian lazy la primera vez que se acceden.\n        \"\"\"\n        self._orchestrator: Orchestrator | None = None\n        self._websocket_clients: set[WebSocket] = set()\n\n    @property\n    def orchestrator(self) -&gt; Orchestrator:\n        \"\"\"Lazy initialization del orquestador.\"\"\"\n        if self._orchestrator is None:\n            from v2m.services.orchestrator import Orchestrator\n\n            self._orchestrator = Orchestrator(broadcast_fn=self.broadcast_event)\n        return self._orchestrator\n\n    async def broadcast_event(self, event_type: str, data: dict[str, Any]) -&gt; None:\n        \"\"\"Env\u00eda evento a todos los clientes WebSocket conectados.\n\n        Args:\n            event_type: Tipo de evento (ej. 'transcription_update', 'heartbeat').\n            data: Payload del evento como diccionario JSON-serializable.\n\n        Note:\n            Las conexiones muertas se limpian autom\u00e1ticamente.\n        \"\"\"\n        if not self._websocket_clients:\n            return\n\n        message = {\"event\": event_type, \"data\": data}\n        disconnected: list[WebSocket] = []\n\n        for ws in self._websocket_clients:\n            try:\n                await ws.send_json(message)\n            except Exception:\n                disconnected.append(ws)\n\n        # Cleanup de conexiones muertas\n        for ws in disconnected:\n            self._websocket_clients.discard(ws)\n</code></pre>"},{"location":"api/backend/api/#v2m.api.DaemonState.orchestrator","title":"<code>orchestrator</code>  <code>property</code>","text":"<p>Lazy initialization del orquestador.</p>"},{"location":"api/backend/api/#v2m.api.DaemonState.__init__","title":"<code>__init__()</code>","text":"<p>Inicializa el estado global con servicios diferidos.</p> <p>Los servicios pesados (Orchestrator, modelo Whisper) no se crean aqu\u00ed. Se instancian lazy la primera vez que se acceden.</p> Source code in <code>apps/daemon/backend/src/v2m/api.py</code> <pre><code>def __init__(self) -&gt; None:\n    \"\"\"Inicializa el estado global con servicios diferidos.\n\n    Los servicios pesados (Orchestrator, modelo Whisper) no se crean aqu\u00ed.\n    Se instancian lazy la primera vez que se acceden.\n    \"\"\"\n    self._orchestrator: Orchestrator | None = None\n    self._websocket_clients: set[WebSocket] = set()\n</code></pre>"},{"location":"api/backend/api/#v2m.api.DaemonState.broadcast_event","title":"<code>broadcast_event(event_type, data)</code>  <code>async</code>","text":"<p>Env\u00eda evento a todos los clientes WebSocket conectados.</p> <p>Parameters:</p> Name Type Description Default <code>event_type</code> <code>str</code> <p>Tipo de evento (ej. 'transcription_update', 'heartbeat').</p> required <code>data</code> <code>dict[str, Any]</code> <p>Payload del evento como diccionario JSON-serializable.</p> required Note <p>Las conexiones muertas se limpian autom\u00e1ticamente.</p> Source code in <code>apps/daemon/backend/src/v2m/api.py</code> <pre><code>async def broadcast_event(self, event_type: str, data: dict[str, Any]) -&gt; None:\n    \"\"\"Env\u00eda evento a todos los clientes WebSocket conectados.\n\n    Args:\n        event_type: Tipo de evento (ej. 'transcription_update', 'heartbeat').\n        data: Payload del evento como diccionario JSON-serializable.\n\n    Note:\n        Las conexiones muertas se limpian autom\u00e1ticamente.\n    \"\"\"\n    if not self._websocket_clients:\n        return\n\n    message = {\"event\": event_type, \"data\": data}\n    disconnected: list[WebSocket] = []\n\n    for ws in self._websocket_clients:\n        try:\n            await ws.send_json(message)\n        except Exception:\n            disconnected.append(ws)\n\n    # Cleanup de conexiones muertas\n    for ws in disconnected:\n        self._websocket_clients.discard(ws)\n</code></pre>"},{"location":"api/backend/api/#endpoints","title":"Endpoints","text":"<p>options: show_source: true</p> <p>options: show_source: true</p> <p>options: show_source: true</p> <p>options: show_source: true</p> <p>options: show_source: true</p> <p>options: show_source: true</p> <p>options: show_source: true</p>"},{"location":"api/backend/api/#v2m.api.toggle_recording","title":"<code>v2m.api.toggle_recording()</code>  <code>async</code>","text":"<p>Toggle de grabaci\u00f3n (iniciar/detener).</p> <p>Si no est\u00e1 grabando \u2192 inicia grabaci\u00f3n. Si est\u00e1 grabando \u2192 detiene y transcribe.</p> <p>Este es el endpoint principal que usa el atajo de teclado.</p> Source code in <code>apps/daemon/backend/src/v2m/api.py</code> <pre><code>@app.post(\"/toggle\", response_model=ToggleResponse)\nasync def toggle_recording() -&gt; ToggleResponse:\n    \"\"\"Toggle de grabaci\u00f3n (iniciar/detener).\n\n    Si no est\u00e1 grabando \u2192 inicia grabaci\u00f3n.\n    Si est\u00e1 grabando \u2192 detiene y transcribe.\n\n    Este es el endpoint principal que usa el atajo de teclado.\n    \"\"\"\n    return await _state.orchestrator.toggle()\n</code></pre>"},{"location":"api/backend/api/#v2m.api.start_recording","title":"<code>v2m.api.start_recording()</code>  <code>async</code>","text":"<p>Inicia grabaci\u00f3n expl\u00edcitamente.</p> Source code in <code>apps/daemon/backend/src/v2m/api.py</code> <pre><code>@app.post(\"/start\", response_model=ToggleResponse)\nasync def start_recording() -&gt; ToggleResponse:\n    \"\"\"Inicia grabaci\u00f3n expl\u00edcitamente.\"\"\"\n    return await _state.orchestrator.start()\n</code></pre>"},{"location":"api/backend/api/#v2m.api.stop_recording","title":"<code>v2m.api.stop_recording()</code>  <code>async</code>","text":"<p>Detiene grabaci\u00f3n y transcribe.</p> Source code in <code>apps/daemon/backend/src/v2m/api.py</code> <pre><code>@app.post(\"/stop\", response_model=ToggleResponse)\nasync def stop_recording() -&gt; ToggleResponse:\n    \"\"\"Detiene grabaci\u00f3n y transcribe.\"\"\"\n    return await _state.orchestrator.stop()\n</code></pre>"},{"location":"api/backend/api/#v2m.api.get_status","title":"<code>v2m.api.get_status()</code>  <code>async</code>","text":"<p>Retorna estado actual del daemon.</p> Source code in <code>apps/daemon/backend/src/v2m/api.py</code> <pre><code>@app.get(\"/status\", response_model=StatusResponse)\nasync def get_status() -&gt; StatusResponse:\n    \"\"\"Retorna estado actual del daemon.\"\"\"\n    return _state.orchestrator.get_status()\n</code></pre>"},{"location":"api/backend/api/#v2m.api.health_check","title":"<code>v2m.api.health_check()</code>  <code>async</code>","text":"<p>Health check para systemd/scripts.</p> Source code in <code>apps/daemon/backend/src/v2m/api.py</code> <pre><code>@app.get(\"/health\", response_model=HealthResponse)\nasync def health_check() -&gt; HealthResponse:\n    \"\"\"Health check para systemd/scripts.\"\"\"\n    return HealthResponse()\n</code></pre>"},{"location":"api/backend/api/#v2m.api.process_text","title":"<code>v2m.api.process_text(request)</code>  <code>async</code>","text":"<p>Procesa texto con LLM (limpieza, puntuaci\u00f3n, formato).</p> <p>El backend se selecciona seg\u00fan config.toml (gemini/ollama/local).</p> Source code in <code>apps/daemon/backend/src/v2m/api.py</code> <pre><code>@app.post(\"/llm/process\", response_model=LLMResponse)\nasync def process_text(request: ProcessTextRequest) -&gt; LLMResponse:\n    \"\"\"Procesa texto con LLM (limpieza, puntuaci\u00f3n, formato).\n\n    El backend se selecciona seg\u00fan config.toml (gemini/ollama/local).\n    \"\"\"\n    return await _state.orchestrator.process_text(request.text)\n</code></pre>"},{"location":"api/backend/api/#v2m.api.translate_text","title":"<code>v2m.api.translate_text(request)</code>  <code>async</code>","text":"<p>Traduce texto con LLM.</p> Source code in <code>apps/daemon/backend/src/v2m/api.py</code> <pre><code>@app.post(\"/llm/translate\", response_model=LLMResponse)\nasync def translate_text(request: TranslateTextRequest) -&gt; LLMResponse:\n    \"\"\"Traduce texto con LLM.\"\"\"\n    return await _state.orchestrator.translate_text(request.text, request.target_lang)\n</code></pre>"},{"location":"api/backend/config/","title":"Configuraci\u00f3n","text":"<p>Sistema de configuraci\u00f3n tipada usando Pydantic Settings.</p>"},{"location":"api/backend/config/#settings-principal","title":"Settings Principal","text":"<p>options: show_source: false members: - paths - transcription - llm - gemini - notifications</p>"},{"location":"api/backend/config/#v2m.config.Settings","title":"<code>v2m.config.Settings</code>","text":"<p>               Bases: <code>BaseSettings</code></p> <p>Configuraci\u00f3n Principal de la Aplicaci\u00f3n.</p> <p>Agrega todas las secciones de configuraci\u00f3n utilizando Pydantic Settings.</p> Atributos <p>paths: Configuraci\u00f3n de rutas. transcription: Configuraci\u00f3n de transcripci\u00f3n. gemini: Configuraci\u00f3n de Gemini LLM. notifications: Configuraci\u00f3n de notificaciones. llm: Configuraci\u00f3n de LLM.</p> Source code in <code>apps/daemon/backend/src/v2m/config.py</code> <pre><code>class Settings(BaseSettings):\n    \"\"\"Configuraci\u00f3n Principal de la Aplicaci\u00f3n.\n\n    Agrega todas las secciones de configuraci\u00f3n utilizando Pydantic Settings.\n\n    Atributos:\n        paths: Configuraci\u00f3n de rutas.\n        transcription: Configuraci\u00f3n de transcripci\u00f3n.\n        gemini: Configuraci\u00f3n de Gemini LLM.\n        notifications: Configuraci\u00f3n de notificaciones.\n        llm: Configuraci\u00f3n de LLM.\n    \"\"\"\n\n    paths: PathsConfig = Field(default_factory=PathsConfig)\n    # whisper field removed in favor of transcription.whisper\n    gemini: GeminiConfig = Field(default_factory=GeminiConfig)\n    notifications: NotificationsConfig = Field(default_factory=NotificationsConfig)\n    llm: LLMConfig = Field(default_factory=LLMConfig)\n    transcription: TranscriptionConfig = Field(default_factory=TranscriptionConfig)\n\n    model_config = SettingsConfigDict(\n        env_file=\".env\",\n        env_file_encoding=\"utf-8\",\n        extra=\"ignore\",\n        toml_file=BASE_DIR / \"config.toml\",\n        frozen=True,\n    )\n\n    @classmethod\n    def settings_customise_sources(\n        cls,\n        settings_cls: type[BaseSettings],\n        init_settings: PydanticBaseSettingsSource,\n        env_settings: PydanticBaseSettingsSource,\n        dotenv_settings: PydanticBaseSettingsSource,\n        file_secret_settings: PydanticBaseSettingsSource,\n    ) -&gt; tuple[PydanticBaseSettingsSource, ...]:\n        \"\"\"Personaliza la prioridad de las fuentes de configuraci\u00f3n.\"\"\"\n        return (\n            init_settings,\n            env_settings,\n            dotenv_settings,\n            TomlConfigSettingsSource(settings_cls),\n            file_secret_settings,\n        )\n</code></pre>"},{"location":"api/backend/config/#v2m.config.Settings.settings_customise_sources","title":"<code>settings_customise_sources(settings_cls, init_settings, env_settings, dotenv_settings, file_secret_settings)</code>  <code>classmethod</code>","text":"<p>Personaliza la prioridad de las fuentes de configuraci\u00f3n.</p> Source code in <code>apps/daemon/backend/src/v2m/config.py</code> <pre><code>@classmethod\ndef settings_customise_sources(\n    cls,\n    settings_cls: type[BaseSettings],\n    init_settings: PydanticBaseSettingsSource,\n    env_settings: PydanticBaseSettingsSource,\n    dotenv_settings: PydanticBaseSettingsSource,\n    file_secret_settings: PydanticBaseSettingsSource,\n) -&gt; tuple[PydanticBaseSettingsSource, ...]:\n    \"\"\"Personaliza la prioridad de las fuentes de configuraci\u00f3n.\"\"\"\n    return (\n        init_settings,\n        env_settings,\n        dotenv_settings,\n        TomlConfigSettingsSource(settings_cls),\n        file_secret_settings,\n    )\n</code></pre>"},{"location":"api/backend/config/#configuracion-de-rutas","title":"Configuraci\u00f3n de Rutas","text":"<p>options: show_source: false</p>"},{"location":"api/backend/config/#v2m.config.PathsConfig","title":"<code>v2m.config.PathsConfig</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuraci\u00f3n para rutas de archivos y directorios.</p> Atributos <p>recording_flag: Ruta al archivo PID que indica grabaci\u00f3n activa. audio_file: Ruta al archivo WAV temporal para audio grabado. log_file: Ruta al archivo de log para depuraci\u00f3n. venv_path: Ruta al entorno virtual de Python.</p> Source code in <code>apps/daemon/backend/src/v2m/config.py</code> <pre><code>class PathsConfig(BaseModel):\n    \"\"\"Configuraci\u00f3n para rutas de archivos y directorios.\n\n    Atributos:\n        recording_flag: Ruta al archivo PID que indica grabaci\u00f3n activa.\n        audio_file: Ruta al archivo WAV temporal para audio grabado.\n        log_file: Ruta al archivo de log para depuraci\u00f3n.\n        venv_path: Ruta al entorno virtual de Python.\n    \"\"\"\n\n    recording_flag: Path = Field(default=RUNTIME_DIR / \"v2m_recording.pid\")\n    audio_file: Path = Field(default=RUNTIME_DIR / \"v2m_audio.wav\")\n    log_file: Path = Field(default=RUNTIME_DIR / \"v2m_debug.log\")\n    venv_path: Path = Field(default=BASE_DIR / \"venv\")\n</code></pre>"},{"location":"api/backend/config/#configuracion-de-transcripcion","title":"Configuraci\u00f3n de Transcripci\u00f3n","text":"<p>options: show_source: false</p> <p>options: show_source: false</p> <p>options: show_source: false</p>"},{"location":"api/backend/config/#v2m.config.TranscriptionConfig","title":"<code>v2m.config.TranscriptionConfig</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuraci\u00f3n del Servicio de Transcripci\u00f3n.</p> Atributos <p>backend: Selector de backend (\"whisper\"). Defecto: \"whisper\" whisper: Configuraci\u00f3n para el backend Whisper.</p> Source code in <code>apps/daemon/backend/src/v2m/config.py</code> <pre><code>class TranscriptionConfig(BaseModel):\n    \"\"\"Configuraci\u00f3n del Servicio de Transcripci\u00f3n.\n\n    Atributos:\n        backend: Selector de backend (\"whisper\"). Defecto: \"whisper\"\n        whisper: Configuraci\u00f3n para el backend Whisper.\n    \"\"\"\n\n    backend: str = Field(default=\"whisper\")\n    whisper: WhisperConfig = Field(default_factory=WhisperConfig)\n</code></pre>"},{"location":"api/backend/config/#v2m.config.WhisperConfig","title":"<code>v2m.config.WhisperConfig</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuraci\u00f3n del modelo de transcripci\u00f3n Whisper.</p> Atributos <p>model: Nombre o ruta del modelo Whisper (ej. 'tiny', 'base', 'large-v3').     Defecto: 'large-v2' language: C\u00f3digo de idioma ISO 639-1 (ej. 'es', 'en') o 'auto'.     Defecto: 'es' device: Dispositivo de c\u00f3mputo ('cuda' para GPU, 'cpu').     Defecto: 'cuda' compute_type: Precisi\u00f3n num\u00e9rica ('float16', 'int8_float16', 'int8').     Defecto: 'int8_float16' device_index: \u00cdndice de GPU a utilizar. Defecto: 0 num_workers: N\u00famero de workers para procesamiento paralelo. Defecto: 4 beam_size: Tama\u00f1o del beam search. Defecto: 2 best_of: N\u00famero de candidatos a considerar. Defecto: 2 temperature: Temperatura de muestreo (0.0 para determin\u00edstico).     Defecto: 0.0 vad_filter: Activar filtrado VAD. Defecto: True vad_parameters: Configuraci\u00f3n detallada del VAD. audio_device_index: \u00cdndice del dispositivo de entrada de audio (None para defecto).</p> Source code in <code>apps/daemon/backend/src/v2m/config.py</code> <pre><code>class WhisperConfig(BaseModel):\n    \"\"\"Configuraci\u00f3n del modelo de transcripci\u00f3n Whisper.\n\n    Atributos:\n        model: Nombre o ruta del modelo Whisper (ej. 'tiny', 'base', 'large-v3').\n            Defecto: 'large-v2'\n        language: C\u00f3digo de idioma ISO 639-1 (ej. 'es', 'en') o 'auto'.\n            Defecto: 'es'\n        device: Dispositivo de c\u00f3mputo ('cuda' para GPU, 'cpu').\n            Defecto: 'cuda'\n        compute_type: Precisi\u00f3n num\u00e9rica ('float16', 'int8_float16', 'int8').\n            Defecto: 'int8_float16'\n        device_index: \u00cdndice de GPU a utilizar. Defecto: 0\n        num_workers: N\u00famero de workers para procesamiento paralelo. Defecto: 4\n        beam_size: Tama\u00f1o del beam search. Defecto: 2\n        best_of: N\u00famero de candidatos a considerar. Defecto: 2\n        temperature: Temperatura de muestreo (0.0 para determin\u00edstico).\n            Defecto: 0.0\n        vad_filter: Activar filtrado VAD. Defecto: True\n        vad_parameters: Configuraci\u00f3n detallada del VAD.\n        audio_device_index: \u00cdndice del dispositivo de entrada de audio (None para defecto).\n    \"\"\"\n\n    model: str = \"large-v2\"\n    language: str = \"es\"\n    device: str = \"cuda\"\n    compute_type: str = \"int8_float16\"\n    device_index: int = 0\n    num_workers: int = 4\n    beam_size: int = 2\n    best_of: int = 2\n    temperature: float | list[float] = 0.0\n    vad_filter: bool = True\n    audio_device_index: int | None = None\n    keep_warm: bool = Field(default=True)\n    vad_parameters: VadParametersConfig = Field(default_factory=VadParametersConfig)\n</code></pre>"},{"location":"api/backend/config/#v2m.config.VadParametersConfig","title":"<code>v2m.config.VadParametersConfig</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Par\u00e1metros para la Detecci\u00f3n de Actividad de Voz (VAD).</p> <p>El VAD filtra segmentos de silencio antes de la transcripci\u00f3n para mejorar la eficiencia y reducir alucinaciones del modelo.</p> Atributos <p>threshold: Umbral de probabilidad (0.0 a 1.0) para clasificar un segmento como habla.     Defecto: 0.4 (slightly higher to avoid breathing noise) min_speech_duration_ms: Duraci\u00f3n m\u00ednima (ms) para ser considerado habla.     Defecto: 250ms min_silence_duration_ms: Duraci\u00f3n m\u00ednima de silencio (ms) para considerar que el habla termin\u00f3.     Defecto: 1000ms (Spanish prosody safe - preserves natural pauses) speech_pad_ms: Relleno aplicado al inicio/fin de segmentos de habla detectados.     Defecto: 400ms (keeps the start/end of words)</p> Source code in <code>apps/daemon/backend/src/v2m/config.py</code> <pre><code>class VadParametersConfig(BaseModel):\n    \"\"\"Par\u00e1metros para la Detecci\u00f3n de Actividad de Voz (VAD).\n\n    El VAD filtra segmentos de silencio antes de la transcripci\u00f3n para mejorar la eficiencia\n    y reducir alucinaciones del modelo.\n\n    Atributos:\n        threshold: Umbral de probabilidad (0.0 a 1.0) para clasificar un segmento como habla.\n            Defecto: 0.4 (slightly higher to avoid breathing noise)\n        min_speech_duration_ms: Duraci\u00f3n m\u00ednima (ms) para ser considerado habla.\n            Defecto: 250ms\n        min_silence_duration_ms: Duraci\u00f3n m\u00ednima de silencio (ms) para considerar que el habla termin\u00f3.\n            Defecto: 1000ms (Spanish prosody safe - preserves natural pauses)\n        speech_pad_ms: Relleno aplicado al inicio/fin de segmentos de habla detectados.\n            Defecto: 400ms (keeps the start/end of words)\n    \"\"\"\n\n    threshold: float = 0.4\n    min_speech_duration_ms: int = 250\n    min_silence_duration_ms: int = 1000\n    speech_pad_ms: int = 400\n</code></pre>"},{"location":"api/backend/config/#configuracion-llm","title":"Configuraci\u00f3n LLM","text":"<p>options: show_source: false</p> <p>options: show_source: false</p> <p>options: show_source: false</p> <p>options: show_source: false</p>"},{"location":"api/backend/config/#v2m.config.LLMConfig","title":"<code>v2m.config.LLMConfig</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuraci\u00f3n del Servicio LLM.</p> Atributos <p>backend: Selector de backend (\"local\", \"gemini\" u \"ollama\"). Defecto: \"local\" local: Configuraci\u00f3n para el backend local llama.cpp. ollama: Configuraci\u00f3n para el backend Ollama.</p> Source code in <code>apps/daemon/backend/src/v2m/config.py</code> <pre><code>class LLMConfig(BaseModel):\n    \"\"\"Configuraci\u00f3n del Servicio LLM.\n\n    Atributos:\n        backend: Selector de backend (\"local\", \"gemini\" u \"ollama\"). Defecto: \"local\"\n        local: Configuraci\u00f3n para el backend local llama.cpp.\n        ollama: Configuraci\u00f3n para el backend Ollama.\n    \"\"\"\n\n    backend: Literal[\"local\", \"gemini\", \"ollama\"] = Field(default=\"local\")\n    local: LocalLLMConfig = Field(default_factory=LocalLLMConfig)\n    ollama: OllamaConfig = Field(default_factory=OllamaConfig)\n</code></pre>"},{"location":"api/backend/config/#v2m.config.GeminiConfig","title":"<code>v2m.config.GeminiConfig</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuraci\u00f3n del servicio LLM Google Gemini.</p> Atributos <p>model: Identificador del modelo Gemini (ej. 'models/gemini-1.5-flash-latest'). temperature: Temperatura de generaci\u00f3n (0.0 a 2.0). Defecto: 0.3 max_tokens: M\u00e1ximo de tokens a generar. Defecto: 2048 max_input_chars: L\u00edmite de caracteres de entrada. Defecto: 6000 request_timeout: Tiempo de espera de solicitud HTTP en segundos. Defecto: 30 retry_attempts: N\u00famero de reintentos autom\u00e1ticos. Defecto: 3 retry_min_wait: Espera m\u00ednima entre reintentos (segundos). Defecto: 2 retry_max_wait: Espera m\u00e1xima entre reintentos (segundos). Defecto: 10 api_key: Clave de API para Google Cloud (configurar v\u00eda variable de entorno GEMINI_API_KEY).</p> Source code in <code>apps/daemon/backend/src/v2m/config.py</code> <pre><code>class GeminiConfig(BaseModel):\n    \"\"\"Configuraci\u00f3n del servicio LLM Google Gemini.\n\n    Atributos:\n        model: Identificador del modelo Gemini (ej. 'models/gemini-1.5-flash-latest').\n        temperature: Temperatura de generaci\u00f3n (0.0 a 2.0). Defecto: 0.3\n        max_tokens: M\u00e1ximo de tokens a generar. Defecto: 2048\n        max_input_chars: L\u00edmite de caracteres de entrada. Defecto: 6000\n        request_timeout: Tiempo de espera de solicitud HTTP en segundos. Defecto: 30\n        retry_attempts: N\u00famero de reintentos autom\u00e1ticos. Defecto: 3\n        retry_min_wait: Espera m\u00ednima entre reintentos (segundos). Defecto: 2\n        retry_max_wait: Espera m\u00e1xima entre reintentos (segundos). Defecto: 10\n        api_key: Clave de API para Google Cloud (configurar v\u00eda variable de entorno GEMINI_API_KEY).\n    \"\"\"\n\n    model: str = \"models/gemini-1.5-flash-latest\"\n    temperature: float = 0.3\n    max_tokens: int = 2048\n    max_input_chars: int = 6000\n    request_timeout: int = 30\n    retry_attempts: int = 3\n    retry_min_wait: int = 2\n    retry_max_wait: int = 10\n    translation_temperature: float = 0.3\n    api_key: str | None = Field(default=None)\n</code></pre>"},{"location":"api/backend/config/#v2m.config.OllamaConfig","title":"<code>v2m.config.OllamaConfig</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuraci\u00f3n para backend LLM Ollama (SOTA 2026).</p> Atributos <p>host: URL del servidor Ollama. Defecto: http://localhost:11434 model: Nombre del modelo (gemma2:2b, phi3.5-mini, qwen2.5-coder:7b). keep_alive: Tiempo para mantener el modelo cargado. \"0m\" libera VRAM inmediatamente. temperature: Temperatura de generaci\u00f3n. 0.0 para salidas estructuradas determin\u00edsticas. translation_temperature: Temperatura para tareas de traducci\u00f3n.</p> Source code in <code>apps/daemon/backend/src/v2m/config.py</code> <pre><code>class OllamaConfig(BaseModel):\n    \"\"\"Configuraci\u00f3n para backend LLM Ollama (SOTA 2026).\n\n    Atributos:\n        host: URL del servidor Ollama. Defecto: http://localhost:11434\n        model: Nombre del modelo (gemma2:2b, phi3.5-mini, qwen2.5-coder:7b).\n        keep_alive: Tiempo para mantener el modelo cargado. \"0m\" libera VRAM inmediatamente.\n        temperature: Temperatura de generaci\u00f3n. 0.0 para salidas estructuradas determin\u00edsticas.\n        translation_temperature: Temperatura para tareas de traducci\u00f3n.\n    \"\"\"\n\n    host: str = Field(default=\"http://localhost:11434\")\n    model: str = Field(default=\"gemma2:2b\")\n    keep_alive: str = Field(default=\"5m\")\n    temperature: float = Field(default=0.0, ge=0.0, le=2.0)\n    translation_temperature: float = Field(default=0.3, ge=0.0, le=2.0)\n</code></pre>"},{"location":"api/backend/config/#v2m.config.LocalLLMConfig","title":"<code>v2m.config.LocalLLMConfig</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuraci\u00f3n para LLM local usando llama.cpp.</p> Atributos <p>model_path: Ruta al archivo del modelo GGUF. n_gpu_layers: N\u00famero de capas para descargar a la GPU (-1 para todas). n_ctx: Tama\u00f1o de la ventana de contexto. Defecto: 2048 temperature: Temperatura de generaci\u00f3n. Defecto: 0.3 max_tokens: M\u00e1ximo de tokens a generar. Defecto: 512</p> Source code in <code>apps/daemon/backend/src/v2m/config.py</code> <pre><code>class LocalLLMConfig(BaseModel):\n    \"\"\"Configuraci\u00f3n para LLM local usando llama.cpp.\n\n    Atributos:\n        model_path: Ruta al archivo del modelo GGUF.\n        n_gpu_layers: N\u00famero de capas para descargar a la GPU (-1 para todas).\n        n_ctx: Tama\u00f1o de la ventana de contexto. Defecto: 2048\n        temperature: Temperatura de generaci\u00f3n. Defecto: 0.3\n        max_tokens: M\u00e1ximo de tokens a generar. Defecto: 512\n    \"\"\"\n\n    model_path: Path = Field(default=Path(\"models/qwen2.5-3b-instruct-q4_k_m.gguf\"))\n    n_gpu_layers: int = Field(default=-1)\n    n_ctx: int = Field(default=2048, ge=512, le=32768)\n    temperature: float = Field(default=0.3, ge=0.0, le=2.0)\n    translation_temperature: float = Field(default=0.3, ge=0.0, le=2.0)\n    max_tokens: int = Field(default=512, ge=1, le=4096)\n</code></pre>"},{"location":"api/backend/config/#configuracion-de-notificaciones","title":"Configuraci\u00f3n de Notificaciones","text":"<p>options: show_source: false</p>"},{"location":"api/backend/config/#v2m.config.NotificationsConfig","title":"<code>v2m.config.NotificationsConfig</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuraci\u00f3n de notificaciones de escritorio.</p> Atributos <p>expire_time_ms: Tiempo en ms antes del cierre autom\u00e1tico. Defecto: 3000 auto_dismiss: Forzar cierre program\u00e1tico. Defecto: True</p> Source code in <code>apps/daemon/backend/src/v2m/config.py</code> <pre><code>class NotificationsConfig(BaseModel):\n    \"\"\"Configuraci\u00f3n de notificaciones de escritorio.\n\n    Atributos:\n        expire_time_ms: Tiempo en ms antes del cierre autom\u00e1tico. Defecto: 3000\n        auto_dismiss: Forzar cierre program\u00e1tico. Defecto: True\n    \"\"\"\n\n    expire_time_ms: int = Field(default=3000, ge=500, le=30000)\n    auto_dismiss: bool = Field(default=True)\n</code></pre>"},{"location":"api/backend/llm/","title":"Servicios LLM","text":"<p>Proveedores de modelos de lenguaje para procesamiento de texto.</p>"},{"location":"api/backend/llm/#google-gemini-cloud","title":"Google Gemini (Cloud)","text":"<p>Servicio LLM que conecta con la API de Google Gemini para procesamiento de texto y traducciones.</p> <p>Ubicaci\u00f3n: <code>v2m/infrastructure/gemini_llm_service.py</code></p> <p>M\u00e9todos principales:</p> <ul> <li><code>process_text(text: str) -&gt; str</code> - Refina texto con puntuaci\u00f3n y gram\u00e1tica</li> <li><code>translate_text(text: str, target_lang: str) -&gt; str</code> - Traduce texto</li> </ul>"},{"location":"api/backend/llm/#ollama-local","title":"Ollama (Local)","text":"<p>Servicio LLM local que conecta con el servidor Ollama para privacidad total.</p> <p>Ubicaci\u00f3n: <code>v2m/infrastructure/ollama_llm_service.py</code></p> <p>Configuraci\u00f3n: <code>http://localhost:11434</code></p>"},{"location":"api/backend/llm/#local-llamacpp","title":"Local (llama.cpp)","text":"<p>Servicio LLM embebido usando llama-cpp-python directamente.</p> <p>Ubicaci\u00f3n: <code>v2m/infrastructure/local_llm_service.py</code></p>"},{"location":"api/backend/llm/#patron-de-diseno","title":"Patr\u00f3n de Dise\u00f1o","text":"<p>Todos los servicios LLM implementan una interfaz com\u00fan:</p> <pre><code>class LLMService(Protocol):\n    def process_text(self, text: str) -&gt; str:\n        \"\"\"Refina texto con gram\u00e1tica y puntuaci\u00f3n.\"\"\"\n        ...\n\n    def translate_text(self, text: str, target_lang: str) -&gt; str:\n        \"\"\"Traduce texto al idioma especificado.\"\"\"\n        ...\n</code></pre> <p>El <code>Orchestrator</code> selecciona el backend seg\u00fan <code>config.llm.backend</code>:</p> <ul> <li><code>\"gemini\"</code> \u2192 GeminiLLMService</li> <li><code>\"ollama\"</code> \u2192 OllamaLLMService</li> <li><code>\"local\"</code> \u2192 LocalLLMService</li> </ul>"},{"location":"api/backend/orchestrator/","title":"Orchestrator","text":"<p>El Orchestrator es el servicio central que coordina todo el flujo de trabajo de Voice2Machine.</p>"},{"location":"api/backend/orchestrator/#referencia-de-api","title":"Referencia de API","text":"<p>options: show_source: true members: - init - toggle - start - stop - warmup - shutdown - get_status - process_text - translate_text</p>"},{"location":"api/backend/orchestrator/#v2m.services.orchestrator.Orchestrator","title":"<code>v2m.services.orchestrator.Orchestrator</code>","text":"<p>Orquestador central del sistema Voice2Machine.</p> <p>Coordina todos los servicios del sistema (audio, transcripci\u00f3n, LLM, clipboard, notificaciones) usando Lazy Initialization. Reemplaza el patr\u00f3n CQRS + DI Container con un enfoque m\u00e1s directo y f\u00e1cil de depurar.</p> <p>El Orchestrator es el \u00fanico punto de entrada para las operaciones de negocio. FastAPI delega todas las acciones aqu\u00ed, manteniendo los endpoints \"tontos\".</p> <p>Attributes:</p> Name Type Description <code>_is_recording</code> <code>bool</code> <p>Estado actual de grabaci\u00f3n (True si est\u00e1 capturando audio).</p> <code>_model_loaded</code> <code>bool</code> <p>True si el modelo Whisper est\u00e1 pre-cargado en VRAM.</p> <code>_broadcast_fn</code> <p>Funci\u00f3n opcional para emitir eventos a WebSocket clients.</p> Note <p>Los servicios pesados (Whisper, LLM) se crean cuando se necesitan por primera vez, no al inicio. Esto permite que el servidor FastAPI arranque en ~100ms. Ver: docs/adr/002-orchestrator-pattern.md</p> Example <p>async def main(): ...     orch = Orchestrator() ...     await orch.warmup()  # Pre-cargar Whisper en VRAM ...     response = await orch.toggle() ...     print(response.text)</p> Source code in <code>apps/daemon/backend/src/v2m/services/orchestrator.py</code> <pre><code>class Orchestrator:\n    \"\"\"Orquestador central del sistema Voice2Machine.\n\n    Coordina todos los servicios del sistema (audio, transcripci\u00f3n, LLM, clipboard,\n    notificaciones) usando Lazy Initialization. Reemplaza el patr\u00f3n CQRS + DI Container\n    con un enfoque m\u00e1s directo y f\u00e1cil de depurar.\n\n    El Orchestrator es el \u00fanico punto de entrada para las operaciones de negocio.\n    FastAPI delega todas las acciones aqu\u00ed, manteniendo los endpoints \"tontos\".\n\n    Attributes:\n        _is_recording: Estado actual de grabaci\u00f3n (True si est\u00e1 capturando audio).\n        _model_loaded: True si el modelo Whisper est\u00e1 pre-cargado en VRAM.\n        _broadcast_fn: Funci\u00f3n opcional para emitir eventos a WebSocket clients.\n\n    Note:\n        Los servicios pesados (Whisper, LLM) se crean cuando se necesitan por primera\n        vez, no al inicio. Esto permite que el servidor FastAPI arranque en ~100ms.\n        Ver: docs/adr/002-orchestrator-pattern.md\n\n    Example:\n        &gt;&gt;&gt; async def main():\n        ...     orch = Orchestrator()\n        ...     await orch.warmup()  # Pre-cargar Whisper en VRAM\n        ...     response = await orch.toggle()\n        ...     print(response.text)\n    \"\"\"\n\n    def __init__(self, broadcast_fn: BroadcastFn | None = None) -&gt; None:\n        \"\"\"Inicializa el orquestador con estado limpio y servicios diferidos.\n\n        Args:\n            broadcast_fn: Funci\u00f3n opcional para enviar eventos en tiempo real\n                a clientes WebSocket. Usada para transcripci\u00f3n provisional.\n\n        Note:\n            Los servicios reales (WhisperWorker, AudioRecorder, etc.) no se\n            crean aqu\u00ed. Se instancian en su primera llamada (Lazy Init).\n        \"\"\"\n        # Estado interno\n        self._is_recording: bool = False\n        self._model_loaded: bool = False\n        self._broadcast_fn = broadcast_fn\n\n        # Servicios con lazy initialization (None hasta que se usen)\n        self._worker: PersistentWhisperWorker | None = None\n        self._recorder: AudioRecorder | None = None\n        self._transcriber: StreamingTranscriber | None = None\n        self._clipboard: LinuxClipboardAdapter | None = None\n        self._notifications: LinuxNotificationService | None = None\n        self._llm_service: Any | None = None  # Tipo din\u00e1mico seg\u00fan backend\n\n    # =========================================================================\n    # Propiedades con Lazy Initialization\n    # =========================================================================\n\n    @property\n    def worker(self) -&gt; PersistentWhisperWorker:\n        \"\"\"Obtiene el worker de Whisper, cre\u00e1ndolo si no existe.\n\n        El worker gestiona el modelo Whisper en VRAM, soportando pol\u00edticas\n        de keep-warm para eliminar latencia de carga en fr\u00edo.\n\n        Returns:\n            PersistentWhisperWorker: Instancia configurada seg\u00fan config.toml.\n\n        Note:\n            La primera llamada puede tardar varios segundos mientras carga\n            el modelo en GPU. Usa warmup() en startup para evitar esto.\n        \"\"\"\n        if self._worker is None:\n            from v2m.infrastructure.persistent_model import PersistentWhisperWorker\n\n            whisper_cfg = config.transcription.whisper\n            self._worker = PersistentWhisperWorker(\n                model_size=whisper_cfg.model,\n                device=whisper_cfg.device,\n                compute_type=whisper_cfg.compute_type,\n                device_index=whisper_cfg.device_index,\n                num_workers=whisper_cfg.num_workers,\n                keep_warm=whisper_cfg.keep_warm,\n            )\n        return self._worker\n\n    @property\n    def recorder(self) -&gt; AudioRecorder:\n        \"\"\"Obtiene el grabador de audio, cre\u00e1ndolo si no existe.\n\n        Utiliza la extensi\u00f3n Rust (v2m_engine) para captura de baja latencia.\n        Ver: docs/adr/005-rust-audio-engine.md\n\n        Returns:\n            AudioRecorder: Instancia configurada para 16kHz mono (requerido por Whisper).\n        \"\"\"\n        if self._recorder is None:\n            from v2m.infrastructure.audio.recorder import AudioRecorder\n\n            whisper_cfg = config.transcription.whisper\n            self._recorder = AudioRecorder(\n                sample_rate=16000,\n                channels=1,\n                device_index=whisper_cfg.audio_device_index,\n            )\n        return self._recorder\n\n    @property\n    def transcriber(self) -&gt; StreamingTranscriber:\n        \"\"\"Obtiene el transcriptor streaming, cre\u00e1ndolo si no existe.\n\n        Conecta AudioRecorder \u2192 WhisperWorker \u2192 WebSocket broadcast para\n        proporcionar transcripci\u00f3n provisional en tiempo real.\n\n        Returns:\n            StreamingTranscriber: Instancia conectada al worker y recorder.\n        \"\"\"\n        if self._transcriber is None:\n            from v2m.infrastructure.streaming_transcriber import StreamingTranscriber\n\n            # Adapter conecta eventos del transcriptor \u2192 WebSocket broadcast\n            session_adapter = WebSocketSessionAdapter(self._broadcast_fn)\n\n            self._transcriber = StreamingTranscriber(\n                worker=self.worker,\n                session_manager=session_adapter,\n                recorder=self.recorder,\n            )\n        return self._transcriber\n\n    @property\n    def clipboard(self) -&gt; LinuxClipboardAdapter:\n        \"\"\"Obtiene el servicio de clipboard del sistema.\n\n        Returns:\n            LinuxClipboardAdapter: Wrapper sobre xclip/wl-copy seg\u00fan el entorno.\n        \"\"\"\n        if self._clipboard is None:\n            from v2m.infrastructure.linux_adapters import LinuxClipboardAdapter\n\n            self._clipboard = LinuxClipboardAdapter()\n        return self._clipboard\n\n    @property\n    def notifications(self) -&gt; LinuxNotificationService:\n        \"\"\"Obtiene el servicio de notificaciones de escritorio.\n\n        Returns:\n            LinuxNotificationService: Wrapper sobre notify-send/libnotify.\n        \"\"\"\n        if self._notifications is None:\n            from v2m.infrastructure.notification_service import LinuxNotificationService\n\n            self._notifications = LinuxNotificationService()\n        return self._notifications\n\n    @property\n    def llm_service(self) -&gt; Any:\n        \"\"\"Obtiene el servicio LLM configurado, cre\u00e1ndolo si no existe.\n\n        Selecciona el backend seg\u00fan config.llm.backend:\n        - \"gemini\": Google Gemini API (cloud)\n        - \"ollama\": Ollama local\n        - \"local\": Modelo embebido\n\n        Returns:\n            LLMService: Instancia del proveedor configurado.\n\n        Note:\n            El tipo de retorno es Any porque los backends tienen interfaces\n            similares pero no id\u00e9nticas. En producci\u00f3n, todos implementan\n            process_text(str) -&gt; str y translate_text(str, str) -&gt; str.\n        \"\"\"\n        if self._llm_service is None:\n            backend = config.llm.backend\n\n            if backend == \"gemini\":\n                from v2m.infrastructure.gemini_llm_service import GeminiLLMService\n\n                self._llm_service = GeminiLLMService()\n            elif backend == \"ollama\":\n                from v2m.infrastructure.ollama_llm_service import OllamaLLMService\n\n                self._llm_service = OllamaLLMService()\n            else:  # \"local\"\n                from v2m.infrastructure.local_llm_service import LocalLLMService\n\n                self._llm_service = LocalLLMService()\n\n            logger.info(f\"LLM backend inicializado: {backend}\")\n        return self._llm_service\n\n    # =========================================================================\n    # M\u00e9todos P\u00fablicos (API Surface)\n    # =========================================================================\n\n    async def warmup(self) -&gt; None:\n        \"\"\"Pre-carga el modelo Whisper en VRAM para eliminar latencia de arranque.\n\n        Ejecuta la inicializaci\u00f3n sincr\u00f3nica del worker en un executor para no\n        bloquear el event loop. Debe llamarse durante el lifecycle startup de\n        FastAPI para tener el modelo \"caliente\" antes de la primera transcripci\u00f3n.\n\n        Raises:\n            RuntimeError: Si el worker no puede inicializarse (error silenciado\n                en logs, el modelo se cargar\u00e1 on-demand si falla aqu\u00ed).\n\n        Example:\n            &gt;&gt;&gt; @asynccontextmanager\n            ... async def lifespan(app: FastAPI):\n            ...     await orchestrator.warmup()\n            ...     yield\n        \"\"\"\n        if self._model_loaded:\n            return\n\n        try:\n            # Warmup sincr\u00f3nico en el executor del worker\n            loop = asyncio.get_running_loop()\n            await loop.run_in_executor(None, self.worker.initialize_sync)\n            self._model_loaded = True\n            logger.info(\"\u2705 Modelo Whisper precargado en VRAM\")\n        except Exception as e:\n            logger.error(f\"\u274c Error en warmup del modelo: {e}\")\n            # No re-lanzamos - el modelo se cargar\u00e1 on-demand\n\n    async def toggle(self) -&gt; ToggleResponse:\n        \"\"\"Alterna el estado de grabaci\u00f3n del sistema.\n\n        Gestiona la l\u00f3gica de cambio de estado: si el sistema est\u00e1 en reposo\n        (idle), inicia la captura de audio y el streaming. Si est\u00e1 grabando,\n        detiene la captura, finaliza la transcripci\u00f3n y copia al portapapeles.\n\n        Este es el endpoint principal usado por los atajos de teclado.\n\n        Returns:\n            ToggleResponse: Objeto con el nuevo estado del sistema y, en caso\n                de detenerse, el texto final transcrito.\n\n        Raises:\n            RuntimeError: Si los servicios de backend (Whisper/Audio) no responden.\n                (Error capturado internamente y reportado en ToggleResponse.message)\n\n        Example:\n            &gt;&gt;&gt; response = await orchestrator.toggle()\n            &gt;&gt;&gt; if response.status == \"idle\" and response.text:\n            ...     print(f\"Transcrito: {response.text}\")\n        \"\"\"\n        if not self._is_recording:\n            return await self.start()\n        else:\n            return await self.stop()\n\n    async def start(self) -&gt; ToggleResponse:\n        \"\"\"Inicia la grabaci\u00f3n de audio y el streaming de transcripci\u00f3n.\n\n        Activa el AudioRecorder y el StreamingTranscriber. Crea un archivo\n        flag en disco para que scripts externos puedan detectar el estado.\n\n        Returns:\n            ToggleResponse: Confirmaci\u00f3n de inicio con status='recording'.\n\n        Raises:\n            RuntimeError: Si ya hay una grabaci\u00f3n en curso o el micr\u00f3fono\n                no est\u00e1 disponible. (Error capturado en response.message)\n\n        Note:\n            Si ya est\u00e1 grabando, retorna inmediatamente sin error.\n        \"\"\"\n        from v2m.api import ToggleResponse\n\n        if self._is_recording:\n            return ToggleResponse(\n                status=\"recording\",\n                message=\"\u26a0\ufe0f Ya est\u00e1 grabando\",\n            )\n\n        try:\n            # Iniciar streaming transcriber\n            await self.transcriber.start()\n            self._is_recording = True\n\n            # Crear flag file para scripts externos\n            config.paths.recording_flag.touch()\n\n            # Notificar al usuario\n            self.notifications.notify(\"\ud83c\udfa4 voice2machine\", \"grabaci\u00f3n iniciada...\")\n\n            logger.info(\"\ud83c\udf99\ufe0f Grabaci\u00f3n iniciada\")\n            return ToggleResponse(\n                status=\"recording\",\n                message=\"\ud83c\udf99\ufe0f Grabando...\",\n            )\n\n        except Exception as e:\n            logger.error(f\"Error iniciando grabaci\u00f3n: {e}\")\n            return ToggleResponse(\n                status=\"error\",\n                message=f\"\u274c Error: {e}\",\n            )\n\n    async def stop(self) -&gt; ToggleResponse:\n        \"\"\"Detiene la grabaci\u00f3n, finaliza transcripci\u00f3n y copia al portapapeles.\n\n        Detiene el AudioRecorder, procesa el audio capturado con Whisper,\n        y copia el texto resultante al portapapeles del sistema.\n\n        Returns:\n            ToggleResponse: Resultado con status='idle' y el texto transcrito\n                en el campo 'text'. Si no se detect\u00f3 voz, text=None.\n\n        Raises:\n            RuntimeError: Si no hay grabaci\u00f3n en curso o Whisper falla.\n                (Error capturado en response.message)\n\n        Example:\n            &gt;&gt;&gt; response = await orchestrator.stop()\n            &gt;&gt;&gt; if response.text:\n            ...     print(f\"Copiado: {response.text}\")\n        \"\"\"\n        from v2m.api import ToggleResponse\n\n        if not self._is_recording:\n            return ToggleResponse(\n                status=\"idle\",\n                message=\"\u26a0\ufe0f No hay grabaci\u00f3n en curso\",\n            )\n\n        try:\n            self._is_recording = False\n\n            # Eliminar flag file\n            if config.paths.recording_flag.exists():\n                config.paths.recording_flag.unlink()\n\n            # Notificar procesamiento\n            self.notifications.notify(\"\u26a1 v2m procesando\", \"procesando...\")\n\n            # Detener y obtener transcripci\u00f3n\n            transcription = await self.transcriber.stop()\n\n            # Validar resultado\n            if not transcription or not transcription.strip():\n                self.notifications.notify(\"\u274c whisper\", \"no se detect\u00f3 voz en el audio\")\n                return ToggleResponse(\n                    status=\"idle\",\n                    message=\"\u274c No se detect\u00f3 voz\",\n                    text=None,\n                )\n\n            # Copiar al portapapeles\n            self.clipboard.copy(transcription)\n\n            # Notificar \u00e9xito\n            preview = transcription[:80]\n            self.notifications.notify(\"\u2705 whisper - copiado\", f\"{preview}...\")\n\n            logger.info(f\"\u2705 Transcripci\u00f3n completada: {len(transcription)} chars\")\n            return ToggleResponse(\n                status=\"idle\",\n                message=\"\u2705 Copiado al portapapeles\",\n                text=transcription,\n            )\n\n        except Exception as e:\n            logger.error(f\"Error deteniendo grabaci\u00f3n: {e}\")\n            self._is_recording = False\n            return ToggleResponse(\n                status=\"error\",\n                message=f\"\u274c Error: {e}\",\n            )\n\n    async def process_text(self, text: str) -&gt; LLMResponse:\n        \"\"\"Procesa texto con LLM para limpieza, puntuaci\u00f3n y formato.\n\n        Env\u00eda el texto al backend LLM configurado (Gemini/Ollama/local) para\n        refinamiento. El resultado se copia autom\u00e1ticamente al portapapeles.\n\n        Args:\n            text: Texto crudo a procesar (t\u00edpicamente output de Whisper).\n\n        Returns:\n            LLMResponse: Texto refinado y nombre del backend usado.\n                Si el LLM falla, retorna el texto original como fallback.\n\n        Raises:\n            ValueError: Si el texto est\u00e1 vac\u00edo (capturado internamente).\n\n        Example:\n            &gt;&gt;&gt; response = await orchestrator.process_text(\"hola como estas\")\n            &gt;&gt;&gt; print(response.text)  # \"Hola, \u00bfc\u00f3mo est\u00e1s?\"\n        \"\"\"\n        from v2m.api import LLMResponse\n\n        backend_name = config.llm.backend\n\n        try:\n            # El servicio LLM puede ser sync o async\n            if asyncio.iscoroutinefunction(self.llm_service.process_text):\n                refined = await self.llm_service.process_text(text)\n            else:\n                refined = await asyncio.to_thread(self.llm_service.process_text, text)\n\n            # Copiar al portapapeles\n            self.clipboard.copy(refined)\n            self.notifications.notify(f\"\u2705 {backend_name} - copiado\", f\"{refined[:80]}...\")\n\n            return LLMResponse(text=refined, backend=backend_name)\n\n        except Exception as e:\n            logger.error(f\"Error procesando texto con {backend_name}: {e}\")\n            # Fallback: copiar texto original\n            self.clipboard.copy(text)\n            self.notifications.notify(f\"\u26a0\ufe0f {backend_name} fall\u00f3\", \"usando texto original...\")\n            return LLMResponse(text=text, backend=f\"{backend_name} (fallback)\")\n\n    async def translate_text(self, text: str, target_lang: str) -&gt; LLMResponse:\n        r\"\"\"Traduce texto a otro idioma usando el LLM configurado.\n\n        Valida el idioma destino para prevenir inyecci\u00f3n de prompts maliciosos,\n        luego delega al backend LLM para traducci\u00f3n.\n\n        Args:\n            text: Texto a traducir en cualquier idioma.\n            target_lang: C\u00f3digo o nombre del idioma destino (ej. 'en', 'espa\u00f1ol').\n                Debe coincidir con regex: ^[a-zA-Z\\s\\-]{2,20}$\n\n        Returns:\n            LLMResponse: Texto traducido y backend usado.\n                Si hay error, retorna texto original con backend='error'.\n\n        Raises:\n            ValueError: Si target_lang tiene caracteres inv\u00e1lidos (sanitizado).\n\n        Example:\n            &gt;&gt;&gt; response = await orchestrator.translate_text(\"Buenos d\u00edas\", \"en\")\n            &gt;&gt;&gt; print(response.text)  # \"Good morning\"\n        \"\"\"\n        from v2m.api import LLMResponse\n\n        backend_name = config.llm.backend\n\n        # Validar target_lang para prevenir inyecci\u00f3n\n        if not re.match(r\"^[a-zA-Z\\s\\-]{2,20}$\", target_lang):\n            logger.warning(f\"Idioma inv\u00e1lido: {target_lang}\")\n            self.notifications.notify(\"\u274c Error\", \"Idioma de destino inv\u00e1lido\")\n            return LLMResponse(text=text, backend=\"error\")\n\n        try:\n            if asyncio.iscoroutinefunction(self.llm_service.translate_text):\n                translated = await self.llm_service.translate_text(text, target_lang)\n            else:\n                translated = await asyncio.to_thread(self.llm_service.translate_text, text, target_lang)\n\n            self.clipboard.copy(translated)\n            self.notifications.notify(f\"\u2705 Traducci\u00f3n ({target_lang})\", f\"{translated[:80]}...\")\n\n            return LLMResponse(text=translated, backend=backend_name)\n\n        except Exception as e:\n            logger.error(f\"Error traduciendo con {backend_name}: {e}\")\n            self.notifications.notify(\"\u274c Error traducci\u00f3n\", \"Fallo al traducir\")\n            return LLMResponse(text=text, backend=f\"{backend_name} (error)\")\n\n    def get_status(self) -&gt; StatusResponse:\n        \"\"\"Retorna el estado actual del daemon de forma sincr\u00f3nica.\n\n        M\u00e9todo ligero sin I/O, \u00fatil para health checks y polling.\n\n        Returns:\n            StatusResponse: Estado de grabaci\u00f3n ('idle'|'recording'),\n                flag booleano de grabaci\u00f3n, y si el modelo est\u00e1 cargado.\n\n        Example:\n            &gt;&gt;&gt; status = orchestrator.get_status()\n            &gt;&gt;&gt; if status.model_loaded:\n            ...     print(\"Whisper listo\")\n        \"\"\"\n        from v2m.api import StatusResponse\n\n        state = \"recording\" if self._is_recording else \"idle\"\n\n        return StatusResponse(\n            state=state,\n            recording=self._is_recording,\n            model_loaded=self._model_loaded,\n        )\n\n    async def shutdown(self) -&gt; None:\n        \"\"\"Libera todos los recursos al apagar el servidor.\n\n        Detiene grabaci\u00f3n activa, descarga el modelo de VRAM, y cierra\n        servicios de notificaciones. Llamado autom\u00e1ticamente en el\n        lifecycle shutdown de FastAPI.\n\n        Note:\n            Los errores se logean pero no se re-lanzan para asegurar\n            un apagado limpio incluso con fallos parciales.\n\n        Example:\n            &gt;&gt;&gt; @asynccontextmanager\n            ... async def lifespan(app: FastAPI):\n            ...     await orchestrator.warmup()\n            ...     yield\n            ...     await orchestrator.shutdown()\n        \"\"\"\n        logger.info(\"Liberando recursos del orquestador...\")\n\n        # Detener grabaci\u00f3n si est\u00e1 activa\n        if self._is_recording:\n            with contextlib.suppress(Exception):\n                await self.stop()\n\n        # Descargar modelo de VRAM\n        if self._worker:\n            try:\n                await self._worker.unload()\n            except Exception as e:\n                logger.warning(f\"Error descargando modelo: {e}\")\n\n        # Cerrar servicio de notificaciones\n        if self._notifications:\n            self._notifications.shutdown(wait=False)\n\n        logger.info(\"\u2705 Recursos liberados\")\n</code></pre>"},{"location":"api/backend/orchestrator/#v2m.services.orchestrator.Orchestrator.clipboard","title":"<code>clipboard</code>  <code>property</code>","text":"<p>Obtiene el servicio de clipboard del sistema.</p> <p>Returns:</p> Name Type Description <code>LinuxClipboardAdapter</code> <code>LinuxClipboardAdapter</code> <p>Wrapper sobre xclip/wl-copy seg\u00fan el entorno.</p>"},{"location":"api/backend/orchestrator/#v2m.services.orchestrator.Orchestrator.llm_service","title":"<code>llm_service</code>  <code>property</code>","text":"<p>Obtiene el servicio LLM configurado, cre\u00e1ndolo si no existe.</p> <p>Selecciona el backend seg\u00fan config.llm.backend: - \"gemini\": Google Gemini API (cloud) - \"ollama\": Ollama local - \"local\": Modelo embebido</p> <p>Returns:</p> Name Type Description <code>LLMService</code> <code>Any</code> <p>Instancia del proveedor configurado.</p> Note <p>El tipo de retorno es Any porque los backends tienen interfaces similares pero no id\u00e9nticas. En producci\u00f3n, todos implementan process_text(str) -&gt; str y translate_text(str, str) -&gt; str.</p>"},{"location":"api/backend/orchestrator/#v2m.services.orchestrator.Orchestrator.notifications","title":"<code>notifications</code>  <code>property</code>","text":"<p>Obtiene el servicio de notificaciones de escritorio.</p> <p>Returns:</p> Name Type Description <code>LinuxNotificationService</code> <code>LinuxNotificationService</code> <p>Wrapper sobre notify-send/libnotify.</p>"},{"location":"api/backend/orchestrator/#v2m.services.orchestrator.Orchestrator.recorder","title":"<code>recorder</code>  <code>property</code>","text":"<p>Obtiene el grabador de audio, cre\u00e1ndolo si no existe.</p> <p>Utiliza la extensi\u00f3n Rust (v2m_engine) para captura de baja latencia. Ver: docs/adr/005-rust-audio-engine.md</p> <p>Returns:</p> Name Type Description <code>AudioRecorder</code> <code>AudioRecorder</code> <p>Instancia configurada para 16kHz mono (requerido por Whisper).</p>"},{"location":"api/backend/orchestrator/#v2m.services.orchestrator.Orchestrator.transcriber","title":"<code>transcriber</code>  <code>property</code>","text":"<p>Obtiene el transcriptor streaming, cre\u00e1ndolo si no existe.</p> <p>Conecta AudioRecorder \u2192 WhisperWorker \u2192 WebSocket broadcast para proporcionar transcripci\u00f3n provisional en tiempo real.</p> <p>Returns:</p> Name Type Description <code>StreamingTranscriber</code> <code>StreamingTranscriber</code> <p>Instancia conectada al worker y recorder.</p>"},{"location":"api/backend/orchestrator/#v2m.services.orchestrator.Orchestrator.worker","title":"<code>worker</code>  <code>property</code>","text":"<p>Obtiene el worker de Whisper, cre\u00e1ndolo si no existe.</p> <p>El worker gestiona el modelo Whisper en VRAM, soportando pol\u00edticas de keep-warm para eliminar latencia de carga en fr\u00edo.</p> <p>Returns:</p> Name Type Description <code>PersistentWhisperWorker</code> <code>PersistentWhisperWorker</code> <p>Instancia configurada seg\u00fan config.toml.</p> Note <p>La primera llamada puede tardar varios segundos mientras carga el modelo en GPU. Usa warmup() en startup para evitar esto.</p>"},{"location":"api/backend/orchestrator/#v2m.services.orchestrator.Orchestrator.__init__","title":"<code>__init__(broadcast_fn=None)</code>","text":"<p>Inicializa el orquestador con estado limpio y servicios diferidos.</p> <p>Parameters:</p> Name Type Description Default <code>broadcast_fn</code> <code>BroadcastFn | None</code> <p>Funci\u00f3n opcional para enviar eventos en tiempo real a clientes WebSocket. Usada para transcripci\u00f3n provisional.</p> <code>None</code> Note <p>Los servicios reales (WhisperWorker, AudioRecorder, etc.) no se crean aqu\u00ed. Se instancian en su primera llamada (Lazy Init).</p> Source code in <code>apps/daemon/backend/src/v2m/services/orchestrator.py</code> <pre><code>def __init__(self, broadcast_fn: BroadcastFn | None = None) -&gt; None:\n    \"\"\"Inicializa el orquestador con estado limpio y servicios diferidos.\n\n    Args:\n        broadcast_fn: Funci\u00f3n opcional para enviar eventos en tiempo real\n            a clientes WebSocket. Usada para transcripci\u00f3n provisional.\n\n    Note:\n        Los servicios reales (WhisperWorker, AudioRecorder, etc.) no se\n        crean aqu\u00ed. Se instancian en su primera llamada (Lazy Init).\n    \"\"\"\n    # Estado interno\n    self._is_recording: bool = False\n    self._model_loaded: bool = False\n    self._broadcast_fn = broadcast_fn\n\n    # Servicios con lazy initialization (None hasta que se usen)\n    self._worker: PersistentWhisperWorker | None = None\n    self._recorder: AudioRecorder | None = None\n    self._transcriber: StreamingTranscriber | None = None\n    self._clipboard: LinuxClipboardAdapter | None = None\n    self._notifications: LinuxNotificationService | None = None\n    self._llm_service: Any | None = None  # Tipo din\u00e1mico seg\u00fan backend\n</code></pre>"},{"location":"api/backend/orchestrator/#v2m.services.orchestrator.Orchestrator.get_status","title":"<code>get_status()</code>","text":"<p>Retorna el estado actual del daemon de forma sincr\u00f3nica.</p> <p>M\u00e9todo ligero sin I/O, \u00fatil para health checks y polling.</p> <p>Returns:</p> Name Type Description <code>StatusResponse</code> <code>StatusResponse</code> <p>Estado de grabaci\u00f3n ('idle'|'recording'), flag booleano de grabaci\u00f3n, y si el modelo est\u00e1 cargado.</p> Example <p>status = orchestrator.get_status() if status.model_loaded: ...     print(\"Whisper listo\")</p> Source code in <code>apps/daemon/backend/src/v2m/services/orchestrator.py</code> <pre><code>def get_status(self) -&gt; StatusResponse:\n    \"\"\"Retorna el estado actual del daemon de forma sincr\u00f3nica.\n\n    M\u00e9todo ligero sin I/O, \u00fatil para health checks y polling.\n\n    Returns:\n        StatusResponse: Estado de grabaci\u00f3n ('idle'|'recording'),\n            flag booleano de grabaci\u00f3n, y si el modelo est\u00e1 cargado.\n\n    Example:\n        &gt;&gt;&gt; status = orchestrator.get_status()\n        &gt;&gt;&gt; if status.model_loaded:\n        ...     print(\"Whisper listo\")\n    \"\"\"\n    from v2m.api import StatusResponse\n\n    state = \"recording\" if self._is_recording else \"idle\"\n\n    return StatusResponse(\n        state=state,\n        recording=self._is_recording,\n        model_loaded=self._model_loaded,\n    )\n</code></pre>"},{"location":"api/backend/orchestrator/#v2m.services.orchestrator.Orchestrator.process_text","title":"<code>process_text(text)</code>  <code>async</code>","text":"<p>Procesa texto con LLM para limpieza, puntuaci\u00f3n y formato.</p> <p>Env\u00eda el texto al backend LLM configurado (Gemini/Ollama/local) para refinamiento. El resultado se copia autom\u00e1ticamente al portapapeles.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>Texto crudo a procesar (t\u00edpicamente output de Whisper).</p> required <p>Returns:</p> Name Type Description <code>LLMResponse</code> <code>LLMResponse</code> <p>Texto refinado y nombre del backend usado. Si el LLM falla, retorna el texto original como fallback.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>Si el texto est\u00e1 vac\u00edo (capturado internamente).</p> Example <p>response = await orchestrator.process_text(\"hola como estas\") print(response.text)  # \"Hola, \u00bfc\u00f3mo est\u00e1s?\"</p> Source code in <code>apps/daemon/backend/src/v2m/services/orchestrator.py</code> <pre><code>async def process_text(self, text: str) -&gt; LLMResponse:\n    \"\"\"Procesa texto con LLM para limpieza, puntuaci\u00f3n y formato.\n\n    Env\u00eda el texto al backend LLM configurado (Gemini/Ollama/local) para\n    refinamiento. El resultado se copia autom\u00e1ticamente al portapapeles.\n\n    Args:\n        text: Texto crudo a procesar (t\u00edpicamente output de Whisper).\n\n    Returns:\n        LLMResponse: Texto refinado y nombre del backend usado.\n            Si el LLM falla, retorna el texto original como fallback.\n\n    Raises:\n        ValueError: Si el texto est\u00e1 vac\u00edo (capturado internamente).\n\n    Example:\n        &gt;&gt;&gt; response = await orchestrator.process_text(\"hola como estas\")\n        &gt;&gt;&gt; print(response.text)  # \"Hola, \u00bfc\u00f3mo est\u00e1s?\"\n    \"\"\"\n    from v2m.api import LLMResponse\n\n    backend_name = config.llm.backend\n\n    try:\n        # El servicio LLM puede ser sync o async\n        if asyncio.iscoroutinefunction(self.llm_service.process_text):\n            refined = await self.llm_service.process_text(text)\n        else:\n            refined = await asyncio.to_thread(self.llm_service.process_text, text)\n\n        # Copiar al portapapeles\n        self.clipboard.copy(refined)\n        self.notifications.notify(f\"\u2705 {backend_name} - copiado\", f\"{refined[:80]}...\")\n\n        return LLMResponse(text=refined, backend=backend_name)\n\n    except Exception as e:\n        logger.error(f\"Error procesando texto con {backend_name}: {e}\")\n        # Fallback: copiar texto original\n        self.clipboard.copy(text)\n        self.notifications.notify(f\"\u26a0\ufe0f {backend_name} fall\u00f3\", \"usando texto original...\")\n        return LLMResponse(text=text, backend=f\"{backend_name} (fallback)\")\n</code></pre>"},{"location":"api/backend/orchestrator/#v2m.services.orchestrator.Orchestrator.shutdown","title":"<code>shutdown()</code>  <code>async</code>","text":"<p>Libera todos los recursos al apagar el servidor.</p> <p>Detiene grabaci\u00f3n activa, descarga el modelo de VRAM, y cierra servicios de notificaciones. Llamado autom\u00e1ticamente en el lifecycle shutdown de FastAPI.</p> Note <p>Los errores se logean pero no se re-lanzan para asegurar un apagado limpio incluso con fallos parciales.</p> Example <p>@asynccontextmanager ... async def lifespan(app: FastAPI): ...     await orchestrator.warmup() ...     yield ...     await orchestrator.shutdown()</p> Source code in <code>apps/daemon/backend/src/v2m/services/orchestrator.py</code> <pre><code>async def shutdown(self) -&gt; None:\n    \"\"\"Libera todos los recursos al apagar el servidor.\n\n    Detiene grabaci\u00f3n activa, descarga el modelo de VRAM, y cierra\n    servicios de notificaciones. Llamado autom\u00e1ticamente en el\n    lifecycle shutdown de FastAPI.\n\n    Note:\n        Los errores se logean pero no se re-lanzan para asegurar\n        un apagado limpio incluso con fallos parciales.\n\n    Example:\n        &gt;&gt;&gt; @asynccontextmanager\n        ... async def lifespan(app: FastAPI):\n        ...     await orchestrator.warmup()\n        ...     yield\n        ...     await orchestrator.shutdown()\n    \"\"\"\n    logger.info(\"Liberando recursos del orquestador...\")\n\n    # Detener grabaci\u00f3n si est\u00e1 activa\n    if self._is_recording:\n        with contextlib.suppress(Exception):\n            await self.stop()\n\n    # Descargar modelo de VRAM\n    if self._worker:\n        try:\n            await self._worker.unload()\n        except Exception as e:\n            logger.warning(f\"Error descargando modelo: {e}\")\n\n    # Cerrar servicio de notificaciones\n    if self._notifications:\n        self._notifications.shutdown(wait=False)\n\n    logger.info(\"\u2705 Recursos liberados\")\n</code></pre>"},{"location":"api/backend/orchestrator/#v2m.services.orchestrator.Orchestrator.start","title":"<code>start()</code>  <code>async</code>","text":"<p>Inicia la grabaci\u00f3n de audio y el streaming de transcripci\u00f3n.</p> <p>Activa el AudioRecorder y el StreamingTranscriber. Crea un archivo flag en disco para que scripts externos puedan detectar el estado.</p> <p>Returns:</p> Name Type Description <code>ToggleResponse</code> <code>ToggleResponse</code> <p>Confirmaci\u00f3n de inicio con status='recording'.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>Si ya hay una grabaci\u00f3n en curso o el micr\u00f3fono no est\u00e1 disponible. (Error capturado en response.message)</p> Note <p>Si ya est\u00e1 grabando, retorna inmediatamente sin error.</p> Source code in <code>apps/daemon/backend/src/v2m/services/orchestrator.py</code> <pre><code>async def start(self) -&gt; ToggleResponse:\n    \"\"\"Inicia la grabaci\u00f3n de audio y el streaming de transcripci\u00f3n.\n\n    Activa el AudioRecorder y el StreamingTranscriber. Crea un archivo\n    flag en disco para que scripts externos puedan detectar el estado.\n\n    Returns:\n        ToggleResponse: Confirmaci\u00f3n de inicio con status='recording'.\n\n    Raises:\n        RuntimeError: Si ya hay una grabaci\u00f3n en curso o el micr\u00f3fono\n            no est\u00e1 disponible. (Error capturado en response.message)\n\n    Note:\n        Si ya est\u00e1 grabando, retorna inmediatamente sin error.\n    \"\"\"\n    from v2m.api import ToggleResponse\n\n    if self._is_recording:\n        return ToggleResponse(\n            status=\"recording\",\n            message=\"\u26a0\ufe0f Ya est\u00e1 grabando\",\n        )\n\n    try:\n        # Iniciar streaming transcriber\n        await self.transcriber.start()\n        self._is_recording = True\n\n        # Crear flag file para scripts externos\n        config.paths.recording_flag.touch()\n\n        # Notificar al usuario\n        self.notifications.notify(\"\ud83c\udfa4 voice2machine\", \"grabaci\u00f3n iniciada...\")\n\n        logger.info(\"\ud83c\udf99\ufe0f Grabaci\u00f3n iniciada\")\n        return ToggleResponse(\n            status=\"recording\",\n            message=\"\ud83c\udf99\ufe0f Grabando...\",\n        )\n\n    except Exception as e:\n        logger.error(f\"Error iniciando grabaci\u00f3n: {e}\")\n        return ToggleResponse(\n            status=\"error\",\n            message=f\"\u274c Error: {e}\",\n        )\n</code></pre>"},{"location":"api/backend/orchestrator/#v2m.services.orchestrator.Orchestrator.stop","title":"<code>stop()</code>  <code>async</code>","text":"<p>Detiene la grabaci\u00f3n, finaliza transcripci\u00f3n y copia al portapapeles.</p> <p>Detiene el AudioRecorder, procesa el audio capturado con Whisper, y copia el texto resultante al portapapeles del sistema.</p> <p>Returns:</p> Name Type Description <code>ToggleResponse</code> <code>ToggleResponse</code> <p>Resultado con status='idle' y el texto transcrito en el campo 'text'. Si no se detect\u00f3 voz, text=None.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>Si no hay grabaci\u00f3n en curso o Whisper falla. (Error capturado en response.message)</p> Example <p>response = await orchestrator.stop() if response.text: ...     print(f\"Copiado: {response.text}\")</p> Source code in <code>apps/daemon/backend/src/v2m/services/orchestrator.py</code> <pre><code>async def stop(self) -&gt; ToggleResponse:\n    \"\"\"Detiene la grabaci\u00f3n, finaliza transcripci\u00f3n y copia al portapapeles.\n\n    Detiene el AudioRecorder, procesa el audio capturado con Whisper,\n    y copia el texto resultante al portapapeles del sistema.\n\n    Returns:\n        ToggleResponse: Resultado con status='idle' y el texto transcrito\n            en el campo 'text'. Si no se detect\u00f3 voz, text=None.\n\n    Raises:\n        RuntimeError: Si no hay grabaci\u00f3n en curso o Whisper falla.\n            (Error capturado en response.message)\n\n    Example:\n        &gt;&gt;&gt; response = await orchestrator.stop()\n        &gt;&gt;&gt; if response.text:\n        ...     print(f\"Copiado: {response.text}\")\n    \"\"\"\n    from v2m.api import ToggleResponse\n\n    if not self._is_recording:\n        return ToggleResponse(\n            status=\"idle\",\n            message=\"\u26a0\ufe0f No hay grabaci\u00f3n en curso\",\n        )\n\n    try:\n        self._is_recording = False\n\n        # Eliminar flag file\n        if config.paths.recording_flag.exists():\n            config.paths.recording_flag.unlink()\n\n        # Notificar procesamiento\n        self.notifications.notify(\"\u26a1 v2m procesando\", \"procesando...\")\n\n        # Detener y obtener transcripci\u00f3n\n        transcription = await self.transcriber.stop()\n\n        # Validar resultado\n        if not transcription or not transcription.strip():\n            self.notifications.notify(\"\u274c whisper\", \"no se detect\u00f3 voz en el audio\")\n            return ToggleResponse(\n                status=\"idle\",\n                message=\"\u274c No se detect\u00f3 voz\",\n                text=None,\n            )\n\n        # Copiar al portapapeles\n        self.clipboard.copy(transcription)\n\n        # Notificar \u00e9xito\n        preview = transcription[:80]\n        self.notifications.notify(\"\u2705 whisper - copiado\", f\"{preview}...\")\n\n        logger.info(f\"\u2705 Transcripci\u00f3n completada: {len(transcription)} chars\")\n        return ToggleResponse(\n            status=\"idle\",\n            message=\"\u2705 Copiado al portapapeles\",\n            text=transcription,\n        )\n\n    except Exception as e:\n        logger.error(f\"Error deteniendo grabaci\u00f3n: {e}\")\n        self._is_recording = False\n        return ToggleResponse(\n            status=\"error\",\n            message=f\"\u274c Error: {e}\",\n        )\n</code></pre>"},{"location":"api/backend/orchestrator/#v2m.services.orchestrator.Orchestrator.toggle","title":"<code>toggle()</code>  <code>async</code>","text":"<p>Alterna el estado de grabaci\u00f3n del sistema.</p> <p>Gestiona la l\u00f3gica de cambio de estado: si el sistema est\u00e1 en reposo (idle), inicia la captura de audio y el streaming. Si est\u00e1 grabando, detiene la captura, finaliza la transcripci\u00f3n y copia al portapapeles.</p> <p>Este es el endpoint principal usado por los atajos de teclado.</p> <p>Returns:</p> Name Type Description <code>ToggleResponse</code> <code>ToggleResponse</code> <p>Objeto con el nuevo estado del sistema y, en caso de detenerse, el texto final transcrito.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>Si los servicios de backend (Whisper/Audio) no responden. (Error capturado internamente y reportado en ToggleResponse.message)</p> Example <p>response = await orchestrator.toggle() if response.status == \"idle\" and response.text: ...     print(f\"Transcrito: {response.text}\")</p> Source code in <code>apps/daemon/backend/src/v2m/services/orchestrator.py</code> <pre><code>async def toggle(self) -&gt; ToggleResponse:\n    \"\"\"Alterna el estado de grabaci\u00f3n del sistema.\n\n    Gestiona la l\u00f3gica de cambio de estado: si el sistema est\u00e1 en reposo\n    (idle), inicia la captura de audio y el streaming. Si est\u00e1 grabando,\n    detiene la captura, finaliza la transcripci\u00f3n y copia al portapapeles.\n\n    Este es el endpoint principal usado por los atajos de teclado.\n\n    Returns:\n        ToggleResponse: Objeto con el nuevo estado del sistema y, en caso\n            de detenerse, el texto final transcrito.\n\n    Raises:\n        RuntimeError: Si los servicios de backend (Whisper/Audio) no responden.\n            (Error capturado internamente y reportado en ToggleResponse.message)\n\n    Example:\n        &gt;&gt;&gt; response = await orchestrator.toggle()\n        &gt;&gt;&gt; if response.status == \"idle\" and response.text:\n        ...     print(f\"Transcrito: {response.text}\")\n    \"\"\"\n    if not self._is_recording:\n        return await self.start()\n    else:\n        return await self.stop()\n</code></pre>"},{"location":"api/backend/orchestrator/#v2m.services.orchestrator.Orchestrator.translate_text","title":"<code>translate_text(text, target_lang)</code>  <code>async</code>","text":"<p>Traduce texto a otro idioma usando el LLM configurado.</p> <p>Valida el idioma destino para prevenir inyecci\u00f3n de prompts maliciosos, luego delega al backend LLM para traducci\u00f3n.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>Texto a traducir en cualquier idioma.</p> required <code>target_lang</code> <code>str</code> <p>C\u00f3digo o nombre del idioma destino (ej. 'en', 'espa\u00f1ol'). Debe coincidir con regex: ^[a-zA-Z\\s-]{2,20}$</p> required <p>Returns:</p> Name Type Description <code>LLMResponse</code> <code>LLMResponse</code> <p>Texto traducido y backend usado. Si hay error, retorna texto original con backend='error'.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>Si target_lang tiene caracteres inv\u00e1lidos (sanitizado).</p> Example <p>response = await orchestrator.translate_text(\"Buenos d\u00edas\", \"en\") print(response.text)  # \"Good morning\"</p> Source code in <code>apps/daemon/backend/src/v2m/services/orchestrator.py</code> <pre><code>async def translate_text(self, text: str, target_lang: str) -&gt; LLMResponse:\n    r\"\"\"Traduce texto a otro idioma usando el LLM configurado.\n\n    Valida el idioma destino para prevenir inyecci\u00f3n de prompts maliciosos,\n    luego delega al backend LLM para traducci\u00f3n.\n\n    Args:\n        text: Texto a traducir en cualquier idioma.\n        target_lang: C\u00f3digo o nombre del idioma destino (ej. 'en', 'espa\u00f1ol').\n            Debe coincidir con regex: ^[a-zA-Z\\s\\-]{2,20}$\n\n    Returns:\n        LLMResponse: Texto traducido y backend usado.\n            Si hay error, retorna texto original con backend='error'.\n\n    Raises:\n        ValueError: Si target_lang tiene caracteres inv\u00e1lidos (sanitizado).\n\n    Example:\n        &gt;&gt;&gt; response = await orchestrator.translate_text(\"Buenos d\u00edas\", \"en\")\n        &gt;&gt;&gt; print(response.text)  # \"Good morning\"\n    \"\"\"\n    from v2m.api import LLMResponse\n\n    backend_name = config.llm.backend\n\n    # Validar target_lang para prevenir inyecci\u00f3n\n    if not re.match(r\"^[a-zA-Z\\s\\-]{2,20}$\", target_lang):\n        logger.warning(f\"Idioma inv\u00e1lido: {target_lang}\")\n        self.notifications.notify(\"\u274c Error\", \"Idioma de destino inv\u00e1lido\")\n        return LLMResponse(text=text, backend=\"error\")\n\n    try:\n        if asyncio.iscoroutinefunction(self.llm_service.translate_text):\n            translated = await self.llm_service.translate_text(text, target_lang)\n        else:\n            translated = await asyncio.to_thread(self.llm_service.translate_text, text, target_lang)\n\n        self.clipboard.copy(translated)\n        self.notifications.notify(f\"\u2705 Traducci\u00f3n ({target_lang})\", f\"{translated[:80]}...\")\n\n        return LLMResponse(text=translated, backend=backend_name)\n\n    except Exception as e:\n        logger.error(f\"Error traduciendo con {backend_name}: {e}\")\n        self.notifications.notify(\"\u274c Error traducci\u00f3n\", \"Fallo al traducir\")\n        return LLMResponse(text=text, backend=f\"{backend_name} (error)\")\n</code></pre>"},{"location":"api/backend/orchestrator/#v2m.services.orchestrator.Orchestrator.warmup","title":"<code>warmup()</code>  <code>async</code>","text":"<p>Pre-carga el modelo Whisper en VRAM para eliminar latencia de arranque.</p> <p>Ejecuta la inicializaci\u00f3n sincr\u00f3nica del worker en un executor para no bloquear el event loop. Debe llamarse durante el lifecycle startup de FastAPI para tener el modelo \"caliente\" antes de la primera transcripci\u00f3n.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>Si el worker no puede inicializarse (error silenciado en logs, el modelo se cargar\u00e1 on-demand si falla aqu\u00ed).</p> Example <p>@asynccontextmanager ... async def lifespan(app: FastAPI): ...     await orchestrator.warmup() ...     yield</p> Source code in <code>apps/daemon/backend/src/v2m/services/orchestrator.py</code> <pre><code>async def warmup(self) -&gt; None:\n    \"\"\"Pre-carga el modelo Whisper en VRAM para eliminar latencia de arranque.\n\n    Ejecuta la inicializaci\u00f3n sincr\u00f3nica del worker en un executor para no\n    bloquear el event loop. Debe llamarse durante el lifecycle startup de\n    FastAPI para tener el modelo \"caliente\" antes de la primera transcripci\u00f3n.\n\n    Raises:\n        RuntimeError: Si el worker no puede inicializarse (error silenciado\n            en logs, el modelo se cargar\u00e1 on-demand si falla aqu\u00ed).\n\n    Example:\n        &gt;&gt;&gt; @asynccontextmanager\n        ... async def lifespan(app: FastAPI):\n        ...     await orchestrator.warmup()\n        ...     yield\n    \"\"\"\n    if self._model_loaded:\n        return\n\n    try:\n        # Warmup sincr\u00f3nico en el executor del worker\n        loop = asyncio.get_running_loop()\n        await loop.run_in_executor(None, self.worker.initialize_sync)\n        self._model_loaded = True\n        logger.info(\"\u2705 Modelo Whisper precargado en VRAM\")\n    except Exception as e:\n        logger.error(f\"\u274c Error en warmup del modelo: {e}\")\n</code></pre>"},{"location":"api/backend/transcription/","title":"Transcripci\u00f3n","text":"<p>Servicios de transcripci\u00f3n de audio a texto usando faster-whisper.</p>"},{"location":"api/backend/transcription/#persistentwhisperworker","title":"PersistentWhisperWorker","text":"<p>Worker persistente que mantiene el modelo Whisper cargado en VRAM entre sesiones.</p> <p>Ubicaci\u00f3n: <code>v2m/infrastructure/persistent_model.py</code></p>"},{"location":"api/backend/transcription/#caracteristicas","title":"Caracter\u00edsticas","text":"<ul> <li>Lazy Loading: El modelo se carga la primera vez que se necesita</li> <li>Keep-Warm Policy: Mantiene el modelo en VRAM seg\u00fan configuraci\u00f3n</li> <li>GPU Optimizado: Usa <code>float16</code> o <code>int8_float16</code> para m\u00e1ximo rendimiento</li> </ul>"},{"location":"api/backend/transcription/#metodos","title":"M\u00e9todos","text":"<pre><code>class PersistentWhisperWorker:\n    def initialize_sync(self) -&gt; None:\n        \"\"\"Carga el modelo en VRAM (s\u00edncrono, para warmup).\"\"\"\n\n    async def transcribe(self, audio: np.ndarray) -&gt; str:\n        \"\"\"Transcribe audio a texto.\"\"\"\n\n    async def unload(self) -&gt; None:\n        \"\"\"Libera el modelo de VRAM.\"\"\"\n</code></pre>"},{"location":"api/backend/transcription/#streamingtranscriber","title":"StreamingTranscriber","text":"<p>Transcriptor en tiempo real que proporciona feedback provisional mientras el usuario habla.</p> <p>Ubicaci\u00f3n: <code>v2m/infrastructure/streaming_transcriber.py</code></p>"},{"location":"api/backend/transcription/#flujo-de-datos","title":"Flujo de Datos","text":"<pre><code>graph LR\n    A[AudioRecorder] --&gt; B[VAD]\n    B --&gt; C[StreamingTranscriber]\n    C --&gt; D[WhisperWorker]\n    D --&gt; E[WebSocket]\n\n    style C fill:#e3f2fd</code></pre>"},{"location":"api/backend/transcription/#integracion","title":"Integraci\u00f3n","text":"<p>El <code>StreamingTranscriber</code> emite eventos via WebSocket:</p> <ul> <li><code>transcription_update</code>: Texto provisional durante grabaci\u00f3n</li> <li><code>transcription_final</code>: Texto final al detener</li> </ul>"},{"location":"en/","title":"\ud83d\udde3\ufe0f Voice2Machine: Local Voice Dictation","text":""},{"location":"en/#purpose","title":"\ud83c\udfaf Purpose","text":"<p>The goal is simple:</p> <p>Be able to dictate text anywhere in your operating system.</p> <p>The idea is to transcribe audio using your local GPU for maximum speed and accuracy, regardless of the application you're using (code editor, browser, chat, etc.).</p> <p>This project transforms a simple script into a robust modular application based on a Backend Daemon (Python), designed with Hexagonal Architecture to ensure maintainability, scalability, and absolute privacy.</p>"},{"location":"en/#documentation","title":"\ud83d\udcda Documentation","text":"<p>The documentation is organized to serve different needs:</p>"},{"location":"en/#exploration","title":"\ud83d\ude80 Exploration","text":"<ul> <li>Quick Start: Start dictating in minutes.</li> <li>Glossary: Defines key terms like Daemon, Whisper, and REST API.</li> </ul>"},{"location":"en/#procedures","title":"\ud83d\udee0\ufe0f Procedures","text":"<ul> <li>Installation: Step-by-step guide for Ubuntu/Debian.</li> <li>Contributing: How to collaborate on the project.</li> </ul>"},{"location":"en/#reference","title":"\u2699\ufe0f Reference","text":"<ul> <li>Configuration: Adjust models, devices, and behaviors.</li> <li>Keyboard Shortcuts: Reference for global commands.</li> <li>REST API: HTTP endpoints documentation.</li> <li>Python API: Backend classes and methods reference.</li> </ul>"},{"location":"en/#concepts","title":"\ud83e\udde0 Concepts","text":"<ul> <li>Architecture: Hexagonal Design and system components.</li> <li>Decisions (ADR): Record of important technical decisions.</li> </ul>"},{"location":"en/#maintenance","title":"\ud83d\udd27 Maintenance","text":"<ul> <li>Troubleshooting: Diagnosis and fixing common errors.</li> <li>Changelog: Project change history.</li> </ul>"},{"location":"en/api_reference/","title":"REST API Reference","text":"<p>This section documents the Voice2Machine Daemon REST API (v0.2.0+).</p> <p>Updated Architecture</p> <p>Voice2Machine uses FastAPI for client-server communication, replacing the previous Unix Sockets IPC system. This allows testing endpoints directly with <code>curl</code> or any HTTP client.</p>"},{"location":"en/api_reference/#general-information","title":"General Information","text":"Property Value Base URL <code>http://localhost:8765</code> Protocol HTTP/1.1 + WebSocket Format JSON (UTF-8) Interactive Docs <code>http://localhost:8765/docs</code> (Swagger UI)"},{"location":"en/api_reference/#rest-endpoints","title":"REST Endpoints","text":""},{"location":"en/api_reference/#post-toggle","title":"POST <code>/toggle</code>","text":"<p>Recording toggle (start/stop). This is the main endpoint used by keyboard shortcuts.</p> Request <p><code>bash     curl -X POST http://localhost:8765/toggle | jq</code></p> Response (Starting) <p><code>json     {       \"status\": \"recording\",       \"message\": \"Recording started\",       \"text\": null     }</code></p> Response (Stopping) <p><code>json     {       \"status\": \"idle\",       \"message\": \"Transcription complete\",       \"text\": \"The transcribed text appears here...\"     }</code></p>"},{"location":"en/api_reference/#post-start","title":"POST <code>/start</code>","text":"<p>Explicitly start recording. Useful when you need separate start/stop control.</p> Request <p><code>bash     curl -X POST http://localhost:8765/start | jq</code></p> Response <p><code>json     {       \"status\": \"recording\",       \"message\": \"Recording started\",       \"text\": null     }</code></p>"},{"location":"en/api_reference/#post-stop","title":"POST <code>/stop</code>","text":"<p>Stop recording and transcribe captured audio.</p> Request <p><code>bash     curl -X POST http://localhost:8765/stop | jq</code></p> Response <p><code>json     {       \"status\": \"idle\",       \"message\": \"Transcription complete\",       \"text\": \"The transcribed text appears here...\"     }</code></p>"},{"location":"en/api_reference/#post-llmprocess","title":"POST <code>/llm/process</code>","text":"<p>Process text with LLM (cleanup, punctuation, formatting). Backend is selected based on <code>config.toml</code>.</p> Request <p><code>bash     curl -X POST http://localhost:8765/llm/process \\       -H \"Content-Type: application/json\" \\       -d '{\"text\": \"hello how are you hope youre well\"}' | jq</code></p> Response <p><code>json     {       \"text\": \"Hello, how are you? Hope you're well.\",       \"backend\": \"gemini\"     }</code></p>"},{"location":"en/api_reference/#post-llmtranslate","title":"POST <code>/llm/translate</code>","text":"<p>Translate text to another language using LLM.</p> Request <p><code>bash     curl -X POST http://localhost:8765/llm/translate \\       -H \"Content-Type: application/json\" \\       -d '{\"text\": \"Good morning\", \"target_lang\": \"es\"}' | jq</code></p> Response <p><code>json     {       \"text\": \"Buenos d\u00edas\",       \"backend\": \"gemini\"     }</code></p>"},{"location":"en/api_reference/#get-status","title":"GET <code>/status</code>","text":"<p>Returns current daemon state.</p> Request <p><code>bash     curl http://localhost:8765/status | jq</code></p> Response <p><code>json     {       \"state\": \"idle\",       \"recording\": false,       \"model_loaded\": true     }</code></p> <p>Possible States:</p> State Description <code>idle</code> Waiting for commands <code>recording</code> Recording audio <code>processing</code> Transcribing or processing with LLM"},{"location":"en/api_reference/#get-health","title":"GET <code>/health</code>","text":"<p>Health check for systemd/monitoring scripts.</p> Request <p><code>bash     curl http://localhost:8765/health | jq</code></p> Response <p><code>json     {       \"status\": \"ok\",       \"version\": \"0.2.0\"     }</code></p>"},{"location":"en/api_reference/#websocket","title":"WebSocket","text":""},{"location":"en/api_reference/#ws-wsevents","title":"WS <code>/ws/events</code>","text":"<p>Real-time event stream. Useful for showing provisional transcription while user speaks.</p> Connection (JavaScript) <pre><code>const ws = new WebSocket('ws://localhost:8765/ws/events');\n\n    ws.onmessage = (event) =&gt; {\n      const { event: eventType, data } = JSON.parse(event.data);\n      console.log(`Event: ${eventType}`, data);\n    };\n    ```\n\n=== \"Connection (Python)\"\n```python\nimport asyncio\nimport websockets\n\n    async def listen():\n        async with websockets.connect('ws://localhost:8765/ws/events') as ws:\n            async for message in ws:\n                print(message)\n\n    asyncio.run(listen())\n    ```\n\n**Emitted Events:**\n\n| Event                  | Fields                           | Description                                 |\n| ---------------------- | -------------------------------- | ------------------------------------------- |\n| `transcription_update` | `text: str`, `final: bool`       | Transcription update (provisional or final) |\n| `heartbeat`            | `timestamp: float`, `state: str` | Heartbeat to keep connection alive          |\n\n---\n\n## Data Models\n\n### ToggleResponse\n\n```python\nclass ToggleResponse(BaseModel):\n    status: str      # 'recording' | 'idle'\n    message: str     # Descriptive message\n    text: str | None # Transcribed text (only on stop)\n</code></pre>"},{"location":"en/api_reference/#statusresponse","title":"StatusResponse","text":"<pre><code>class StatusResponse(BaseModel):\n    state: str        # 'idle' | 'recording' | 'processing'\n    recording: bool   # True if recording\n    model_loaded: bool # True if Whisper is in VRAM\n</code></pre>"},{"location":"en/api_reference/#llmresponse","title":"LLMResponse","text":"<pre><code>class LLMResponse(BaseModel):\n    text: str    # Processed/translated text\n    backend: str # 'gemini' | 'ollama' | 'local'\n</code></pre>"},{"location":"en/api_reference/#error-codes","title":"Error Codes","text":"HTTP Code Meaning <code>200</code> Successful operation <code>422</code> Validation error (invalid payload) <code>500</code> Internal server error <p>Debugging</p> <p>Use the interactive documentation at <code>http://localhost:8765/docs</code> to test endpoints visually.</p>"},{"location":"en/architecture/","title":"\ud83e\udde9 System Architecture","text":"<p>Technical Philosophy</p> <p>Voice2Machine implements a strict Hexagonal Architecture (Ports &amp; Adapters), prioritizing decoupling, testability, and technological independence. The system adheres to SOTA 2026 standards like static typing in Python (Protocol) and Frontend/Backend separation via REST API.</p>"},{"location":"en/architecture/#high-level-diagram","title":"\ud83c\udfd7\ufe0f High-Level Diagram","text":"<pre><code>graph TD\n    subgraph Clients [\"\ud83d\udd0c Clients (CLI / Scripts / GUI / Tauri)\"]\n        ClientApp[\"Any HTTP Client\"]\n    end\n\n    subgraph Backend [\"\ud83d\udc0d Backend Daemon (Python + FastAPI)\"]\n        API[\"FastAPI Server&lt;br&gt;(api.py)\"]\n\n        subgraph Hexagon [\"Hexagon (Core)\"]\n            Orchestrator[\"Orchestrator&lt;br&gt;(Coordination)\"]\n            Domain[\"Domain&lt;br&gt;(Interfaces/Models)\"]\n        end\n\n        subgraph Infra [\"Infrastructure (Adapters)\"]\n            Whisper[\"Whisper Adapter&lt;br&gt;(faster-whisper)\"]\n            Audio[\"Audio Engine&lt;br&gt;(Rust v2m_engine)\"]\n            LLM[\"LLM Providers&lt;br&gt;(Gemini/Ollama)\"]\n        end\n    end\n\n    ClientApp &lt;--&gt;|REST + WebSocket| API\n    API --&gt; Orchestrator\n    Orchestrator --&gt; Domain\n    Whisper -.-&gt;|Implements| Domain\n    Audio -.-&gt;|Implements| Domain\n    LLM -.-&gt;|Implements| Domain\n\n    style Clients fill:#e3f2fd,stroke:#1565c0\n    style Backend fill:#e8f5e9,stroke:#2e7d32\n    style Hexagon fill:#fff3e0,stroke:#ef6c00\n    style Infra fill:#f3e5f5,stroke:#7b1fa2</code></pre>"},{"location":"en/architecture/#backend-components","title":"\ud83d\udce6 Backend Components","text":""},{"location":"en/architecture/#1-api-layer-fastapi","title":"1. API Layer (FastAPI)","text":"<p>Located in <code>apps/daemon/backend/src/v2m/api.py</code>.</p> <ul> <li>REST Endpoints: <code>/toggle</code>, <code>/start</code>, <code>/stop</code>, <code>/status</code>, <code>/health</code></li> <li>WebSocket: <code>/ws/events</code> for real-time transcription streaming</li> <li>Auto-documentation: Swagger UI at <code>/docs</code></li> </ul> <p>Migration Complete</p> <p>The previous system used Unix Domain Sockets with custom binary protocol. Since v0.2.0, we use FastAPI for simplicity and compatibility with any HTTP client.</p>"},{"location":"en/architecture/#2-orchestrator-coordination","title":"2. Orchestrator (Coordination)","text":"<p>Located in <code>apps/daemon/backend/src/v2m/services/orchestrator.py</code>.</p> <p>The Orchestrator is the central coordination point that:</p> <ul> <li>Manages the complete lifecycle: recording \u2192 transcription \u2192 post-processing</li> <li>Maintains system state (idle, recording, processing)</li> <li>Coordinates communication between adapters without coupling them directly</li> <li>Emits events to connected WebSocket clients</li> </ul> <pre><code>class Orchestrator:\n    async def toggle(self) -&gt; ToggleResponse: ...\n    async def start(self) -&gt; ToggleResponse: ...\n    async def stop(self) -&gt; ToggleResponse: ...\n    async def warmup(self) -&gt; None: ...\n</code></pre>"},{"location":"en/architecture/#3-core-the-hexagon","title":"3. Core (The Hexagon)","text":"<p>Located in <code>apps/daemon/backend/src/v2m/core/</code> and <code>domain/</code>.</p> <ul> <li>Ports (Interfaces): Defined using <code>typing.Protocol</code> + <code>@runtime_checkable</code> for structural checking at runtime</li> <li>Domain Models: DTOs with Pydantic V2 for automatic validation</li> <li>Strict Contracts: Adapters implement interfaces, not concrete classes</li> </ul>"},{"location":"en/architecture/#4-infrastructure-adapters","title":"4. Infrastructure (Adapters)","text":"<p>Located in <code>apps/daemon/backend/src/v2m/infrastructure/</code>.</p> Adapter Responsibility WhisperAdapter Transcription with <code>faster-whisper</code>. Lazy loading to save VRAM AudioRecorder Audio capture using Rust extension (<code>v2m_engine</code>) LLMProviders Factory for Gemini/Ollama based on configuration SystemMonitor Real-time GPU/CPU telemetry"},{"location":"en/architecture/#client-backend-communication","title":"\u26a1 Client-Backend Communication","text":"<p>Voice2Machine uses FastAPI REST + WebSocket for communication:</p>"},{"location":"en/architecture/#rest-synchronous","title":"REST (Synchronous)","text":"<pre><code># Toggle recording\ncurl -X POST http://localhost:8765/toggle | jq\n\n# Check status\ncurl http://localhost:8765/status | jq\n</code></pre>"},{"location":"en/architecture/#websocket-streaming","title":"WebSocket (Streaming)","text":"<pre><code>const ws = new WebSocket(\"ws://localhost:8765/ws/events\");\nws.onmessage = (e) =&gt; {\n  const { event, data } = JSON.parse(e.data);\n  if (event === \"transcription_update\") {\n    console.log(data.text, data.final);\n  }\n};\n</code></pre>"},{"location":"en/architecture/#native-extensions-rust","title":"\ud83e\udd80 Native Extensions (Rust)","text":"<p>For critical tasks where Python's GIL is a bottleneck, we use native extensions compiled in Rust (<code>v2m_engine</code>):</p> Component Function Audio I/O Direct WAV writing to disk (zero-copy) VAD Ultra-low latency voice detection (Silero ONNX) Ring Buffer Lock-free circular buffer for real-time audio"},{"location":"en/architecture/#data-flow","title":"\ud83d\udd04 Data Flow","text":"<pre><code>sequenceDiagram\n    participant User\n    participant Client as HTTP Client\n    participant API as FastAPI\n    participant Orch as Orchestrator\n    participant Audio as AudioRecorder\n    participant Whisper as WhisperAdapter\n\n    User-&gt;&gt;Client: Press shortcut\n    Client-&gt;&gt;API: POST /toggle\n    API-&gt;&gt;Orch: toggle()\n\n    alt Not recording\n        Orch-&gt;&gt;Audio: start_recording()\n        Audio--&gt;&gt;Orch: OK\n        Orch--&gt;&gt;API: status=recording\n    else Recording\n        Orch-&gt;&gt;Audio: stop_recording()\n        Audio--&gt;&gt;Orch: audio_buffer\n        Orch-&gt;&gt;Whisper: transcribe(buffer)\n        Whisper--&gt;&gt;Orch: text\n        Orch--&gt;&gt;API: status=idle, text=...\n    end\n\n    API--&gt;&gt;Client: ToggleResponse\n    Client-&gt;&gt;User: Copy to clipboard</code></pre>"},{"location":"en/architecture/#2026-design-principles","title":"\ud83d\udee1\ufe0f 2026 Design Principles","text":"Principle Implementation Local-First No data leaves the machine unless a cloud provider is explicitly configured Privacy-By-Design Audio processed in memory, temp files deleted after transcription Resilience Automatic error recovery, subsystem restart if they fail Observability Structured logging (OpenTelemetry), real-time metrics Performance is Design Async FastAPI, Rust for hot paths, warm model in VRAM"},{"location":"en/changelog/","title":"Changelog","text":"<p>All notable changes to this project will be documented in this file.</p> <p>The format is based on Keep a Changelog, and this project adheres to Semantic Versioning.</p>"},{"location":"en/changelog/#added","title":"Added","text":"<ul> <li>FastAPI REST API: New HTTP API replacing the Unix Sockets-based IPC system</li> <li>WebSocket streaming: <code>/ws/events</code> endpoint for real-time provisional transcription</li> <li>Swagger documentation: Interactive UI at <code>/docs</code> for testing endpoints</li> <li>Orchestrator pattern: New coordination pattern that simplifies workflow</li> <li>Rust audio engine: Native <code>v2m_engine</code> extension for low-latency audio capture</li> <li>MkDocs documentation system: Structured documentation with Material theme</li> </ul>"},{"location":"en/changelog/#changed","title":"Changed","text":"<ul> <li>Simplified architecture: From CQRS/CommandBus to more direct Orchestrator pattern</li> <li>Communication: From binary Unix Domain Sockets to standard HTTP REST</li> <li>State model: Centralized management in <code>DaemonState</code> with lazy initialization</li> <li>Updated README.md with new architecture</li> </ul>"},{"location":"en/changelog/#removed","title":"Removed","text":"<ul> <li><code>daemon.py</code>: Replaced by <code>api.py</code> (FastAPI)</li> <li><code>client.py</code>: No longer needed, use <code>curl</code> or any HTTP client</li> <li>Binary IPC protocol: Replaced by standard JSON</li> </ul>"},{"location":"en/changelog/#fixed","title":"Fixed","text":"<ul> <li>Startup latency: Server starts in ~100ms, model loads in background</li> <li>Memory leaks in WebSocket connections</li> </ul>"},{"location":"en/changelog/#planned","title":"Planned","text":"<ul> <li>Support for multiple simultaneous transcription languages</li> <li>Web dashboard for real-time monitoring</li> <li>Integration with more LLM providers</li> </ul>"},{"location":"en/changelog/#added_1","title":"Added","text":"<ul> <li>Initial Voice2Machine system version</li> <li>Local transcription support with Whisper (faster-whisper)</li> <li>Basic LLM integration (Ollama/Gemini)</li> <li>Unix Domain Sockets-based IPC system</li> <li>Hexagonal architecture with ports and adapters</li> <li>TOML-based configuration</li> </ul>"},{"location":"en/configuration/","title":"\u2699\ufe0f Configuration Guide","text":"<p>Configuration Management</p> <p>Configuration is primarily managed through the Frontend GUI (Gear icon \u2699\ufe0f). However, advanced users can directly edit the <code>config.toml</code> file.</p> <p>File location: <code>$XDG_CONFIG_HOME/v2m/config.toml</code> (usually <code>~/.config/v2m/config.toml</code>).</p>"},{"location":"en/configuration/#1-local-transcription-transcription","title":"1. Local Transcription (<code>[transcription]</code>)","text":"<p>The heart of the system. These parameters control the Faster-Whisper engine.</p> Parameter Type Default Description and Best Practice 2026 <code>model</code> <code>str</code> <code>distil-large-v3</code> Model to load. <code>distil-large-v3</code> offers extreme speed with SOTA accuracy. Options: <code>large-v3-turbo</code>, <code>medium</code>. <code>device</code> <code>str</code> <code>cuda</code> <code>cuda</code> (NVIDIA GPU) is mandatory for real-time experience. <code>cpu</code> is functional but not recommended. <code>compute_type</code> <code>str</code> <code>float16</code> Tensor precision. <code>float16</code> or <code>int8_float16</code> optimize VRAM and throughput on modern GPUs. <code>use_faster_whisper</code> <code>bool</code> <code>true</code> Enables the optimized CTranslate2 backend."},{"location":"en/configuration/#voice-activity-detection-vad","title":"Voice Activity Detection (VAD)","text":"<p>The system uses Silero VAD (Rust version in <code>v2m_engine</code>) to filter silence before invoking Whisper, saving GPU cycles.</p> <ul> <li><code>vad_filter</code> (<code>true</code>): Activates pre-filtering.</li> <li><code>vad_parameters</code>: Fine-tune sensitivity (silence threshold, minimum voice duration).</li> </ul>"},{"location":"en/configuration/#2-llm-services-llm","title":"2. LLM Services (<code>[llm]</code>)","text":"<p>Voice2Machine implements a Provider pattern to support multiple AI backends for text refinement.</p>"},{"location":"en/configuration/#global-configuration","title":"Global Configuration","text":"Parameter Description <code>provider</code> Active provider: <code>gemini</code> (Cloud) or <code>ollama</code> (Local). <code>model</code> Specific model name (e.g., <code>gemini-1.5-flash</code> or <code>llama3:8b</code>)."},{"location":"en/configuration/#specific-providers","title":"Specific Providers","text":""},{"location":"en/configuration/#google-gemini-provider-gemini","title":"Google Gemini (<code>provider = \"gemini\"</code>)","text":"<p>Requires API Key. Ideal for users without powerful GPU (VRAM &lt; 8GB).</p> <ul> <li>Recommended model: <code>gemini-1.5-flash-latest</code> (minimum latency).</li> <li>Temperature: <code>0.3</code> (conservative) for grammar correction.</li> </ul>"},{"location":"en/configuration/#ollama-provider-ollama","title":"Ollama (<code>provider = \"ollama\"</code>)","text":"<p>Total privacy. Requires running the Ollama server (<code>ollama serve</code>).</p> <ul> <li>Endpoint: <code>http://localhost:11434</code></li> <li>Recommended model: <code>qwen2.5:7b</code> or <code>llama3.1:8b</code>.</li> </ul>"},{"location":"en/configuration/#3-recording-recording","title":"3. Recording (<code>[recording]</code>)","text":"<p>Controls audio capture via <code>SoundDevice</code> and <code>v2m_engine</code>.</p> <ul> <li><code>sample_rate</code>: <code>16000</code> (Fixed, required by Whisper).</li> <li><code>channels</code>: <code>1</code> (Mono).</li> <li><code>device_index</code>: Microphone ID. If <code>null</code>, uses system default (PulseAudio/PipeWire).</li> </ul>"},{"location":"en/configuration/#4-system-system","title":"4. System (<code>[system]</code>)","text":"<p>Low-level configuration for the Daemon and communication.</p> <ul> <li><code>host</code>: Server host (<code>127.0.0.1</code> for local-only access).</li> <li><code>port</code>: HTTP port (<code>8765</code> by default).</li> <li><code>log_level</code>: <code>INFO</code> by default. Change to <code>DEBUG</code> for deep diagnostics.</li> </ul>"},{"location":"en/configuration/#secrets-and-security","title":"Secrets and Security","text":"<p>API keys are managed via environment variables or secure storage, never in plain text inside <code>config.toml</code> if possible.</p> <pre><code># Define in .env or system environment\nexport GEMINI_API_KEY=\"AIzaSy_YOUR_KEY_HERE\"\n</code></pre> <p>Important</p> <p>Restart the daemon (<code>python -m v2m.main</code>) after manually editing the configuration file to apply changes.</p>"},{"location":"en/contributing/","title":"\u2764\ufe0f Contributing Guide","text":"<p>Thank you for your interest in contributing to Voice2Machine! This project is built on collaboration and quality code.</p> <p>To maintain our \"State of the Art 2026\" standards, we follow strict but fair rules. Please read this before submitting your first Pull Request.</p>"},{"location":"en/contributing/#workflow","title":"\ud83d\ude80 Workflow","text":"<ol> <li>Discussion First: Before writing code, open an Issue to discuss the change. This avoids duplicate work or rejections due to architectural misalignment.</li> <li>Fork &amp; Branch:<ul> <li>Fork the repository.</li> <li>Create a descriptive branch: <code>feat/new-gpu-support</code> or <code>fix/transcription-error</code>.</li> </ul> </li> <li>Local Development: Follow the Installation guide to set up your development environment.</li> </ol>"},{"location":"en/contributing/#quality-standards","title":"\ud83d\udccf Quality Standards","text":""},{"location":"en/contributing/#code","title":"Code","text":"<ul> <li>Backend (Python):</li> <li>Strict static typing (100% Type Hints).</li> <li>Linter: <code>ruff check src/ --fix</code>.</li> <li>Formatter: <code>ruff format src/</code>.</li> <li>Tests: <code>pytest</code> must pass at 100%.</li> <li>Frontend (Tauri/React):</li> <li>Strict TypeScript (no <code>any</code>).</li> <li>Linter: <code>npm run lint</code>.</li> <li>Functional components and Hooks.</li> </ul>"},{"location":"en/contributing/#commits","title":"Commits","text":"<p>We use Conventional Commits. Your commit message must follow this format:</p> <pre><code>&lt;type&gt;(&lt;scope&gt;): &lt;short description&gt;\n\n[Optional detailed body]\n</code></pre> <p>Allowed Types:</p> <ul> <li><code>feat</code>: New functionality.</li> <li><code>fix</code>: Bug fix.</li> <li><code>docs</code>: Documentation only.</li> <li><code>refactor</code>: Code change that doesn't fix bugs or add features.</li> <li><code>test</code>: Add or fix tests.</li> <li><code>chore</code>: Maintenance, dependencies.</li> </ul> <p>Example:</p> <p><code>feat(whisper): upgrade to faster-whisper 1.0.0 for 20% speedup</code></p>"},{"location":"en/contributing/#documentation-docs-as-code","title":"Documentation (Docs as Code)","text":"<p>If you change functionality, you must update documentation in <code>docs/docs/</code>.</p> <ul> <li>Verify that <code>mkdocs serve</code> works locally.</li> <li>Follow the Style Guide.</li> </ul>"},{"location":"en/contributing/#pull-request-checklist","title":"\u2705 Pull Request Checklist","text":"<p>Before submitting your PR:</p> <ul> <li> I have run local tests and they pass.</li> <li> I have linted the code (<code>ruff</code>, <code>eslint</code>).</li> <li> I have updated relevant documentation.</li> <li> I have added an entry to <code>CHANGELOG.md</code> (if applicable).</li> <li> My code follows Hexagonal Architecture (no prohibited cross-imports).</li> </ul> <p>Help</p> <p>If you have questions about architecture or design, check the documents in <code>docs/docs/adr/</code> or ask in the corresponding Issue.</p>"},{"location":"en/glossary/","title":"Glossary","text":"<p>This glossary defines technical and domain terms used in Voice2Machine.</p>"},{"location":"en/glossary/#general-terms","title":"General Terms","text":""},{"location":"en/glossary/#local-first","title":"Local-First","text":"<p>Design philosophy where data (audio, text) is processed and stored exclusively on the user's device, without relying on the cloud.</p>"},{"location":"en/glossary/#daemon","title":"Daemon","text":"<p>Background process (written in Python) that manages recording, transcription, and communication with the frontend.</p>"},{"location":"en/glossary/#rest-api","title":"REST API","text":"<p>Communication mechanism between the Daemon (Python) and clients (scripts, frontends). We use FastAPI with standard HTTP endpoints and WebSocket for real-time events.</p>"},{"location":"en/glossary/#technical-components","title":"Technical Components","text":""},{"location":"en/glossary/#whisper","title":"Whisper","text":"<p>Speech recognition model (ASR) developed by OpenAI. Voice2Machine uses <code>faster-whisper</code>, an optimized implementation with CTranslate2.</p>"},{"location":"en/glossary/#orchestrator","title":"Orchestrator","text":"<p>Central coordination component that manages the complete workflow lifecycle: recording \u2192 transcription \u2192 post-processing. Replaces the previous CQRS/CommandBus pattern with a simpler direct approach.</p>"},{"location":"en/glossary/#backendprovider","title":"BackendProvider","text":"<p>Frontend component (React Context) that manages connection with the Daemon and distributes state to the UI.</p>"},{"location":"en/glossary/#telemetrycontext","title":"TelemetryContext","text":"<p>Sub-context in React optimized for high-frequency updates (GPU metrics, audio levels) to avoid unnecessary re-renders of the main UI.</p>"},{"location":"en/glossary/#hexagonal-architecture","title":"Hexagonal Architecture","text":"<p>Also known as \"Ports and Adapters\". Design pattern where the core business logic (the hexagon) is isolated from external concerns (databases, APIs, UI) through well-defined interfaces (ports) and implementations (adapters).</p>"},{"location":"en/installation/","title":"\ud83d\udee0\ufe0f Installation and Setup","text":"<p>Prerequisite</p> <p>This project is optimized for Linux (Debian/Ubuntu). State of the Art 2026: We use hardware acceleration (CUDA) and a modular approach to ensure privacy and performance.</p> <p>This guide will take you from zero to a fully functional dictation system on your local machine.</p>"},{"location":"en/installation/#method-1-automatic-installation-recommended","title":"\ud83d\ude80 Method 1: Automatic Installation (Recommended)","text":"<p>We've created a script that handles all the \"dirty work\" for you: verifies your system, installs dependencies (apt), creates the virtual environment (venv), and configures credentials.</p> <pre><code># Run from the project root\n./apps/daemon/backend/scripts/setup/install.sh\n</code></pre> <p>What this script does:</p> <ol> <li>\ud83d\udce6 Installs system libraries (<code>ffmpeg</code>, <code>xclip</code>, <code>pulseaudio-utils</code>).</li> <li>\ud83d\udc0d Creates an isolated Python environment (<code>venv</code>).</li> <li>\u2699\ufe0f Installs project dependencies (<code>faster-whisper</code>, <code>torch</code>).</li> <li>\ud83d\udd11 Helps you configure your Gemini API Key (optional, for generative AI).</li> <li>\ud83d\udda5\ufe0f Verifies if you have a compatible NVIDIA GPU.</li> </ol>"},{"location":"en/installation/#method-2-manual-installation","title":"\ud83d\udee0\ufe0f Method 2: Manual Installation","text":"<p>If you prefer full control or the automatic script fails, follow these steps.</p>"},{"location":"en/installation/#1-system-dependencies-system-level","title":"1. System Dependencies (System Level)","text":"<p>We need tools to manipulate audio and clipboard at the OS level.</p> <pre><code>sudo apt update\nsudo apt install ffmpeg xclip pulseaudio-utils python3-venv build-essential python3-dev\n</code></pre>"},{"location":"en/installation/#2-python-environment","title":"2. Python Environment","text":"<p>We isolate libraries to avoid conflicts.</p> <pre><code># Navigate to the backend directory\ncd apps/daemon/backend\n\n# Create virtual environment\npython3 -m venv venv\n\n# Activate environment (Do this every time you work on the project!)\nsource venv/bin/activate\n\n# Install dependencies\npip install -e .\n</code></pre>"},{"location":"en/installation/#3-ai-configuration-optional","title":"3. AI Configuration (Optional)","text":"<p>To use \"Text Refinement\" features (rewriting with LLM), you need a Google Gemini API Key.</p> <ol> <li>Get your key at Google AI Studio.</li> <li>Create a <code>.env</code> file at the root:</li> </ol> <pre><code>echo 'GEMINI_API_KEY=\"your_api_key_here\"' &gt; .env\n</code></pre>"},{"location":"en/installation/#verification","title":"\u2705 Verification","text":"<p>Make sure everything works before continuing.</p>"},{"location":"en/installation/#1-verify-gpu-acceleration","title":"1. Verify GPU Acceleration","text":"<p>This confirms that Whisper can use your graphics card (essential for speed).</p> <pre><code>python apps/daemon/backend/scripts/diagnostics/check_cuda.py\n</code></pre>"},{"location":"en/installation/#2-system-diagnostics","title":"2. System Diagnostics","text":"<p>Verify that the daemon and audio services are ready.</p> <pre><code>python apps/daemon/backend/scripts/diagnostics/health_check.py\n</code></pre>"},{"location":"en/installation/#next-steps","title":"\u23ed\ufe0f Next Steps","text":"<p>Once installed, it's time to configure how you interact with the tool.</p> <ul> <li>Detailed Configuration - Adjust models and sensitivity.</li> <li>Keyboard Shortcuts - Configure your magic keys.</li> </ul>"},{"location":"en/keyboard_shortcuts/","title":"\u2328\ufe0f Keyboard Shortcuts and Scripts","text":"<p>Integration Philosophy</p> <p>Voice2Machine doesn't hijack your keyboard. It provides \"atomic\" scripts that you bind to your favorite window manager (GNOME, KDE, Hyprland, i3). This ensures universal compatibility and zero background resource consumption for listening to keys.</p>"},{"location":"en/keyboard_shortcuts/#main-scripts","title":"\ud83d\udd17 Main Scripts","text":"<p>To activate features, you must create global shortcuts that execute these scripts located in <code>scripts/</code>.</p>"},{"location":"en/keyboard_shortcuts/#1-dictation-toggle","title":"1. Dictation (Toggle)","text":"<ul> <li>Script: <code>scripts/v2m-toggle.sh</code></li> <li>Function: Recording toggle.</li> <li>Idle State: Starts recording \ud83d\udd34 (Confirmation sound).</li> <li>Recording State: Stops, transcribes, and pastes the text \ud83d\udfe2.</li> <li>Suggested Shortcut: <code>Super + V</code> or mouse side button.</li> </ul>"},{"location":"en/keyboard_shortcuts/#2-ai-refinement","title":"2. AI Refinement","text":"<ul> <li>Script: <code>scripts/v2m-llm.sh</code></li> <li>Function: Contextual text improvement.</li> <li>Reads current clipboard.</li> <li>Sends text to configured LLM provider (Gemini/Ollama).</li> <li>Replaces clipboard with improved version.</li> <li>Suggested Shortcut: <code>Super + G</code>.</li> </ul>"},{"location":"en/keyboard_shortcuts/#configuration-examples","title":"\ud83d\udc27 Configuration Examples","text":""},{"location":"en/keyboard_shortcuts/#gnome-ubuntu","title":"GNOME / Ubuntu","text":"<ol> <li>Go to Settings &gt; Keyboard &gt; Keyboard Shortcuts &gt; View and Customize.</li> <li>Select Custom Shortcuts.</li> <li>Add new:<ul> <li>Name: <code>V2M: Dictate</code></li> <li>Command: <code>/home/your_user/voice2machine/scripts/v2m-toggle.sh</code></li> <li>Shortcut: <code>Super+V</code></li> </ul> </li> </ol>"},{"location":"en/keyboard_shortcuts/#hyprland","title":"Hyprland","text":"<p>In your <code>hyprland.conf</code>:</p> <pre><code>bind = SUPER, V, exec, /home/$USER/voice2machine/scripts/v2m-toggle.sh\nbind = SUPER, G, exec, /home/$USER/voice2machine/scripts/v2m-llm.sh\n</code></pre>"},{"location":"en/keyboard_shortcuts/#i3-sway","title":"i3 / Sway","text":"<p>In your <code>config</code>:</p> <pre><code>bindsym Mod4+v exec --no-startup-id /home/$USER/voice2machine/scripts/v2m-toggle.sh\nbindsym Mod4+g exec --no-startup-id /home/$USER/voice2machine/scripts/v2m-llm.sh\n</code></pre>"},{"location":"en/keyboard_shortcuts/#troubleshooting","title":"\u26a0\ufe0f Troubleshooting","text":"<p>Execution Permissions</p> <p>If the shortcut seems \"dead\", verify that scripts have execution permissions: <code>bash     chmod +x scripts/v2m-toggle.sh scripts/v2m-llm.sh</code></p> <p>Wayland vs X11</p> <p>Scripts automatically detect your graphics server. - X11: Uses <code>xclip</code> and <code>xdotool</code>. - Wayland: Uses <code>wl-copy</code> and <code>wtype</code> (make sure you have them installed if using pure Wayland).</p> <p>Latency</p> <p>These scripts use HTTP requests to communicate with the daemon, ensuring activation latency &lt; 10ms. They don't start a heavy Python instance each time.</p>"},{"location":"en/quick_start/","title":"\ud83d\udd79\ufe0f Quick Start","text":"<p>Executive Summary</p> <p>Voice2Machine has two superpowers: Dictation (Voice \u2192 Text) and Refinement (Text \u2192 Better Text).</p> <p>This visual guide helps you understand the main workflows so you can be productive in minutes.</p>"},{"location":"en/quick_start/#1-dictation-flow-voice-text","title":"1. Dictation Flow (Voice \u2192 Text)","text":"<p>Ideal for: Writing emails, code, or quick messages without touching the keyboard.</p> <ol> <li>Focus: Click on the text field where you want to write.</li> <li>Activate shortcut (Configurable, by default running <code>v2m-toggle.sh</code>). You'll hear a start sound \ud83d\udd14.</li> <li>Speak clearly. Don't worry about being robotic, speak naturally.</li> <li>Press the shortcut again to stop. You'll hear an end sound \ud83d\udd15.</li> <li>The text will paste automatically into your active field (or remain in clipboard if auto-paste is disabled).</li> </ol> <pre><code>flowchart LR\n    A((\ud83c\udfa4 START)) --&gt;|Record| B{Local Whisper}\n    B --&gt;|Transcribe| C[\ud83d\udccb Clipboard / Paste]\n\n    style A fill:#ff6b6b,stroke:#333,stroke-width:2px,color:white\n    style B fill:#feca57,stroke:#333,stroke-width:2px\n    style C fill:#48dbfb,stroke:#333,stroke-width:2px</code></pre>"},{"location":"en/quick_start/#2-refinement-flow-text-ai-text","title":"2. Refinement Flow (Text \u2192 AI \u2192 Text)","text":"<p>Ideal for: Correcting grammar, translating, or giving professional formatting to a rough draft.</p> <ol> <li>Select and Copy (<code>Ctrl + C</code>) the text you want to improve.</li> <li>Activate the AI shortcut (running <code>v2m-llm.sh</code>).</li> <li>Wait a few seconds (the AI is thinking \ud83e\udde0).</li> <li>The improved text will replace your clipboard contents.</li> <li>Paste (<code>Ctrl + V</code>) the result.</li> </ol> <pre><code>flowchart LR\n    A[\ud83d\udccb Original Text] --&gt;|Copy| B((\ud83e\udde0 AI SHORTCUT))\n    B --&gt;|Process| C{Local LLM / Gemini}\n    C --&gt;|Improve| D[\u2728 Polished Text]\n\n    style A fill:#c8d6e5,stroke:#333,stroke-width:2px\n    style B fill:#5f27cd,stroke:#333,stroke-width:2px,color:white\n    style C fill:#feca57,stroke:#333,stroke-width:2px\n    style D fill:#1dd1a1,stroke:#333,stroke-width:2px</code></pre>"},{"location":"en/quick_start/#pro-tips","title":"\ud83d\udca1 Pro Tips","text":"<p>!!! tip \"Improve Your Accuracy\" - Speak fluently: Whisper understands context from complete sentences better than isolated words. - Hardware: A noise-canceling microphone dramatically improves results. - Configuration: You can adjust the LLM \"temperature\" in settings to make it more creative or more literal.</p> <p>Privacy Guaranteed</p> <p>Dictation is 100% local (runs on your GPU). Refinement can be local (Ollama) or cloud (Gemini), you have full control in settings.</p>"},{"location":"en/style_guide/","title":"Style Guide and Governance","text":"<p>This guide defines the standards for Voice2Machine documentation, aligned with \"State of the Art 2026\".</p>"},{"location":"en/style_guide/#fundamental-principles","title":"Fundamental Principles","text":"<ol> <li>Docs as Code: Documentation lives in the repository, is versioned with Git, and validated in CI/CD.</li> <li>Universal Accessibility: Strict WCAG 2.1 Level AA compliance.</li> <li>Localization: The source of truth (<code>docs/</code>) is in Native Latin American Spanish. Root files (<code>README.md</code>, <code>AGENTS.md</code>) are in English (USA) and Spanish.</li> </ol>"},{"location":"en/style_guide/#accessibility-wcag-21-aa","title":"Accessibility (WCAG 2.1 AA)","text":"<ul> <li>Alt Text: All images must have descriptive <code>alt text</code>.</li> <li>Heading Hierarchy: Don't skip levels (H1 -&gt; H2 -&gt; H3).</li> <li>Contrast: Diagrams and screenshots must have high contrast.</li> <li>Links: Use descriptive text (\"see installation guide\" instead of \"click here\").</li> </ul>"},{"location":"en/style_guide/#tone-and-voice","title":"Tone and Voice","text":"<ul> <li>Audience: Developers and technical users.</li> <li>Tone: Professional, concise, direct (\"Do this\" instead of \"You could do this\").</li> <li>Person: Second person (\"Configure your environment\") or impersonal (\"The environment is configured\").</li> <li>English: Standard American English. Avoid excessive local idioms.</li> </ul>"},{"location":"en/style_guide/#markdown-structure","title":"Markdown Structure","text":""},{"location":"en/style_guide/#admonitions-notes","title":"Admonitions (Notes)","text":"<p>Use admonition blocks to highlight information:</p> <pre><code>!!! note \"Note\"\nNeutral information.\n\n!!! tip \"Tip\"\nOptimization help.\n\n!!! warning \"Warning\"\nBe careful with this.\n\n!!! danger \"Danger\"\nRisk of data loss.\n</code></pre>"},{"location":"en/style_guide/#code","title":"Code","text":"<p>Code blocks with specified language:</p> <pre><code>def my_function():\n    pass\n</code></pre>"},{"location":"en/style_guide/#governance-process","title":"Governance Process","text":"<ol> <li>Changes: Any code change affecting functionality requires documentation update in the same PR.</li> <li>Review: Documentation PRs require human review.</li> <li>Maintenance: Quarterly obsolescence review.</li> </ol>"},{"location":"en/troubleshooting/","title":"\ud83d\udd27 Troubleshooting","text":"<p>Golden Rule</p> <p>For any problem, the first step is always to check system logs. <code>bash     # View logs in real-time     tail -f ~/.local/state/v2m/v2m.log</code></p>"},{"location":"en/troubleshooting/#audio-and-recording","title":"\ud83d\uded1 Audio and Recording","text":""},{"location":"en/troubleshooting/#no-sound-empty-transcription","title":"No sound / Empty transcription","text":"<ul> <li>Symptom: Recording starts and stops, but no text is generated.</li> <li>Diagnosis:   Run the audio diagnostic script:   <pre><code>python scripts/diagnose_audio.py\n</code></pre></li> <li>Solutions:</li> <li>Audio Driver: Voice2Machine uses <code>SoundDevice</code>. Make sure your system (PulseAudio/PipeWire) has an active default microphone.</li> <li>Permissions: On Linux, your user must belong to the <code>audio</code> group (<code>sudo usermod -aG audio $USER</code>).</li> </ul>"},{"location":"en/troubleshooting/#cut-off-or-incomplete-phrases","title":"Cut-off or incomplete phrases","text":"<ul> <li>Cause: The silence detector (VAD) is too aggressive.</li> <li>Solution:   Adjust settings in <code>config.toml</code> or via the GUI:</li> <li>Reduce the <code>threshold</code> (e.g., from <code>0.35</code> to <code>0.30</code>).</li> <li>Increase <code>min_silence_duration_ms</code> (e.g., to <code>800ms</code>).</li> </ul>"},{"location":"en/troubleshooting/#performance-and-gpu","title":"\ud83d\udc22 Performance and GPU","text":""},{"location":"en/troubleshooting/#slow-transcription-2-seconds","title":"Slow transcription (&gt; 2 seconds)","text":"<ul> <li>Probable Cause: Whisper is running on CPU instead of GPU.</li> <li>Verification:   <pre><code>python scripts/test_whisper_gpu.py\n</code></pre></li> <li>Solution:</li> <li>Install updated NVIDIA drivers (CUDA 12 compatible).</li> <li>Verify <code>config.toml</code> has <code>device = \"cuda\"</code>.</li> <li>If you don't have a dedicated GPU, switch model to <code>distil-medium.en</code> or <code>base</code>.</li> </ul>"},{"location":"en/troubleshooting/#error-cuda-out-of-memory","title":"Error <code>CUDA out of memory</code>","text":"<ul> <li>Cause: Your GPU doesn't have enough VRAM for the selected model.</li> <li>Solution:</li> <li>Change <code>compute_type</code> to <code>int8_float16</code> (reduces VRAM usage by half).</li> <li>Use a lighter model (<code>distil-large-v3</code> consumes less than original <code>large-v3</code>).</li> </ul>"},{"location":"en/troubleshooting/#connectivity-and-daemon","title":"\ud83d\udd0c Connectivity and Daemon","text":""},{"location":"en/troubleshooting/#connection-refused-in-gui-or-scripts","title":"\"Connection refused\" in GUI or Scripts","text":"<ul> <li>Cause: The backend process (Python) isn't running or the server crashed.</li> <li>Solution:</li> <li>Verify status:       <pre><code>pgrep -a python | grep v2m\n</code></pre></li> <li>If not running, start manually to see startup errors:       <pre><code>python -m v2m.main\n</code></pre></li> <li>If it says \"Address already in use\", kill the existing process:       <pre><code>pkill -f \"v2m.main\"\n</code></pre></li> </ul>"},{"location":"en/troubleshooting/#keyboard-shortcuts-dont-respond","title":"Keyboard shortcuts don't respond","text":"<ul> <li>Cause: Permission issue or incorrect path in window manager configuration.</li> <li>Solution:</li> <li>Run the script manually in terminal: <code>scripts/v2m-toggle.sh</code>.</li> <li>If it works, the error is in your shortcut configuration (e.g., relative path <code>~/</code> instead of <code>/home/...</code>).</li> <li>If it doesn't work, verify permissions: <code>chmod +x scripts/*.sh</code>.</li> </ul>"},{"location":"en/troubleshooting/#ai-errors-llm","title":"\ud83e\udde0 AI Errors (LLM)","text":""},{"location":"en/troubleshooting/#error-401403-with-gemini","title":"Error 401/403 with Gemini","text":"<ul> <li>Cause: Invalid or expired API Key.</li> <li>Solution: Regenerate your key at Google AI Studio and update the <code>.env</code> file or <code>GEMINI_API_KEY</code> environment variable.</li> </ul>"},{"location":"en/troubleshooting/#connection-refused-with-ollama","title":"\"Connection refused\" with Ollama","text":"<ul> <li>Cause: The Ollama server isn't running.</li> <li>Solution: Run <code>ollama serve</code> in another terminal.</li> </ul>"},{"location":"en/adr/","title":"Architecture Decision Records (ADRs)","text":"<p>An Architecture Decision Record (ADR) is a document that captures an important architectural decision, along with its context and consequences.</p>"},{"location":"en/adr/#index-of-decisions","title":"Index of Decisions","text":"ADR Title Status Date ADR-001 Migration from Unix Sockets IPC to FastAPI REST Accepted 2025-01 ADR-002 Replacement of CQRS/CommandBus with Orchestrator Accepted 2025-01 ADR-003 Selection of faster-whisper over whisper.cpp Accepted 2024-06 ADR-004 Hexagonal Architecture (Ports and Adapters) Accepted 2024-03 ADR-005 Rust Audio Engine (v2m_engine) Accepted 2025-01 ADR-006 Local-first: Processing Without Cloud Accepted 2024-03"},{"location":"en/adr/#when-to-write-an-adr","title":"When to Write an ADR?","text":"<p>Write an ADR when you make a significant decision that affects the project's structure, dependencies, interfaces, or technology.</p> <p>See ADR Template for the format.</p>"},{"location":"en/adr/001-fastapi-migration/","title":"ADR-001: Migration from Unix Sockets IPC to FastAPI REST","text":""},{"location":"en/adr/001-fastapi-migration/#status","title":"Status","text":"<p>Accepted</p>"},{"location":"en/adr/001-fastapi-migration/#date","title":"Date","text":"<p>2025-01-15</p>"},{"location":"en/adr/001-fastapi-migration/#context","title":"Context","text":"<p>The original Voice2Machine system used Unix Domain Sockets with a custom binary protocol for communication between the Daemon (Python) and clients (scripts, Tauri frontend).</p>"},{"location":"en/adr/001-fastapi-migration/#previous-system-limitations","title":"Previous system limitations:","text":"<ol> <li>Debugging complexity: Binary messages required specialized tools for inspection</li> <li>Learning curve: New developers needed to understand the proprietary protocol</li> <li>Standard tools incompatibility: Couldn't use <code>curl</code>, Postman, or browsers for testing</li> <li>Protocol maintenance: Every change required synchronized client and server updates</li> <li>Interactive documentation: No way to auto-generate docs</li> </ol>"},{"location":"en/adr/001-fastapi-migration/#requirements","title":"Requirements:","text":"<ul> <li>Maintain latency &lt; 50ms for critical operations (toggle)</li> <li>Allow real-time event streaming</li> <li>Simplify onboarding for new developers</li> <li>Facilitate testing and debugging</li> </ul>"},{"location":"en/adr/001-fastapi-migration/#decision","title":"Decision","text":"<p>Migrate to FastAPI as REST API framework, completely replacing the proprietary IPC system.</p>"},{"location":"en/adr/001-fastapi-migration/#implementation","title":"Implementation:","text":"<ul> <li>FastAPI + Uvicorn: Async HTTP server with performance comparable to Go/Rust</li> <li>WebSocket: For event streaming (provisional transcription)</li> <li>Pydantic V2: Automatic validation and OpenAPI schema generation</li> <li>Swagger UI: Interactive documentation at <code>/docs</code></li> </ul>"},{"location":"en/adr/001-fastapi-migration/#consequences","title":"Consequences","text":""},{"location":"en/adr/001-fastapi-migration/#positive","title":"Positive","text":"<ul> <li>\u2705 Trivial debugging: <code>curl -X POST localhost:8765/toggle | jq</code></li> <li>\u2705 Automatic documentation: Swagger UI included without extra effort</li> <li>\u2705 Standard ecosystem: Compatible with any HTTP client</li> <li>\u2705 Simplified testing: FastAPI TestClient for integration tests</li> <li>\u2705 Fast onboarding: A Junior can understand the API in minutes</li> </ul>"},{"location":"en/adr/001-fastapi-migration/#negative","title":"Negative","text":"<ul> <li>\u26a0\ufe0f HTTP overhead: ~2-5ms additional vs raw sockets (acceptable)</li> <li>\u26a0\ufe0f Exposed port: Requires firewall configuration (mitigated with <code>127.0.0.1</code> only)</li> <li>\u26a0\ufe0f Additional dependency: FastAPI + Uvicorn (~2MB)</li> </ul>"},{"location":"en/adr/001-fastapi-migration/#alternatives-considered","title":"Alternatives Considered","text":""},{"location":"en/adr/001-fastapi-migration/#grpc","title":"gRPC","text":"<ul> <li>Rejected: Requires additional tooling (protoc), similar learning curve to original IPC, no native Swagger UI.</li> </ul>"},{"location":"en/adr/001-fastapi-migration/#graphql","title":"GraphQL","text":"<ul> <li>Rejected: Unnecessary overhead for simple RPC-style operations, greater complexity.</li> </ul>"},{"location":"en/adr/001-fastapi-migration/#keep-unix-sockets","title":"Keep Unix Sockets","text":"<ul> <li>Rejected: Didn't solve debugging and onboarding problems.</li> </ul>"},{"location":"en/adr/001-fastapi-migration/#references","title":"References","text":"<ul> <li>FastAPI Documentation</li> <li>Uvicorn Performance</li> </ul>"},{"location":"en/adr/002-orchestrator-pattern/","title":"ADR-002: Replacement of CQRS/CommandBus with Orchestrator","text":""},{"location":"en/adr/002-orchestrator-pattern/#status","title":"Status","text":"<p>Accepted</p>"},{"location":"en/adr/002-orchestrator-pattern/#date","title":"Date","text":"<p>2025-01-15</p>"},{"location":"en/adr/002-orchestrator-pattern/#context","title":"Context","text":"<p>The original backend implemented the CQRS (Command Query Responsibility Segregation) pattern with a CommandBus to process user actions.</p>"},{"location":"en/adr/002-orchestrator-pattern/#identified-problems","title":"Identified problems:","text":"<ol> <li>Over-engineering: For a system with ~10 commands, CQRS overhead was disproportionate</li> <li>Excessive indirection: Command \u2192 CommandBus \u2192 Handler \u2192 Result \u2192 Response</li> <li>Boilerplate: Each new feature required creating Command DTO + Handler + registering in bus</li> <li>Complex debugging: Deep stack traces obscured the real flow</li> <li>Verbose testing: CommandBus mocks in every test</li> </ol>"},{"location":"en/adr/002-orchestrator-pattern/#requirements","title":"Requirements:","text":"<ul> <li>Maintain separation of concerns (don't couple API to infrastructure)</li> <li>Simplify control flow</li> <li>Reduce boilerplate for new features</li> <li>Facilitate testing and debugging</li> </ul>"},{"location":"en/adr/002-orchestrator-pattern/#decision","title":"Decision","text":"<p>Replace CQRS/CommandBus with a central Orchestrator that coordinates workflow directly.</p>"},{"location":"en/adr/002-orchestrator-pattern/#implementation","title":"Implementation:","text":"<pre><code>class Orchestrator:\n    async def toggle(self) -&gt; ToggleResponse: ...\n    async def start(self) -&gt; ToggleResponse: ...\n    async def stop(self) -&gt; ToggleResponse: ...\n</code></pre> <p>The Orchestrator:</p> <ul> <li>Exposes direct methods for each operation</li> <li>Coordinates adapters (AudioRecorder, WhisperAdapter, LLMProvider)</li> <li>Maintains system state</li> <li>Emits events to WebSocket clients</li> </ul>"},{"location":"en/adr/002-orchestrator-pattern/#consequences","title":"Consequences","text":""},{"location":"en/adr/002-orchestrator-pattern/#positive","title":"Positive","text":"<ul> <li>\u2705 Explicit flow: API \u2192 Orchestrator \u2192 Adapters (3 layers vs 6+)</li> <li>\u2705 Less code: Eliminated ~500 LOC of CommandBus infrastructure</li> <li>\u2705 Simple debugging: Clear and short stack traces</li> <li>\u2705 Direct testing: Mock adapters, not abstract buses</li> <li>\u2705 Onboarding: New devs understand the system in minutes</li> </ul>"},{"location":"en/adr/002-orchestrator-pattern/#negative","title":"Negative","text":"<ul> <li>\u26a0\ufe0f Less extensible: Adding a global \"middleware\" is less trivial</li> <li>\u26a0\ufe0f Orchestrator \"god object\": Risk of growing too large (mitigated with composition)</li> </ul>"},{"location":"en/adr/002-orchestrator-pattern/#alternatives-considered","title":"Alternatives Considered","text":""},{"location":"en/adr/002-orchestrator-pattern/#keep-simplified-cqrs","title":"Keep simplified CQRS","text":"<ul> <li>Rejected: Even simplified, the conceptual overhead wasn't justified.</li> </ul>"},{"location":"en/adr/002-orchestrator-pattern/#event-sourcing","title":"Event Sourcing","text":"<ul> <li>Rejected: Even greater over-engineering for current use case.</li> </ul>"},{"location":"en/adr/002-orchestrator-pattern/#simple-functions-no-class","title":"Simple Functions (no class)","text":"<ul> <li>Rejected: Lost state management and lifecycle.</li> </ul>"},{"location":"en/adr/002-orchestrator-pattern/#references","title":"References","text":"<ul> <li>Martin Fowler on CQRS</li> <li>When not to use CQRS</li> </ul>"},{"location":"en/adr/003-faster-whisper/","title":"ADR-003: Selection of faster-whisper over whisper.cpp","text":""},{"location":"en/adr/003-faster-whisper/#status","title":"Status","text":"<p>Accepted</p>"},{"location":"en/adr/003-faster-whisper/#date","title":"Date","text":"<p>2024-06-01</p>"},{"location":"en/adr/003-faster-whisper/#context","title":"Context","text":"<p>For local voice transcription, we needed to choose a Whisper model implementation from OpenAI that would maximize performance on consumer NVIDIA GPUs (RTX 3060-4090).</p>"},{"location":"en/adr/003-faster-whisper/#options-evaluated","title":"Options evaluated:","text":"<ol> <li>OpenAI Whisper (original): Reference implementation in PyTorch</li> <li>whisper.cpp: Pure C++ implementation with CUDA support</li> <li>faster-whisper: Implementation on CTranslate2 (optimized C++/CUDA)</li> </ol>"},{"location":"en/adr/003-faster-whisper/#requirements","title":"Requirements:","text":"<ul> <li>Latency &lt; 500ms for 5 seconds of audio (8x real-time minimum)</li> <li>Support for <code>large-v3</code> models and <code>distil</code> variants</li> <li>INT8/FP16 quantization to optimize VRAM</li> <li>Python API for backend integration</li> <li>Audio streaming/chunking</li> </ul>"},{"location":"en/adr/003-faster-whisper/#decision","title":"Decision","text":"<p>Adopt faster-whisper as the main transcription engine.</p>"},{"location":"en/adr/003-faster-whisper/#justification","title":"Justification:","text":"Criteria Whisper (PyTorch) whisper.cpp faster-whisper Speed 1x (baseline) 4x 4-8x VRAM (large-v3) 10GB 6GB 4-5GB Python API \u2705 Native \u274c Bindings \u2705 Excellent Quantization Limited \u2705 \u2705 INT8/FP16 Maintenance OpenAI Community Active (Systran)"},{"location":"en/adr/003-faster-whisper/#consequences","title":"Consequences","text":""},{"location":"en/adr/003-faster-whisper/#positive","title":"Positive","text":"<ul> <li>\u2705 4-8x faster than original Whisper with same accuracy</li> <li>\u2705 ~50% less VRAM: Allows using large-v3 on 6GB GPUs</li> <li>\u2705 Pythonic API: Natural integration with FastAPI async</li> <li>\u2705 Distil models support: <code>distil-large-v3</code> for minimum latency</li> </ul>"},{"location":"en/adr/003-faster-whisper/#negative","title":"Negative","text":"<ul> <li>\u26a0\ufe0f Additional dependency: CTranslate2 binary (~100MB)</li> <li>\u26a0\ufe0f Less portable: Requires compatible CUDA toolkit</li> <li>\u26a0\ufe0f Lag on new models: New OpenAI releases take ~2 weeks to be available</li> </ul>"},{"location":"en/adr/003-faster-whisper/#alternatives-considered","title":"Alternatives Considered","text":""},{"location":"en/adr/003-faster-whisper/#whispercpp","title":"whisper.cpp","text":"<ul> <li>Rejected: Immature Python bindings, more complex debugging.</li> </ul>"},{"location":"en/adr/003-faster-whisper/#openai-whisper","title":"OpenAI Whisper","text":"<ul> <li>Rejected: Too slow for real-time experience without enterprise hardware.</li> </ul>"},{"location":"en/adr/003-faster-whisper/#whisper-jax","title":"Whisper JAX","text":"<ul> <li>Rejected: Requires TPU or complex JAX on CUDA configuration.</li> </ul>"},{"location":"en/adr/003-faster-whisper/#references","title":"References","text":"<ul> <li>faster-whisper GitHub</li> <li>CTranslate2</li> <li>Whisper Benchmarks</li> </ul>"},{"location":"en/adr/004-hexagonal-architecture/","title":"ADR-004: Hexagonal Architecture (Ports and Adapters)","text":""},{"location":"en/adr/004-hexagonal-architecture/#status","title":"Status","text":"<p>Accepted</p>"},{"location":"en/adr/004-hexagonal-architecture/#date","title":"Date","text":"<p>2024-03-01</p>"},{"location":"en/adr/004-hexagonal-architecture/#context","title":"Context","text":"<p>Voice2Machine started as a monolithic script of ~200 lines. As functionality grew, we faced typical problems of coupled code:</p> <ol> <li>Difficult testing: Mocks of GPU, audio, external APIs</li> <li>Cascade changes: Modifying Whisper required touching 5+ files</li> <li>Vendor lock-in: Switching from Ollama to Gemini required massive refactor</li> <li>Diffuse responsibilities: Unclear where to put new logic</li> </ol>"},{"location":"en/adr/004-hexagonal-architecture/#requirements","title":"Requirements:","text":"<ul> <li>Framework-agnostic business core</li> <li>Interchangeable adapters (e.g., swap Whisper for another ASR)</li> <li>Testability without real hardware</li> <li>Clear boundaries between layers</li> </ul>"},{"location":"en/adr/004-hexagonal-architecture/#decision","title":"Decision","text":"<p>Adopt Hexagonal Architecture (Ports &amp; Adapters) as the structural pattern.</p>"},{"location":"en/adr/004-hexagonal-architecture/#folder-structure","title":"Folder structure:","text":"<pre><code>src/v2m/\n\u251c\u2500\u2500 core/           # Configuration, logging, base interfaces\n\u251c\u2500\u2500 domain/         # Models, ports (interfaces), errors\n\u251c\u2500\u2500 services/       # Orchestrator, coordination\n\u2514\u2500\u2500 infrastructure/ # Adapters (Whisper, Audio, LLM)\n</code></pre>"},{"location":"en/adr/004-hexagonal-architecture/#port-implementation","title":"Port implementation:","text":"<pre><code>from typing import Protocol, runtime_checkable\n\n@runtime_checkable\nclass TranscriptionService(Protocol):\n    async def transcribe(self, audio: bytes) -&gt; str: ...\n</code></pre> <p>Adapters implement ports:</p> <pre><code>class WhisperAdapter:\n    async def transcribe(self, audio: bytes) -&gt; str:\n        # Concrete implementation with faster-whisper\n</code></pre>"},{"location":"en/adr/004-hexagonal-architecture/#consequences","title":"Consequences","text":""},{"location":"en/adr/004-hexagonal-architecture/#positive","title":"Positive","text":"<ul> <li>\u2705 Isolated testing: Unit tests without GPU or network</li> <li>\u2705 Flexibility: Switching Gemini for Ollama is editing 1 file</li> <li>\u2705 Onboarding: Predictable and documented structure</li> <li>\u2705 Type safety: <code>Protocol</code> + mypy detects incompatibilities at compile time</li> </ul>"},{"location":"en/adr/004-hexagonal-architecture/#negative","title":"Negative","text":"<ul> <li>\u26a0\ufe0f More files: ~20 files vs ~5 from original script</li> <li>\u26a0\ufe0f Indirection: Must navigate between layers to understand complete flow</li> <li>\u26a0\ufe0f Initial overhead: More complex setup for simple features</li> </ul>"},{"location":"en/adr/004-hexagonal-architecture/#alternatives-considered","title":"Alternatives Considered","text":""},{"location":"en/adr/004-hexagonal-architecture/#clean-architecture-uncle-bob","title":"Clean Architecture (Uncle Bob)","text":"<ul> <li>Rejected: Too many layers (Entities, Use Cases, Interface Adapters, Frameworks) for the scope.</li> </ul>"},{"location":"en/adr/004-hexagonal-architecture/#mvcmvp","title":"MVC/MVP","text":"<ul> <li>Rejected: UI-oriented, doesn't apply well to a backend daemon.</li> </ul>"},{"location":"en/adr/004-hexagonal-architecture/#simple-modules","title":"Simple Modules","text":"<ul> <li>Rejected: In practice, we returned to original coupling.</li> </ul>"},{"location":"en/adr/004-hexagonal-architecture/#references","title":"References","text":"<ul> <li>Alistair Cockburn - Hexagonal Architecture</li> <li>Netflix - Ready for changes with Hexagonal Architecture</li> </ul>"},{"location":"en/adr/005-rust-audio-engine/","title":"ADR-005: Rust Audio Engine (v2m_engine)","text":""},{"location":"en/adr/005-rust-audio-engine/#status","title":"Status","text":"<p>Accepted</p>"},{"location":"en/adr/005-rust-audio-engine/#date","title":"Date","text":"<p>2025-01-10</p>"},{"location":"en/adr/005-rust-audio-engine/#context","title":"Context","text":"<p>Audio capture in pure Python presented critical limitations for a real-time dictation experience:</p>"},{"location":"en/adr/005-rust-audio-engine/#identified-problems","title":"Identified problems:","text":"<ol> <li>GIL blocking: Audio capture competed with transcription for the GIL</li> <li>Variable latency: 10-50ms jitter in audio buffering</li> <li>sounddevice overhead: Python callbacks added latency</li> <li>Inefficient VAD: Silero VAD in Python processed samples with overhead</li> </ol>"},{"location":"en/adr/005-rust-audio-engine/#requirements","title":"Requirements:","text":"<ul> <li>Capture latency &lt; 10ms</li> <li>Pre-processed VAD before Python</li> <li>Lock-free circular buffer</li> <li>Zero-copy when possible</li> </ul>"},{"location":"en/adr/005-rust-audio-engine/#decision","title":"Decision","text":"<p>Develop native Rust extension (<code>v2m_engine</code>) for critical audio tasks.</p>"},{"location":"en/adr/005-rust-audio-engine/#components","title":"Components:","text":"<pre><code>v2m_engine/\n\u251c\u2500\u2500 src/\n\u2502   \u251c\u2500\u2500 audio_capture.rs  # CPAL-based capture\n\u2502   \u251c\u2500\u2500 ring_buffer.rs    # Lock-free circular buffer\n\u2502   \u251c\u2500\u2500 vad.rs            # Silero ONNX inference\n\u2502   \u2514\u2500\u2500 lib.rs            # PyO3 bindings\n</code></pre>"},{"location":"en/adr/005-rust-audio-engine/#python-interface","title":"Python interface:","text":"<pre><code>from v2m_engine import AudioCapture, VADProcessor\n\ncapture = AudioCapture(sample_rate=16000, buffer_size=4096)\nvad = VADProcessor(threshold=0.5)\n\nasync with capture.stream() as audio:\n    if vad.contains_speech(audio):\n        await transcriber.process(audio)\n</code></pre>"},{"location":"en/adr/005-rust-audio-engine/#consequences","title":"Consequences","text":""},{"location":"en/adr/005-rust-audio-engine/#positive","title":"Positive","text":"<ul> <li>\u2705 Latency &lt; 5ms: CPAL + lock-free buffers</li> <li>\u2705 GIL-free: Audio thread independent from Python</li> <li>\u2705 Efficient VAD: Native ONNX runtime, 10x faster</li> <li>\u2705 Zero-copy: NumPy arrays share memory with Rust</li> </ul>"},{"location":"en/adr/005-rust-audio-engine/#negative","title":"Negative","text":"<ul> <li>\u26a0\ufe0f Build complexity: Requires Rust toolchain + maturin</li> <li>\u26a0\ufe0f Cross-language debugging: Mixed Python/Rust stack traces</li> <li>\u26a0\ufe0f Portability: Platform-specific binaries</li> </ul>"},{"location":"en/adr/005-rust-audio-engine/#alternatives-considered","title":"Alternatives Considered","text":""},{"location":"en/adr/005-rust-audio-engine/#pure-python-sounddevice-numpy","title":"Pure Python (sounddevice + numpy)","text":"<ul> <li>Rejected: GIL blocking and unacceptable latency.</li> </ul>"},{"location":"en/adr/005-rust-audio-engine/#cython","title":"Cython","text":"<ul> <li>Rejected: Still tied to GIL, limited benefits.</li> </ul>"},{"location":"en/adr/005-rust-audio-engine/#c-extension","title":"C++ Extension","text":"<ul> <li>Rejected: Rust offers memory safety without GC, better tooling (cargo).</li> </ul>"},{"location":"en/adr/005-rust-audio-engine/#references","title":"References","text":"<ul> <li>PyO3 - Rust bindings for Python</li> <li>CPAL - Cross-platform Audio Library</li> <li>Lock-free Ring Buffer</li> </ul>"},{"location":"en/adr/006-local-first/","title":"ADR-006: Local-first: Processing Without Cloud","text":""},{"location":"en/adr/006-local-first/#status","title":"Status","text":"<p>Accepted</p>"},{"location":"en/adr/006-local-first/#date","title":"Date","text":"<p>2024-03-01</p>"},{"location":"en/adr/006-local-first/#context","title":"Context","text":"<p>Existing dictation services (Google Speech-to-Text, Whisper API, Dragon) require sending audio to external servers.</p>"},{"location":"en/adr/006-local-first/#problems-with-cloud-based-dictation","title":"Problems with cloud-based dictation:","text":"<ol> <li>Privacy: Sensitive audio (medical, legal, personal) leaves the machine</li> <li>Network latency: 100-500ms additional RTT</li> <li>Availability: Requires internet connection</li> <li>Costs: Transcription APIs charge per minute</li> <li>Rate limits: Throttling on intensive use</li> </ol>"},{"location":"en/adr/006-local-first/#user-requirements","title":"User requirements:","text":"<ul> <li>Absolute privacy: No data should leave the machine</li> <li>Offline operation: System must work without internet</li> <li>Minimum latency: &lt; 500ms end-to-end</li> <li>Zero cost: No subscriptions or usage fees</li> </ul>"},{"location":"en/adr/006-local-first/#decision","title":"Decision","text":"<p>Adopt \"Local-first\" philosophy where all voice processing occurs on the user's device.</p>"},{"location":"en/adr/006-local-first/#implementation","title":"Implementation:","text":"Component Local Solution Transcription faster-whisper on local GPU LLM (optional) Ollama with local models Audio Processed in RAM Storage Only temporary files, deleted post-use"},{"location":"en/adr/006-local-first/#configurable-exceptions","title":"Configurable exceptions:","text":"<p>User can opt-in to cloud services for text refinement:</p> <ul> <li>Google Gemini API (for LLM)</li> <li>But never for raw audio</li> </ul>"},{"location":"en/adr/006-local-first/#consequences","title":"Consequences","text":""},{"location":"en/adr/006-local-first/#positive","title":"Positive","text":"<ul> <li>\u2705 Guaranteed privacy: Audio never leaves the device</li> <li>\u2705 No network latency: All processing local</li> <li>\u2705 Works offline: No internet required for dictation</li> <li>\u2705 Predictable cost: Only hardware (GPU), no subscriptions</li> <li>\u2705 Compliance: Compatible with regulations (HIPAA, GDPR)</li> </ul>"},{"location":"en/adr/006-local-first/#negative","title":"Negative","text":"<ul> <li>\u26a0\ufe0f Requires GPU: Without NVIDIA GPU, degraded performance</li> <li>\u26a0\ufe0f Local LLM models: Lower quality than GPT-4/Gemini Pro</li> <li>\u26a0\ufe0f Manual updates: Models don't auto-update</li> </ul>"},{"location":"en/adr/006-local-first/#alternatives-considered","title":"Alternatives Considered","text":""},{"location":"en/adr/006-local-first/#hybrid-local-stt-cloud-llm-default","title":"Hybrid (local STT + cloud LLM default)","text":"<ul> <li>Rejected: Violates privacy-by-default principle.</li> </ul>"},{"location":"en/adr/006-local-first/#cloud-first-with-local-cache","title":"Cloud-first with local cache","text":"<ul> <li>Rejected: Unnecessary complexity, audio still needs to be uploaded.</li> </ul>"},{"location":"en/adr/006-local-first/#federated-learning","title":"Federated Learning","text":"<ul> <li>Rejected: Over-engineering for current scope.</li> </ul>"},{"location":"en/adr/006-local-first/#references","title":"References","text":"<ul> <li>Local-first Software</li> <li>Ink &amp; Switch - Seven Ideals</li> <li>GDPR and Voice Data</li> </ul>"},{"location":"en/adr/template/","title":"ADR-XXX: Short Decision Title","text":""},{"location":"en/adr/template/#status","title":"Status","text":"<p>[Proposed | Accepted | Rejected | Obsolete]</p>"},{"location":"en/adr/template/#context","title":"Context","text":"<p>Describe the context and problem we are solving.</p> <ul> <li>What is the current limitation?</li> <li>What technical or business requirements drive this?</li> </ul>"},{"location":"en/adr/template/#decision","title":"Decision","text":"<p>Describe the decision made.</p> <ul> <li>\"We will use X technology for Y component...\"</li> </ul>"},{"location":"en/adr/template/#consequences","title":"Consequences","text":"<p>What becomes easier or harder because of this change?</p>"},{"location":"en/adr/template/#positive","title":"Positive","text":"<p>-</p>"},{"location":"en/adr/template/#negative","title":"Negative","text":"<p>-</p>"},{"location":"en/adr/template/#alternatives-considered","title":"Alternatives Considered","text":"<ul> <li>Option A: Why rejected.</li> <li>Option B: Why rejected.</li> </ul>"},{"location":"en/api/","title":"Python API - Index","text":"<p>This section provides auto-generated documentation of Python classes and functions from the Voice2Machine backend.</p> <p>Generated with mkdocstrings</p> <p>This documentation is automatically extracted from source code docstrings. For the most up-to-date version, always check the code at <code>apps/daemon/backend/src/v2m/</code>.</p>"},{"location":"en/api/#main-modules","title":"Main Modules","text":""},{"location":"en/api/#interfaces","title":"Interfaces","text":"<p>Protocols and contracts that define the expected behavior of adapters.</p>"},{"location":"en/api/#domain","title":"Domain","text":"<p>Domain models, ports, and error types.</p>"},{"location":"en/api/#services","title":"Services","text":"<p>Application services including the main Orchestrator.</p>"},{"location":"en/api/#quick-navigation","title":"Quick Navigation","text":"Class/Function Description <code>Orchestrator</code> Central workflow coordinator <code>TranscriptionService</code> Port for transcription services <code>AudioRecorder</code> Interface for audio capture <code>LLMProvider</code> Base interface for AI providers"},{"location":"en/api/domain/","title":"Domain","text":"<p>This page documents domain models and data types.</p>"},{"location":"en/api/domain/#data-models","title":"Data Models","text":""},{"location":"en/api/domain/#correctionresult","title":"CorrectionResult","text":"<p>Structured output model for text refinement.</p> <p>This model forces LLMs to respond in a predictable JSON format, facilitating parsing and reducing format hallucinations.</p> <pre><code>from pydantic import BaseModel, Field\n\nclass CorrectionResult(BaseModel):\n    corrected_text: str = Field(\n        description=\"Corrected text with improved grammar and coherence\"\n    )\n    explanation: str | None = Field(\n        default=None,\n        description=\"Changes made to the original text\"\n    )\n</code></pre> <p>Usage Example:</p> <pre><code>result = CorrectionResult(\n    corrected_text=\"Hello, how are you?\",\n    explanation=\"Added punctuation and question marks\"\n)\n</code></pre>"},{"location":"en/api/domain/#domain-errors","title":"Domain Errors","text":"<p>The system defines specific exceptions for different error types:</p> Exception Description <code>TranscriptionError</code> Error during audio transcription <code>AudioCaptureError</code> Error in audio capture (microphone unavailable) <code>LLMError</code> Error communicating with LLM provider <code>ConfigurationError</code> System configuration error"},{"location":"en/api/interfaces/","title":"Interfaces (Protocols)","text":"<p>This page documents the protocols (interfaces) that define system contracts.</p>"},{"location":"en/api/interfaces/#orchestrator","title":"Orchestrator","text":"<p>The Orchestrator is the central component that coordinates the entire workflow.</p>"},{"location":"en/api/interfaces/#main-methods","title":"Main Methods","text":"<pre><code>class Orchestrator:\n    async def toggle(self) -&gt; ToggleResponse:\n        \"\"\"Recording toggle (start/stop).\"\"\"\n\n    async def start(self) -&gt; ToggleResponse:\n        \"\"\"Starts audio recording.\"\"\"\n\n    async def stop(self) -&gt; ToggleResponse:\n        \"\"\"Stops recording and transcribes audio.\"\"\"\n\n    async def warmup(self) -&gt; None:\n        \"\"\"Pre-loads Whisper model into VRAM.\"\"\"\n\n    async def shutdown(self) -&gt; None:\n        \"\"\"Releases resources on server shutdown.\"\"\"\n\n    def get_status(self) -&gt; StatusResponse:\n        \"\"\"Returns current daemon state.\"\"\"\n\n    async def process_text(self, text: str) -&gt; LLMResponse:\n        \"\"\"Processes text with LLM (cleanup, punctuation).\"\"\"\n\n    async def translate_text(self, text: str, target_lang: str) -&gt; LLMResponse:\n        \"\"\"Translates text with LLM.\"\"\"\n</code></pre>"},{"location":"en/api/interfaces/#toggleresponse","title":"ToggleResponse","text":"<pre><code>class ToggleResponse(BaseModel):\n    status: str      # 'recording' | 'idle'\n    message: str     # Descriptive message\n    text: str | None # Transcribed text (only on stop)\n</code></pre>"},{"location":"en/api/interfaces/#statusresponse","title":"StatusResponse","text":"<pre><code>class StatusResponse(BaseModel):\n    state: str        # 'idle' | 'recording' | 'processing'\n    recording: bool   # True if recording\n    model_loaded: bool # True if Whisper is in VRAM\n</code></pre>"},{"location":"en/api/interfaces/#llmresponse","title":"LLMResponse","text":"<pre><code>class LLMResponse(BaseModel):\n    text: str    # Processed/translated text\n    backend: str # 'gemini' | 'ollama' | 'local'\n</code></pre>"},{"location":"en/api/services/","title":"Services","text":"<p>This page documents backend application services.</p>"},{"location":"en/api/services/#orchestrator","title":"Orchestrator","text":"<p>The Orchestrator is the central service that coordinates the entire Voice2Machine workflow.</p>"},{"location":"en/api/services/#responsibilities","title":"Responsibilities","text":"<ul> <li>Manages complete lifecycle: recording \u2192 transcription \u2192 post-processing</li> <li>Maintains system state (idle, recording, processing)</li> <li>Coordinates communication between adapters without coupling them directly</li> <li>Emits events to connected WebSocket clients</li> </ul>"},{"location":"en/api/services/#lazy-initialization","title":"Lazy Initialization","text":"<p>All sub-services are created when first needed:</p> <pre><code>@property\ndef worker(self) -&gt; WhisperWorker:\n    \"\"\"Gets the Whisper worker (lazy init).\"\"\"\n    if self._worker is None:\n        self._worker = WhisperWorker()\n    return self._worker\n</code></pre>"},{"location":"en/api/services/#coordinated-services","title":"Coordinated Services","text":"Service Description <code>WhisperWorker</code> Transcription with faster-whisper <code>AudioRecorder</code> Audio capture (Rust extension) <code>StreamingTranscriber</code> Real-time transcription <code>ClipboardService</code> System clipboard access <code>NotificationService</code> Desktop notifications <code>LLMService</code> Processing with Gemini/Ollama"},{"location":"en/api/backend/","title":"Backend API (Python)","text":"<p>This section contains documentation automatically generated from the Voice2Machine backend source code.</p> <p>Auto-generated</p> <p>This documentation syncs automatically with code docstrings. Source of truth: <code>apps/daemon/backend/src/v2m/</code></p>"},{"location":"en/api/backend/#main-modules","title":"Main Modules","text":""},{"location":"en/api/backend/#coordination-service","title":"Coordination Service","text":"<ul> <li>Orchestrator - Central system coordinator</li> <li>REST API - FastAPI endpoints and data models</li> </ul>"},{"location":"en/api/backend/#configuration","title":"Configuration","text":"<ul> <li>Config - Typed configuration system</li> </ul>"},{"location":"en/api/backend/#infrastructure","title":"Infrastructure","text":"<ul> <li>Transcription - Whisper and streaming</li> <li>LLM Services - Gemini, Ollama, Local</li> </ul>"},{"location":"en/api/backend/#layer-navigation","title":"Layer Navigation","text":"<pre><code>graph TD\n    A[REST API] --&gt; B[Orchestrator]\n    B --&gt; C[Infrastructure]\n    C --&gt; D[Whisper]\n    C --&gt; E[Audio Recorder]\n    C --&gt; F[LLM Providers]\n\n    style A fill:#e3f2fd\n    style B fill:#fff3e0\n    style C fill:#f3e5f5</code></pre> Layer Responsibility API HTTP endpoints, validation, serialization Services Workflow coordination Infrastructure External service adapters"},{"location":"en/api/backend/#code-status","title":"Code Status","text":"Metric Value Python Files 27 Docstring Coverage ~70% Style Google Style"},{"location":"en/api/backend/api/","title":"REST API","text":"<p>FastAPI endpoints and data models documentation.</p>"},{"location":"en/api/backend/api/#requestresponse-models","title":"Request/Response Models","text":"<p>options: show_source: false</p> <p>options: show_source: false</p> <p>options: show_source: false</p> <p>options: show_source: false</p> <p>options: show_source: false</p>"},{"location":"en/api/backend/api/#global-state","title":"Global State","text":"<p>options: show_source: true members: - init - orchestrator - broadcast_event</p>"},{"location":"en/api/backend/config/","title":"Configuration","text":"<p>Typed configuration system using Pydantic Settings.</p>"},{"location":"en/api/backend/config/#main-settings","title":"Main Settings","text":"<p>options: show_source: false members: - paths - transcription - llm - gemini - notifications</p>"},{"location":"en/api/backend/config/#paths-configuration","title":"Paths Configuration","text":"<p>options: show_source: false</p>"},{"location":"en/api/backend/config/#transcription-configuration","title":"Transcription Configuration","text":"<p>options: show_source: false</p> <p>options: show_source: false</p> <p>options: show_source: false</p>"},{"location":"en/api/backend/config/#llm-configuration","title":"LLM Configuration","text":"<p>options: show_source: false</p> <p>options: show_source: false</p> <p>options: show_source: false</p> <p>options: show_source: false</p>"},{"location":"en/api/backend/config/#notifications-configuration","title":"Notifications Configuration","text":"<p>options: show_source: false</p>"},{"location":"en/api/backend/llm/","title":"LLM Services","text":"<p>Language model providers for text processing.</p>"},{"location":"en/api/backend/llm/#google-gemini-cloud","title":"Google Gemini (Cloud)","text":"<p>LLM service connecting to Google Gemini API for text processing and translations.</p> <p>Location: <code>v2m/infrastructure/gemini_llm_service.py</code></p> <p>Main methods:</p> <ul> <li><code>process_text(text: str) -&gt; str</code> - Refines text with punctuation and grammar</li> <li><code>translate_text(text: str, target_lang: str) -&gt; str</code> - Translates text</li> </ul>"},{"location":"en/api/backend/llm/#ollama-local","title":"Ollama (Local)","text":"<p>Local LLM service connecting to Ollama server for total privacy.</p> <p>Location: <code>v2m/infrastructure/ollama_llm_service.py</code></p> <p>Configuration: <code>http://localhost:11434</code></p>"},{"location":"en/api/backend/llm/#local-llamacpp","title":"Local (llama.cpp)","text":"<p>Embedded LLM service using llama-cpp-python directly.</p> <p>Location: <code>v2m/infrastructure/local_llm_service.py</code></p>"},{"location":"en/api/backend/llm/#design-pattern","title":"Design Pattern","text":"<p>All LLM services implement a common interface:</p> <pre><code>class LLMService(Protocol):\n    def process_text(self, text: str) -&gt; str:\n        \"\"\"Refines text with grammar and punctuation.\"\"\"\n        ...\n\n    def translate_text(self, text: str, target_lang: str) -&gt; str:\n        \"\"\"Translates text to specified language.\"\"\"\n        ...\n</code></pre> <p>The <code>Orchestrator</code> selects the backend based on <code>config.llm.backend</code>:</p> <ul> <li><code>\"gemini\"</code> \u2192 GeminiLLMService</li> <li><code>\"ollama\"</code> \u2192 OllamaLLMService</li> <li><code>\"local\"</code> \u2192 LocalLLMService</li> </ul>"},{"location":"en/api/backend/orchestrator/","title":"Orchestrator","text":"<p>The Orchestrator is the central service that coordinates the entire Voice2Machine workflow.</p>"},{"location":"en/api/backend/orchestrator/#api-reference","title":"API Reference","text":"<p>options: show_source: true members: - init - toggle - start - stop - warmup - shutdown - get_status - process_text - translate_text</p>"},{"location":"en/api/backend/transcription/","title":"Transcription","text":"<p>Audio to text transcription services using faster-whisper.</p>"},{"location":"en/api/backend/transcription/#persistentwhisperworker","title":"PersistentWhisperWorker","text":"<p>Persistent worker that keeps the Whisper model loaded in VRAM between sessions.</p> <p>Location: <code>v2m/infrastructure/persistent_model.py</code></p>"},{"location":"en/api/backend/transcription/#features","title":"Features","text":"<ul> <li>Lazy Loading: Model loads the first time it's needed</li> <li>Keep-Warm Policy: Keeps model in VRAM based on configuration</li> <li>GPU Optimized: Uses <code>float16</code> or <code>int8_float16</code> for maximum performance</li> </ul>"},{"location":"en/api/backend/transcription/#methods","title":"Methods","text":"<pre><code>class PersistentWhisperWorker:\n    def initialize_sync(self) -&gt; None:\n        \"\"\"Loads model into VRAM (synchronous, for warmup).\"\"\"\n\n    async def transcribe(self, audio: np.ndarray) -&gt; str:\n        \"\"\"Transcribes audio to text.\"\"\"\n\n    async def unload(self) -&gt; None:\n        \"\"\"Releases model from VRAM.\"\"\"\n</code></pre>"},{"location":"en/api/backend/transcription/#streamingtranscriber","title":"StreamingTranscriber","text":"<p>Real-time transcriber providing provisional feedback while the user speaks.</p> <p>Location: <code>v2m/infrastructure/streaming_transcriber.py</code></p>"},{"location":"en/api/backend/transcription/#data-flow","title":"Data Flow","text":"<pre><code>graph LR\n    A[AudioRecorder] --&gt; B[VAD]\n    B --&gt; C[StreamingTranscriber]\n    C --&gt; D[WhisperWorker]\n    D --&gt; E[WebSocket]\n\n    style C fill:#e3f2fd</code></pre>"},{"location":"en/api/backend/transcription/#integration","title":"Integration","text":"<p>The <code>StreamingTranscriber</code> emits events via WebSocket:</p> <ul> <li><code>transcription_update</code>: Provisional text during recording</li> <li><code>transcription_final</code>: Final text on stop</li> </ul>"}]}
---
target:
 - src/backend
 - src/frontend
---

<sugestion>
| Categoría         | Modelo Recomendado  | Por qué es el mejor para ti                                                                                                                                                                                                                                                     |
| ----------------- | ------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| Mejor General     | Phi-3.5 Mini (3.8B) | Es increíblemente ligero y potente. Cabe perfectamente en los 4GB de VRAM de la RTX 3050, dejando espacio para que tu aplicación v2m-lab (que probablemente use PyTorch/TensorFlow) corra simultáneamente sin crashear por falta de memoria getstream+1​.                       |
| Mejor para Código | Qwen 2.5 Coder (7B) | Si buscas un asistente para programar tu aplicación. Es superior a Llama 3.1 en tareas de código (Python, PyTorch). Necesitarás usar una versión cuantizada (4-bit) para que quepa en 6GB VRAM. En 4GB VRAM, será lento porque usará la RAM del sistema discuss.huggingface+1​. |
| Máxima Eficiencia | Gemma 2 (2B)        | Si tu aplicación v2m-lab ya consume casi toda tu GPU procesando video/audio, este modelo es diminuto pero sorprendentemente capaz para instrucciones simples reddit​.                                                                                                           |
</sugestion>

<task>
### 1. Backend: Implementar Soporte para Ollama con Structured Outputs
- **Configuración**: Actualizar `apps/backend/src/v2m/config.py` para incluir `OllamaConfig` (host, model, keep_alive) y añadir `ollama` como opción válida en `LLMConfig.backend`.
- **Dominio**: Crear o actualizar `apps/backend/src/v2m/domain/ports.py` definiendo el protocolo `TextRefinementService` y el modelo Pydantic `CorrectionResult` (con campos `corrected_text` y `explanation`).
- **Infraestructura**: Implementar `apps/backend/src/v2m/infrastructure/llm/ollama_adapter.py` utilizando la librería `ollama` (async) y forzando el esquema JSON de `CorrectionResult`.
- **Integración**: Asegurar que el daemon (`daemon.py`) o el manejador de comandos utilice el backend seleccionado dinámicamente.

### 2. Frontend: Interfaz de Configuración para Ollama
- **Schema**: Actualizar `apps/frontend/src/schemas/config.ts` para incluir `OllamaConfigSchema` y permitir `ollama` en el enum de backends.
- **UI**: Modificar `apps/frontend/src/components/settings/GeneralSection.tsx` para:
    - Añadir "Ollama" al selector de backends.
    - Mostrar un selector de modelos dinámico (o predefinido con `phi3.5-mini`, `qwen2.5-coder:7b`, `gemma2:2b`) cuando Ollama esté seleccionado.
    - Añadir campo para el host de Ollama (default: `http://localhost:11434`).

### 3. Optimización de Recursos (VRAM)
- Implementar la lógica de `keep_alive` en el adaptador de Ollama para liberar VRAM si se detecta que el hardware es limitado (RTX 3050/3060).
</task>

<definition of done>
- [ ] El backend puede inicializarse con `backend = "ollama"` en `config.toml`.
- [ ] Las correcciones de texto devuelven un objeto JSON estructurado validado por Pydantic.
- [ ] El frontend permite cambiar entre Gemini, Local (llama.cpp) y Ollama.
- [ ] Se ha verificado que `phi3.5-mini` funciona correctamente con el adaptador.
- [ ] El código sigue los estándares de 2026 (Type hints, Async, Hexagonal).
</definition of done>


<quality vectors>
- Ensure you have used the state of the art as of 2026.
- Ensure you have used best practices as of 2026.
- Mentally, counterargue and look at what you did with a very skeptical and critical eye.
- Then, once identified, apply the improvements.
</quality vectors>

<suggestion of teamate>
Considerando la fecha actual (Enero 2026) y las especificaciones de tu **Dell G15 5511**, he revisado la recomendación para asegurar que uses el "Estado del Arte" (SOTA) real y las mejores prácticas de ingeniería de software para tu arquitectura hexagonal.

Para tu caso de uso (corrección de texto dictado) en un entorno de recursos limitados (laptop gaming), la recomendación definitiva para 2026 es usar modelos altamente eficientes ejecutados vía **Ollama** con **Structured Outputs**.

### 1. Selección de Modelos SOTA 2026 para Dell G15 (4GB/6GB VRAM)

| Categoría         | Modelo Recomendado  | Por qué es el mejor para ti                                                                                                                                                                                                                                                     |
| ----------------- | ------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| Mejor General     | Phi-3.5 Mini (3.8B) | Es increíblemente ligero y potente. Cabe perfectamente en los 4GB de VRAM de la RTX 3050, dejando espacio para que tu aplicación v2m-lab (que probablemente use PyTorch/TensorFlow) corra simultáneamente sin crashear por falta de memoria.                       |
| Mejor para Código | Qwen 2.5 Coder (7B) | Si buscas un asistente para programar tu aplicación. Es superior a Llama 3.1 en tareas de código (Python, PyTorch). Necesitarás usar una versión cuantizada (4-bit) para que quepa en 6GB VRAM. En 4GB VRAM, será lento porque usará la RAM del sistema. |
| Máxima Eficiencia | Gemma 2 (2B)        | Si tu aplicación v2m-lab ya consume casi toda tu GPU procesando video/audio, este modelo es diminuto pero sorprendentemente capaz para instrucciones simples.                                                                                                           |

### 2. Integración "Best Practice" 2026 (Arquitectura Hexagonal)

Tu proyecto `v2m-lab` sigue una arquitectura hexagonal limpia (`domain`, `application`, `infrastructure`). **No uses scripts bash (`curl`)**; eso es deuda técnica.

La práctica estándar en 2026 para garantizar que el LLM devuelva *solo* el texto corregido es usar **Structured Outputs** (Salidas Estructuradas) con Ollama.

#### A. Define la Interfaz (Domain)
En `src/v2m/domain/ports.py` (o similar), asegúrate de tener una interfaz clara:

```python
from typing import Protocol
from pydantic import BaseModel

class CorrectionResult(BaseModel):
    corrected_text: str
    explanation: str | None = None  # Opcional: saber qué cambió

class TextRefinementService(Protocol):
    async def refine(self, text: str) -> CorrectionResult:
        ...
```

#### B. Implementa el Adaptador (Infrastructure)
Crea `src/v2m/infrastructure/llm/ollama_adapter.py`. Usa la librería oficial de Ollama con soporte nativo de Pydantic:

```python
import os
from v2m.domain.ports import TextRefinementService, CorrectionResult
from ollama import AsyncClient

class OllamaRefiner(TextRefinementService):
    def __init__(self, model_name: str = "phi3.5-mini"):
        self.client = AsyncClient(host=os.getenv("OLLAMA_HOST", "http://localhost:11434"))
        self.model = model_name

    async def refine(self, text: str) -> CorrectionResult:
        # keep_alive=0 forces unload after response to free VRAM for Whisper
        response = await self.client.chat(
            model=self.model,
            messages=[
                {"role": "system", "content": "Eres un editor experto. Corrige la gramática y coherencia."},
                {"role": "user", "content": text}
            ],
            format=CorrectionResult.model_json_schema(), # <--- LA CLAVE: Fuerza JSON estricto
            keep_alive="0m" # Optimización para 4GB VRAM
        )
        # Ollama en 2026 valida esto automáticamente contra el esquema
        return CorrectionResult.model_validate_json(response.message.content)
```

### 3. Optimización de Hardware (El Cuello de Botella VRAM)

En una Dell G15, el conflicto principal es: **Whisper (VRAM) vs LLM (VRAM)**.

1.  **Estrategia "Ping-Pong" (Recomendada para 4GB):**
    *   Configura `keep_alive="0m"` en las llamadas a Ollama. Esto fuerza a descargar el modelo de la VRAM inmediatamente después de responder.
2.  **Estrategia "Peso Pluma" (Recomendada para Fluidez):**
    *   Usa **distil-whisper-large-v3** (~1GB VRAM).
    *   Combínalo con **Phi-3.5 Mini** (~2.3GB VRAM).
    *   *Resultado:* Whisper + LLM = ~3.3GB. **Cabe perfectamente en la RTX 3050 de 4GB**.

**Plan de Acción Inmediato:**
1.  Instala Ollama.
2.  Ejecuta `ollama run phi3.5-mini`.
3.  Implementa el adaptador de Python mostrado arriba en tu backend `v2m`.
</suggestion of teamate>
